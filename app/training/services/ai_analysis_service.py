"""AIæ·±åº¦å­¦ä¹ åˆ†ææœåŠ¡ - ğŸ”¥éœ€æ±‚21ç¬¬ä¸‰é˜¶æ®µAIåˆ†ææ ¸å¿ƒå®ç°.

AIæ·±åº¦åˆ†æåŠŸèƒ½ï¼š
- å­¦ä¹ æ¨¡å¼è¯†åˆ«å’Œåˆ†æ
- çŸ¥è¯†æŒæ¡åº¦æ·±åº¦è¯„ä¼°
- å­¦ä¹ æ•ˆç‡æ™ºèƒ½è¯„ä¼°
- åŸºäºDeepSeek AIçš„æ·±åº¦åˆ†æ
- ç”Ÿæˆæ—¶é—´<5ç§’çš„é«˜æ•ˆåˆ†æ
"""

import json
import logging
from datetime import datetime, timedelta
from typing import Any, TypedDict

import httpx
from sqlalchemy import and_, desc, select
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.config import settings
from app.shared.models.enums import TrainingType
from app.training.models.training_models import (
    TrainingRecord,
    TrainingSession,
)

logger = logging.getLogger(__name__)


class PatternRecognitionConfig(TypedDict):
    """å­¦ä¹ æ¨¡å¼è¯†åˆ«é…ç½®ç±»å‹."""

    min_sessions_for_pattern: int
    pattern_confidence_threshold: float
    learning_style_categories: list[str]


class KnowledgeMasteryConfig(TypedDict):
    """çŸ¥è¯†æŒæ¡åº¦é…ç½®ç±»å‹."""

    mastery_threshold: float
    struggling_threshold: float
    knowledge_decay_days: int
    retention_analysis_window: int


class EfficiencyFactors(TypedDict):
    """æ•ˆç‡å› å­é…ç½®ç±»å‹."""

    accuracy_weight: float
    speed_weight: float
    consistency_weight: float
    progress_weight: float


class EfficiencyAssessmentConfig(TypedDict):
    """å­¦ä¹ æ•ˆç‡è¯„ä¼°é…ç½®ç±»å‹."""

    optimal_accuracy_range: tuple[float, float]
    optimal_speed_range: tuple[int, int]
    efficiency_factors: EfficiencyFactors


class AIAnalysisConfig(TypedDict):
    """AIåˆ†æé…ç½®ç±»å‹."""

    ai_model: str
    analysis_timeout: int
    max_data_points: int
    analysis_window_days: int
    confidence_threshold: float
    pattern_recognition: PatternRecognitionConfig
    knowledge_mastery: KnowledgeMasteryConfig
    efficiency_assessment: EfficiencyAssessmentConfig


class AIAnalysisService:
    """AIæ·±åº¦å­¦ä¹ åˆ†ææœåŠ¡ - åŸºäºAIçš„å­¦ä¹ æ¨¡å¼è¯†åˆ«å’Œæ·±åº¦åˆ†æ."""

    def __init__(self, db: AsyncSession) -> None:
        self.db = db

        # AIåˆ†æé…ç½®
        self.analysis_config: AIAnalysisConfig = {
            "ai_model": "deepseek-chat",  # DeepSeek AIæ¨¡å‹
            "analysis_timeout": 5,  # 5ç§’åˆ†æè¶…æ—¶
            "max_data_points": 200,  # æœ€å¤§æ•°æ®ç‚¹æ•°
            "analysis_window_days": 30,  # 30å¤©åˆ†æçª—å£
            "confidence_threshold": 0.8,  # ç½®ä¿¡åº¦é˜ˆå€¼
            # å­¦ä¹ æ¨¡å¼è¯†åˆ«é…ç½®
            "pattern_recognition": {
                "min_sessions_for_pattern": 5,  # æœ€å°‘5ä¸ªä¼šè¯æ‰è¯†åˆ«æ¨¡å¼
                "pattern_confidence_threshold": 0.7,  # æ¨¡å¼è¯†åˆ«ç½®ä¿¡åº¦
                "learning_style_categories": [
                    "visual_learner",
                    "auditory_learner",
                    "kinesthetic_learner",
                    "sequential_learner",
                    "global_learner",
                    "analytical_learner",
                ],
            },
            # çŸ¥è¯†æŒæ¡åº¦åˆ†æé…ç½®
            "knowledge_mastery": {
                "mastery_threshold": 0.85,  # æŒæ¡é˜ˆå€¼85%
                "struggling_threshold": 0.6,  # å›°éš¾é˜ˆå€¼60%
                "knowledge_decay_days": 7,  # çŸ¥è¯†é—å¿˜å‘¨æœŸ7å¤©
                "retention_analysis_window": 14,  # ä¿æŒåˆ†æçª—å£14å¤©
            },
            # å­¦ä¹ æ•ˆç‡è¯„ä¼°é…ç½®
            "efficiency_assessment": {
                "optimal_accuracy_range": (0.75, 0.85),  # æœ€ä½³æ­£ç¡®ç‡èŒƒå›´
                "optimal_speed_range": (45, 90),  # æœ€ä½³ç­”é¢˜æ—¶é—´èŒƒå›´ï¼ˆç§’ï¼‰
                "efficiency_factors": {
                    "accuracy_weight": 0.4,
                    "speed_weight": 0.3,
                    "consistency_weight": 0.2,
                    "progress_weight": 0.1,
                },
            },
        }

    async def generate_comprehensive_analysis_report(
        self, student_id: int, training_type: TrainingType | None = None
    ) -> dict[str, Any]:
        """ç”Ÿæˆç»¼åˆAIåˆ†ææŠ¥å‘Š."""
        try:
            logger.info(f"å¼€å§‹ç”ŸæˆAIæ·±åº¦åˆ†ææŠ¥å‘Š: å­¦ç”Ÿ{student_id}, è®­ç»ƒç±»å‹{training_type}")
            start_time = datetime.now()

            # 1. æ”¶é›†å­¦ä¹ æ•°æ®
            learning_data = await self._collect_learning_data(student_id, training_type)

            if not learning_data["has_sufficient_data"]:
                return {
                    "analysis_available": False,
                    "reason": "æ•°æ®ä¸è¶³ï¼Œéœ€è¦æ›´å¤šå­¦ä¹ è®°å½•",
                    "minimum_requirements": {
                        "sessions": self.analysis_config["pattern_recognition"][
                            "min_sessions_for_pattern"
                        ],
                        "records": 20,
                    },
                }

            # 2. å­¦ä¹ æ¨¡å¼è¯†åˆ«
            learning_patterns = await self._analyze_learning_patterns(learning_data)

            # 3. çŸ¥è¯†æŒæ¡åº¦åˆ†æ
            knowledge_mastery = await self._analyze_knowledge_mastery(learning_data)

            # 4. å­¦ä¹ æ•ˆç‡è¯„ä¼°
            efficiency_assessment = await self._assess_learning_efficiency(learning_data)

            # 5. AIæ·±åº¦åˆ†æï¼ˆè°ƒç”¨DeepSeek APIï¼‰
            ai_insights = await self._generate_ai_insights(
                learning_data,
                learning_patterns,
                knowledge_mastery,
                efficiency_assessment,
            )

            # 6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            analysis_time = (datetime.now() - start_time).total_seconds()

            comprehensive_report = {
                "analysis_available": True,
                "student_id": student_id,
                "training_type": training_type.name if training_type else "ALL",
                "analysis_timestamp": datetime.now(),
                "analysis_duration": analysis_time,
                "meets_time_requirement": analysis_time < self.analysis_config["analysis_timeout"],
                # æ ¸å¿ƒåˆ†æç»“æœ
                "learning_patterns": learning_patterns,
                "knowledge_mastery": knowledge_mastery,
                "efficiency_assessment": efficiency_assessment,
                "ai_insights": ai_insights,
                # æ•°æ®ç»Ÿè®¡
                "data_summary": {
                    "analysis_period_days": self.analysis_config["analysis_window_days"],
                    "total_sessions": learning_data["session_count"],
                    "total_records": learning_data["record_count"],
                    "data_quality_score": learning_data["data_quality_score"],
                },
                # ç½®ä¿¡åº¦è¯„ä¼°
                "confidence_metrics": {
                    "overall_confidence": self._calculate_overall_confidence(
                        learning_patterns, knowledge_mastery, efficiency_assessment
                    ),
                    "pattern_confidence": learning_patterns.get("confidence", 0),
                    "mastery_confidence": knowledge_mastery.get("confidence", 0),
                    "efficiency_confidence": efficiency_assessment.get("confidence", 0),
                },
            }

            logger.info(f"AIæ·±åº¦åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ: å­¦ç”Ÿ{student_id}, è€—æ—¶{analysis_time:.2f}ç§’")
            return comprehensive_report

        except Exception as e:
            logger.error(f"ç”ŸæˆAIæ·±åº¦åˆ†ææŠ¥å‘Šå¤±è´¥: {str(e)}")
            return {
                "analysis_available": False,
                "error": str(e),
                "timestamp": datetime.now(),
            }

    async def _collect_learning_data(
        self, student_id: int, training_type: TrainingType | None = None
    ) -> dict[str, Any]:
        """æ”¶é›†å­¦ä¹ æ•°æ®."""
        try:
            cutoff_date = datetime.now() - timedelta(
                days=self.analysis_config["analysis_window_days"]
            )

            # æ„å»ºæŸ¥è¯¢
            stmt = (
                select(TrainingRecord, TrainingSession)
                .join(TrainingSession, TrainingRecord.session_id == TrainingSession.id)
                .where(
                    and_(
                        TrainingSession.student_id == student_id,
                        TrainingRecord.created_at >= cutoff_date,
                    )
                )
            )

            if training_type:
                stmt = stmt.where(TrainingSession.session_type == training_type)

            stmt = stmt.order_by(desc(TrainingRecord.created_at)).limit(
                self.analysis_config["max_data_points"]
            )

            result = await self.db.execute(stmt)
            data = result.all()

            if not data:
                return {"has_sufficient_data": False, "reason": "æ— å­¦ä¹ è®°å½•"}

            # åˆ†ç¦»è®°å½•å’Œä¼šè¯
            records = [item[0] for item in data]
            sessions = [item[1] for item in data]
            unique_sessions = list({session.id: session for session in sessions}.values())

            # æ£€æŸ¥æ•°æ®å……åˆ†æ€§
            has_sufficient_data = (
                len(unique_sessions)
                >= self.analysis_config["pattern_recognition"]["min_sessions_for_pattern"]
                and len(records) >= 20
            )

            # è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°
            data_quality_score = self._calculate_data_quality_score(records, unique_sessions)

            return {
                "has_sufficient_data": has_sufficient_data,
                "records": records,
                "sessions": unique_sessions,
                "session_count": len(unique_sessions),
                "record_count": len(records),
                "data_quality_score": data_quality_score,
                "analysis_period": {
                    "start_date": cutoff_date,
                    "end_date": datetime.now(),
                    "days": self.analysis_config["analysis_window_days"],
                },
            }

        except Exception as e:
            logger.error(f"æ”¶é›†å­¦ä¹ æ•°æ®å¤±è´¥: {str(e)}")
            return {"has_sufficient_data": False, "error": str(e)}

    async def _analyze_learning_patterns(self, learning_data: dict[str, Any]) -> dict[str, Any]:
        """åˆ†æå­¦ä¹ æ¨¡å¼."""
        try:
            records = learning_data["records"]
            sessions = learning_data["sessions"]

            # 1. å­¦ä¹ æ—¶é—´æ¨¡å¼åˆ†æ
            time_patterns = self._analyze_time_patterns(records, sessions)

            # 2. ç­”é¢˜è¡Œä¸ºæ¨¡å¼åˆ†æ
            behavior_patterns = self._analyze_behavior_patterns(records)

            # 3. éš¾åº¦é€‚åº”æ¨¡å¼åˆ†æ
            difficulty_patterns = self._analyze_difficulty_adaptation_patterns(sessions, records)

            # 4. å­¦ä¹ é£æ ¼è¯†åˆ«
            learning_style = self._identify_learning_style(records, sessions)

            # 5. è®¡ç®—æ¨¡å¼è¯†åˆ«ç½®ä¿¡åº¦
            pattern_confidence = self._calculate_pattern_confidence(
                time_patterns, behavior_patterns, difficulty_patterns, learning_style
            )

            return {
                "time_patterns": time_patterns,
                "behavior_patterns": behavior_patterns,
                "difficulty_patterns": difficulty_patterns,
                "learning_style": learning_style,
                "confidence": pattern_confidence,
                "pattern_summary": self._generate_pattern_summary(
                    time_patterns,
                    behavior_patterns,
                    difficulty_patterns,
                    learning_style,
                ),
            }

        except Exception as e:
            logger.error(f"å­¦ä¹ æ¨¡å¼åˆ†æå¤±è´¥: {str(e)}")
            return {"error": str(e), "confidence": 0}

    async def _analyze_knowledge_mastery(self, learning_data: dict[str, Any]) -> dict[str, Any]:
        """åˆ†æçŸ¥è¯†æŒæ¡åº¦."""
        try:
            records = learning_data["records"]

            # 1. æŒ‰çŸ¥è¯†ç‚¹åˆ†ç»„åˆ†æ
            knowledge_point_analysis = self._analyze_by_knowledge_points(records)

            # 2. æŒæ¡åº¦ç­‰çº§åˆ†ç±»
            mastery_levels = self._classify_mastery_levels(knowledge_point_analysis)

            # 3. çŸ¥è¯†é—å¿˜åˆ†æ
            retention_analysis = self._analyze_knowledge_retention(records)

            # 4. å­¦ä¹ è¿›åº¦è¯„ä¼°
            progress_assessment = self._assess_learning_progress(records)

            # 5. è–„å¼±ç¯èŠ‚è¯†åˆ«
            weak_areas = self._identify_weak_areas(knowledge_point_analysis)

            # 6. è®¡ç®—æŒæ¡åº¦ç½®ä¿¡åº¦
            mastery_confidence = self._calculate_mastery_confidence(
                knowledge_point_analysis, retention_analysis, progress_assessment
            )

            return {
                "knowledge_point_analysis": knowledge_point_analysis,
                "mastery_levels": mastery_levels,
                "retention_analysis": retention_analysis,
                "progress_assessment": progress_assessment,
                "weak_areas": weak_areas,
                "confidence": mastery_confidence,
                "overall_mastery_score": self._calculate_overall_mastery_score(mastery_levels),
            }

        except Exception as e:
            logger.error(f"çŸ¥è¯†æŒæ¡åº¦åˆ†æå¤±è´¥: {str(e)}")
            return {"error": str(e), "confidence": 0}

    async def _assess_learning_efficiency(self, learning_data: dict[str, Any]) -> dict[str, Any]:
        """è¯„ä¼°å­¦ä¹ æ•ˆç‡."""
        try:
            records = learning_data["records"]
            sessions = learning_data["sessions"]

            # 1. å‡†ç¡®ç‡æ•ˆç‡åˆ†æ
            accuracy_efficiency = self._analyze_accuracy_efficiency(records)

            # 2. é€Ÿåº¦æ•ˆç‡åˆ†æ
            speed_efficiency = self._analyze_speed_efficiency(records)

            # 3. ä¸€è‡´æ€§åˆ†æ
            consistency_analysis = self._analyze_learning_consistency(records)

            # 4. è¿›æ­¥é€Ÿåº¦åˆ†æ
            progress_rate = self._analyze_progress_rate(records, sessions)

            # 5. ç»¼åˆæ•ˆç‡è¯„åˆ†
            efficiency_factors = self.analysis_config["efficiency_assessment"]["efficiency_factors"]
            overall_efficiency = (
                accuracy_efficiency["score"] * efficiency_factors["accuracy_weight"]
                + speed_efficiency["score"] * efficiency_factors["speed_weight"]
                + consistency_analysis["score"] * efficiency_factors["consistency_weight"]
                + progress_rate["score"] * efficiency_factors["progress_weight"]
            )

            # 6. æ•ˆç‡ç­‰çº§è¯„å®š
            efficiency_level = self._determine_efficiency_level(overall_efficiency)

            # 7. æ”¹è¿›å»ºè®®
            improvement_suggestions = self._generate_efficiency_improvement_suggestions(
                accuracy_efficiency,
                speed_efficiency,
                consistency_analysis,
                progress_rate,
            )

            return {
                "accuracy_efficiency": accuracy_efficiency,
                "speed_efficiency": speed_efficiency,
                "consistency_analysis": consistency_analysis,
                "progress_rate": progress_rate,
                "overall_efficiency": overall_efficiency,
                "efficiency_level": efficiency_level,
                "improvement_suggestions": improvement_suggestions,
                "confidence": min(0.9, overall_efficiency),  # æ•ˆç‡åˆ†æ•°ä½œä¸ºç½®ä¿¡åº¦
            }

        except Exception as e:
            logger.error(f"å­¦ä¹ æ•ˆç‡è¯„ä¼°å¤±è´¥: {str(e)}")
            return {"error": str(e), "confidence": 0}

    async def _generate_ai_insights(
        self,
        learning_data: dict[str, Any],
        patterns: dict[str, Any],
        mastery: dict[str, Any],
        efficiency: dict[str, Any],
    ) -> dict[str, Any]:
        """ç”ŸæˆAIæ·±åº¦æ´å¯Ÿ."""
        try:
            # å‡†å¤‡AIåˆ†æçš„æ•°æ®æ‘˜è¦
            data_summary = {
                "student_profile": {
                    "total_sessions": learning_data["session_count"],
                    "total_records": learning_data["record_count"],
                    "data_quality": learning_data["data_quality_score"],
                    "analysis_period": learning_data["analysis_period"]["days"],
                },
                "learning_patterns": {
                    "learning_style": patterns.get("learning_style", {}),
                    "time_patterns": patterns.get("time_patterns", {}),
                    "behavior_patterns": patterns.get("behavior_patterns", {}),
                },
                "knowledge_mastery": {
                    "overall_score": mastery.get("overall_mastery_score", 0),
                    "weak_areas": mastery.get("weak_areas", []),
                    "progress_rate": mastery.get("progress_assessment", {}),
                },
                "efficiency_metrics": {
                    "overall_efficiency": efficiency.get("overall_efficiency", 0),
                    "efficiency_level": efficiency.get("efficiency_level", "unknown"),
                    "main_bottlenecks": efficiency.get("improvement_suggestions", []),
                },
            }

            # è°ƒç”¨AIåˆ†æAPI
            ai_analysis = await self._call_deepseek_analysis_api(data_summary)

            return {
                "ai_generated_insights": ai_analysis.get("insights", []),
                "personalized_recommendations": ai_analysis.get("recommendations", []),
                "learning_trajectory_prediction": ai_analysis.get("predictions", {}),
                "confidence_score": ai_analysis.get("confidence", 0.8),
                "analysis_method": "deepseek_ai",
                "api_response_time": ai_analysis.get("response_time", 0),
            }

        except Exception as e:
            logger.error(f"ç”ŸæˆAIæ´å¯Ÿå¤±è´¥: {str(e)}")
            # è¿”å›åŸºäºè§„åˆ™çš„å¤‡ç”¨åˆ†æ
            return await self._generate_fallback_insights(
                learning_data, patterns, mastery, efficiency
            )

    async def _call_deepseek_analysis_api(self, data_summary: dict[str, Any]) -> dict[str, Any]:
        """è°ƒç”¨DeepSeek AIåˆ†æAPI."""
        try:
            start_time = datetime.now()

            # æ„å»ºAIåˆ†ææç¤º
            analysis_prompt = self._build_analysis_prompt(data_summary)

            # è°ƒç”¨DeepSeek API
            async with httpx.AsyncClient(
                timeout=self.analysis_config["analysis_timeout"]
            ) as client:
                response = await client.post(
                    "https://api.deepseek.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {settings.DEEPSEEK_API_KEY}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": self.analysis_config["ai_model"],
                        "messages": [
                            {
                                "role": "system",
                                "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦ä¹ åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ†æå­¦ç”Ÿçš„å­¦ä¹ æ•°æ®å¹¶æä¾›ä¸ªæ€§åŒ–å»ºè®®ã€‚",
                            },
                            {"role": "user", "content": analysis_prompt},
                        ],
                        "temperature": 0.7,
                        "max_tokens": 1000,
                    },
                )

            response_time = (datetime.now() - start_time).total_seconds()

            if response.status_code == 200:
                ai_response = response.json()
                content = ai_response["choices"][0]["message"]["content"]

                # è§£æAIå“åº”
                parsed_analysis = self._parse_ai_response(content)
                parsed_analysis["response_time"] = response_time

                return parsed_analysis
            else:
                logger.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                return {"error": "APIè°ƒç”¨å¤±è´¥", "response_time": response_time}

        except Exception as e:
            logger.error(f"DeepSeek APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return {"error": str(e), "response_time": 0}

    def _build_analysis_prompt(self, data_summary: dict[str, Any]) -> str:
        """æ„å»ºAIåˆ†ææç¤º."""
        prompt = f"""
è¯·åŸºäºä»¥ä¸‹å­¦ä¹ æ•°æ®è¿›è¡Œæ·±åº¦åˆ†æï¼š

å­¦ç”Ÿæ¦‚å†µï¼š
- å­¦ä¹ ä¼šè¯æ•°ï¼š{data_summary["student_profile"]["total_sessions"]}
- ç­”é¢˜è®°å½•æ•°ï¼š{data_summary["student_profile"]["total_records"]}
- æ•°æ®è´¨é‡åˆ†æ•°ï¼š{data_summary["student_profile"]["data_quality"]:.2f}
- åˆ†æå‘¨æœŸï¼š{data_summary["student_profile"]["analysis_period"]}å¤©

å­¦ä¹ æ¨¡å¼ï¼š
- å­¦ä¹ é£æ ¼ï¼š{data_summary["learning_patterns"]["learning_style"]}
- æ—¶é—´æ¨¡å¼ï¼š{data_summary["learning_patterns"]["time_patterns"]}
- è¡Œä¸ºæ¨¡å¼ï¼š{data_summary["learning_patterns"]["behavior_patterns"]}

çŸ¥è¯†æŒæ¡åº¦ï¼š
- æ•´ä½“æŒæ¡åˆ†æ•°ï¼š{data_summary["knowledge_mastery"]["overall_score"]:.2f}
- è–„å¼±ç¯èŠ‚ï¼š{data_summary["knowledge_mastery"]["weak_areas"]}
- è¿›æ­¥æƒ…å†µï¼š{data_summary["knowledge_mastery"]["progress_rate"]}

å­¦ä¹ æ•ˆç‡ï¼š
- æ•´ä½“æ•ˆç‡ï¼š{data_summary["efficiency_metrics"]["overall_efficiency"]:.2f}
- æ•ˆç‡ç­‰çº§ï¼š{data_summary["efficiency_metrics"]["efficiency_level"]}
- ä¸»è¦ç“¶é¢ˆï¼š{data_summary["efficiency_metrics"]["main_bottlenecks"]}

è¯·æä¾›ï¼š
1. æ·±åº¦å­¦ä¹ æ´å¯Ÿï¼ˆ3-5ä¸ªå…³é”®å‘ç°ï¼‰
2. ä¸ªæ€§åŒ–æ”¹è¿›å»ºè®®ï¼ˆå…·ä½“å¯æ‰§è¡Œçš„å»ºè®®ï¼‰
3. å­¦ä¹ è½¨è¿¹é¢„æµ‹ï¼ˆæœªæ¥å‘å±•è¶‹åŠ¿ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«insightsã€recommendationsã€predictionsä¸‰ä¸ªå­—æ®µã€‚
"""
        return prompt

    def _parse_ai_response(self, content: str) -> dict[str, Any]:
        """è§£æAIå“åº”å†…å®¹."""
        try:
            # å°è¯•è§£æJSON
            if content.strip().startswith("{"):
                parsed_content: dict[str, Any] = json.loads(content)
                return parsed_content

            # å¦‚æœä¸æ˜¯JSONï¼Œè¿›è¡Œæ–‡æœ¬è§£æ
            insights: list[str] = []
            recommendations: list[str] = []
            predictions: dict[str, Any] = {}

            lines = content.split("\n")
            current_section = None

            for line in lines:
                line = line.strip()
                if "æ´å¯Ÿ" in line or "insights" in line.lower():
                    current_section = "insights"
                elif "å»ºè®®" in line or "recommendations" in line.lower():
                    current_section = "recommendations"
                elif "é¢„æµ‹" in line or "predictions" in line.lower():
                    current_section = "predictions"
                elif line and line.startswith(("-", "â€¢", "1.", "2.", "3.")):
                    if current_section == "insights":
                        insights.append(line.lstrip("-â€¢123. "))
                    elif current_section == "recommendations":
                        recommendations.append(line.lstrip("-â€¢123. "))

            return {
                "insights": insights,
                "recommendations": recommendations,
                "predictions": predictions,
                "confidence": 0.8,
            }

        except Exception as e:
            logger.error(f"è§£æAIå“åº”å¤±è´¥: {str(e)}")
            return {
                "insights": ["AIåˆ†æå“åº”è§£æå¤±è´¥"],
                "recommendations": ["å»ºè®®é‡æ–°è¿›è¡Œåˆ†æ"],
                "predictions": {},
                "confidence": 0.3,
            }

    async def _generate_fallback_insights(
        self,
        learning_data: dict[str, Any],
        patterns: dict[str, Any],
        mastery: dict[str, Any],
        efficiency: dict[str, Any],
    ) -> dict[str, Any]:
        """ç”Ÿæˆå¤‡ç”¨æ´å¯Ÿï¼ˆåŸºäºè§„åˆ™ï¼‰."""
        insights = []
        recommendations = []

        # åŸºäºæ•ˆç‡ç”Ÿæˆæ´å¯Ÿ
        overall_efficiency = efficiency.get("overall_efficiency", 0)
        if overall_efficiency > 0.8:
            insights.append("å­¦ä¹ æ•ˆç‡ä¼˜ç§€ï¼Œä¿æŒå½“å‰å­¦ä¹ èŠ‚å¥")
        elif overall_efficiency > 0.6:
            insights.append("å­¦ä¹ æ•ˆç‡è‰¯å¥½ï¼Œæœ‰è¿›ä¸€æ­¥æå‡ç©ºé—´")
        else:
            insights.append("å­¦ä¹ æ•ˆç‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®è°ƒæ•´å­¦ä¹ ç­–ç•¥")

        # åŸºäºæŒæ¡åº¦ç”Ÿæˆæ´å¯Ÿ
        mastery_score = mastery.get("overall_mastery_score", 0)
        if mastery_score > 0.85:
            insights.append("çŸ¥è¯†æŒæ¡åº¦å¾ˆé«˜ï¼Œå¯ä»¥å°è¯•æ›´é«˜éš¾åº¦å†…å®¹")
        elif mastery_score > 0.7:
            insights.append("çŸ¥è¯†æŒæ¡åº¦è‰¯å¥½ï¼Œç»§ç»­å·©å›ºè–„å¼±ç¯èŠ‚")
        else:
            insights.append("çŸ¥è¯†æŒæ¡åº¦æœ‰å¾…æé«˜ï¼Œéœ€è¦åŠ å¼ºåŸºç¡€ç»ƒä¹ ")

        # ç”ŸæˆåŸºç¡€å»ºè®®
        weak_areas = mastery.get("weak_areas", [])
        if weak_areas:
            recommendations.append(f"é‡ç‚¹å…³æ³¨è–„å¼±çŸ¥è¯†ç‚¹ï¼š{', '.join(weak_areas[:3])}")

        efficiency_level = efficiency.get("efficiency_level", "unknown")
        if efficiency_level == "low":
            recommendations.append("å»ºè®®è°ƒæ•´å­¦ä¹ æ—¶é—´å®‰æ’ï¼Œæé«˜ä¸“æ³¨åº¦")

        return {
            "ai_generated_insights": insights,
            "personalized_recommendations": recommendations,
            "learning_trajectory_prediction": {"trend": "stable", "confidence": 0.6},
            "confidence_score": 0.6,
            "analysis_method": "rule_based_fallback",
            "api_response_time": 0,
        }

    def _calculate_data_quality_score(self, records: list[Any], sessions: list[Any]) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°."""
        try:
            quality_factors = []

            # æ•°æ®å®Œæ•´æ€§
            complete_records = sum(1 for r in records if r.time_spent and r.time_spent > 0)
            completeness = complete_records / len(records) if records else 0
            quality_factors.append(completeness)

            # æ•°æ®æ—¶é—´åˆ†å¸ƒ
            if len(sessions) > 1:
                session_dates = [s.created_at.date() for s in sessions]
                unique_dates = len(set(session_dates))
                time_distribution = min(1.0, unique_dates / 7)  # ç†æƒ³æƒ…å†µä¸‹7å¤©å†…æœ‰åˆ†å¸ƒ
                quality_factors.append(time_distribution)

            # æ•°æ®é‡å……åˆ†æ€§
            volume_score = min(1.0, len(records) / 50)  # 50ä¸ªè®°å½•ä¸ºæ»¡åˆ†
            quality_factors.append(volume_score)

            return sum(quality_factors) / len(quality_factors) if quality_factors else 0

        except Exception as e:
            logger.error(f"è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°å¤±è´¥: {str(e)}")
            return 0.5

    def _calculate_overall_confidence(
        self,
        patterns: dict[str, Any],
        mastery: dict[str, Any],
        efficiency: dict[str, Any],
    ) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦."""
        confidences: list[float] = [
            patterns.get("confidence", 0),
            mastery.get("confidence", 0),
            efficiency.get("confidence", 0),
        ]

        return sum(confidences) / len(confidences) if confidences else 0

    def _analyze_time_patterns(self, records: list[Any], sessions: list[Any]) -> dict[str, Any]:
        """åˆ†æå­¦ä¹ æ—¶é—´æ¨¡å¼."""
        try:
            if not sessions:
                return {
                    "optimal_learning_time": "unknown",
                    "session_duration_pattern": "unknown",
                }

            # åˆ†æå­¦ä¹ æ—¶é—´åå¥½
            hour_counts: dict[int, int] = {}
            for session in sessions:
                hour = session.created_at.hour
                hour_counts[hour] = hour_counts.get(hour, 0) + 1

            # æ‰¾å‡ºæœ€æ´»è·ƒçš„æ—¶é—´æ®µ
            if hour_counts:
                peak_hour = max(hour_counts.keys(), key=lambda x: hour_counts[x])
                if 6 <= peak_hour < 12:
                    optimal_time = "morning"
                elif 12 <= peak_hour < 18:
                    optimal_time = "afternoon"
                elif 18 <= peak_hour < 22:
                    optimal_time = "evening"
                else:
                    optimal_time = "night"
            else:
                optimal_time = "unknown"

            # åˆ†æä¼šè¯æŒç»­æ—¶é—´æ¨¡å¼
            durations: list[float] = []
            for session in sessions:
                if session.end_time and session.start_time:
                    duration = (session.end_time - session.start_time).total_seconds() / 60
                    durations.append(duration)

            if durations:
                avg_duration = sum(durations) / len(durations)
                if avg_duration < 15:
                    duration_pattern = "short_sessions"
                elif avg_duration < 45:
                    duration_pattern = "medium_sessions"
                else:
                    duration_pattern = "long_sessions"
            else:
                duration_pattern = "unknown"

            return {
                "optimal_learning_time": optimal_time,
                "session_duration_pattern": duration_pattern,
                "average_session_duration": (sum(durations) / len(durations) if durations else 0),
                "peak_hour": hour_counts,
            }

        except Exception as e:
            logger.error(f"åˆ†ææ—¶é—´æ¨¡å¼å¤±è´¥: {str(e)}")
            return {
                "optimal_learning_time": "unknown",
                "session_duration_pattern": "unknown",
            }

    def _analyze_behavior_patterns(self, records: list[Any]) -> dict[str, Any]:
        """åˆ†æç­”é¢˜è¡Œä¸ºæ¨¡å¼."""
        try:
            if not records:
                return {
                    "answer_speed_pattern": "unknown",
                    "accuracy_pattern": "unknown",
                }

            # åˆ†æç­”é¢˜é€Ÿåº¦æ¨¡å¼
            answer_times: list[float] = [
                r.time_spent for r in records if r.time_spent and r.time_spent > 0
            ]
            if answer_times:
                avg_time = sum(answer_times) / len(answer_times)
                if avg_time < 30:
                    speed_pattern = "fast_answerer"
                elif avg_time < 90:
                    speed_pattern = "moderate_answerer"
                else:
                    speed_pattern = "careful_answerer"
            else:
                speed_pattern = "unknown"

            # åˆ†ææ­£ç¡®ç‡æ¨¡å¼
            correct_count = sum(1 for r in records if r.is_correct)
            accuracy = correct_count / len(records) if records else 0

            if accuracy > 0.85:
                accuracy_pattern = "high_achiever"
            elif accuracy > 0.7:
                accuracy_pattern = "steady_learner"
            elif accuracy > 0.5:
                accuracy_pattern = "developing_learner"
            else:
                accuracy_pattern = "struggling_learner"

            # åˆ†æç­”é¢˜ä¸€è‡´æ€§
            if len(answer_times) > 1:
                import statistics

                time_std = statistics.stdev(answer_times)
                consistency = 1 - min(1.0, time_std / avg_time) if avg_time > 0 else 0
            else:
                consistency = 0

            return {
                "answer_speed_pattern": speed_pattern,
                "accuracy_pattern": accuracy_pattern,
                "consistency_score": consistency,
                "average_answer_time": (
                    sum(answer_times) / len(answer_times) if answer_times else 0
                ),
                "accuracy_rate": accuracy,
            }

        except Exception as e:
            logger.error(f"åˆ†æè¡Œä¸ºæ¨¡å¼å¤±è´¥: {str(e)}")
            return {"answer_speed_pattern": "unknown", "accuracy_pattern": "unknown"}

    def _analyze_difficulty_adaptation_patterns(
        self, sessions: list[Any], records: list[Any]
    ) -> dict[str, Any]:
        """åˆ†æéš¾åº¦é€‚åº”æ¨¡å¼."""
        try:
            if not sessions:
                return {
                    "adaptation_ability": "unknown",
                    "preferred_difficulty": "unknown",
                }

            # æŒ‰éš¾åº¦åˆ†ç»„ç»Ÿè®¡
            difficulty_stats = {}
            for session in sessions:
                difficulty = session.difficulty_level
                if difficulty not in difficulty_stats:
                    difficulty_stats[difficulty] = {
                        "sessions": 0,
                        "total_correct": 0,
                        "total_questions": 0,
                    }

                difficulty_stats[difficulty]["sessions"] += 1

                # ç»Ÿè®¡è¯¥ä¼šè¯çš„ç­”é¢˜æƒ…å†µ
                session_records = [r for r in records if r.session_id == session.id]
                difficulty_stats[difficulty]["total_questions"] += len(session_records)
                difficulty_stats[difficulty]["total_correct"] += sum(
                    1 for r in session_records if r.is_correct
                )

            # è®¡ç®—å„éš¾åº¦çš„æ­£ç¡®ç‡
            difficulty_accuracies = {}
            for difficulty, stats in difficulty_stats.items():
                if stats["total_questions"] > 0:
                    accuracy = stats["total_correct"] / stats["total_questions"]
                    difficulty_accuracies[difficulty] = accuracy

            # ç¡®å®šé€‚åº”èƒ½åŠ›
            if len(difficulty_accuracies) >= 2:
                accuracies: list[float] = list(difficulty_accuracies.values())
                if min(accuracies) > 0.7:
                    adaptation_ability = "excellent"
                elif min(accuracies) > 0.5:
                    adaptation_ability = "good"
                else:
                    adaptation_ability = "needs_improvement"
            else:
                adaptation_ability = "insufficient_data"

            # ç¡®å®šåå¥½éš¾åº¦
            if difficulty_accuracies:
                preferred_difficulty = max(
                    difficulty_accuracies.keys(), key=lambda x: difficulty_accuracies[x]
                )
            else:
                preferred_difficulty = "elementary"

            return {
                "adaptation_ability": adaptation_ability,
                "preferred_difficulty": preferred_difficulty,
                "difficulty_accuracies": difficulty_accuracies,
                "difficulty_stats": difficulty_stats,
            }

        except Exception as e:
            logger.error(f"åˆ†æéš¾åº¦é€‚åº”æ¨¡å¼å¤±è´¥: {str(e)}")
            return {"adaptation_ability": "unknown", "preferred_difficulty": "unknown"}

    def _identify_learning_style(self, records: list[Any], sessions: list[Any]) -> dict[str, Any]:
        """è¯†åˆ«å­¦ä¹ é£æ ¼."""
        try:
            if not records or not sessions:
                return {"primary_style": "unknown", "confidence": 0}

            # åˆ†æç­”é¢˜é€Ÿåº¦ç‰¹å¾
            answer_times = [r.time_spent for r in records if r.time_spent and r.time_spent > 0]
            avg_time = sum(answer_times) / len(answer_times) if answer_times else 60

            # åˆ†æä¼šè¯æ—¶é•¿ç‰¹å¾
            session_durations: list[float] = []
            for session in sessions:
                if session.end_time and session.start_time:
                    duration = (session.end_time - session.start_time).total_seconds() / 60
                    session_durations.append(duration)

            avg_session_duration = (
                sum(session_durations) / len(session_durations) if session_durations else 30
            )

            # åˆ†ææ­£ç¡®ç‡å˜åŒ–ç‰¹å¾
            correct_count = sum(1 for r in records if r.is_correct)
            accuracy = correct_count / len(records) if records else 0

            # åŸºäºç‰¹å¾åˆ¤æ–­å­¦ä¹ é£æ ¼
            style_scores = {
                "visual_learner": 0,
                "auditory_learner": 0,
                "kinesthetic_learner": 0,
                "sequential_learner": 0,
                "global_learner": 0,
                "analytical_learner": 0,
            }

            # è§†è§‰å­¦ä¹ è€…ç‰¹å¾ï¼šå¿«é€Ÿç­”é¢˜ï¼ŒçŸ­ä¼šè¯
            if avg_time < 45 and avg_session_duration < 30:
                style_scores["visual_learner"] += 0.3

            # å¬è§‰å­¦ä¹ è€…ç‰¹å¾ï¼šä¸­ç­‰ç­”é¢˜é€Ÿåº¦ï¼Œä¸­ç­‰ä¼šè¯æ—¶é•¿
            if 45 <= avg_time <= 90 and 30 <= avg_session_duration <= 60:
                style_scores["auditory_learner"] += 0.3

            # åŠ¨è§‰å­¦ä¹ è€…ç‰¹å¾ï¼šè¾ƒæ…¢ç­”é¢˜ï¼Œé•¿ä¼šè¯
            if avg_time > 90 and avg_session_duration > 60:
                style_scores["kinesthetic_learner"] += 0.3

            # åºåˆ—å­¦ä¹ è€…ç‰¹å¾ï¼šç¨³å®šçš„æ­£ç¡®ç‡
            if 0.7 <= accuracy <= 0.85:
                style_scores["sequential_learner"] += 0.2

            # æ•´ä½“å­¦ä¹ è€…ç‰¹å¾ï¼šé«˜æ­£ç¡®ç‡æˆ–ä½æ­£ç¡®ç‡ï¼ˆä¸¤æåŒ–ï¼‰
            if accuracy > 0.85 or accuracy < 0.5:
                style_scores["global_learner"] += 0.2

            # åˆ†æå­¦ä¹ è€…ç‰¹å¾ï¼šé«˜æ­£ç¡®ç‡ï¼Œå¿«é€Ÿç­”é¢˜
            if accuracy > 0.8 and avg_time < 60:
                style_scores["analytical_learner"] += 0.3

            # ç¡®å®šä¸»è¦å­¦ä¹ é£æ ¼
            primary_style = max(style_scores.keys(), key=lambda x: style_scores[x])
            confidence = style_scores[primary_style]

            return {
                "primary_style": primary_style,
                "confidence": confidence,
                "style_scores": style_scores,
                "characteristics": {
                    "average_answer_time": avg_time,
                    "average_session_duration": avg_session_duration,
                    "accuracy_rate": accuracy,
                },
            }

        except Exception as e:
            logger.error(f"è¯†åˆ«å­¦ä¹ é£æ ¼å¤±è´¥: {str(e)}")
            return {"primary_style": "unknown", "confidence": 0}

    def _calculate_pattern_confidence(
        self,
        time_patterns: dict[str, Any],
        behavior_patterns: dict[str, Any],
        difficulty_patterns: dict[str, Any],
        learning_style: dict[str, Any],
    ) -> float:
        """è®¡ç®—æ¨¡å¼è¯†åˆ«ç½®ä¿¡åº¦."""
        try:
            confidences = []

            # æ—¶é—´æ¨¡å¼ç½®ä¿¡åº¦
            if time_patterns.get("optimal_learning_time") != "unknown":
                confidences.append(0.8)
            else:
                confidences.append(0.3)

            # è¡Œä¸ºæ¨¡å¼ç½®ä¿¡åº¦
            if behavior_patterns.get("answer_speed_pattern") != "unknown":
                confidences.append(0.8)
            else:
                confidences.append(0.3)

            # éš¾åº¦é€‚åº”ç½®ä¿¡åº¦
            if difficulty_patterns.get("adaptation_ability") != "unknown":
                confidences.append(0.8)
            else:
                confidences.append(0.3)

            # å­¦ä¹ é£æ ¼ç½®ä¿¡åº¦
            style_confidence = learning_style.get("confidence", 0)
            confidences.append(style_confidence)

            return sum(confidences) / len(confidences) if confidences else 0

        except Exception as e:
            logger.error(f"è®¡ç®—æ¨¡å¼ç½®ä¿¡åº¦å¤±è´¥: {str(e)}")
            return 0.5

    def _generate_pattern_summary(
        self,
        time_patterns: dict[str, Any],
        behavior_patterns: dict[str, Any],
        difficulty_patterns: dict[str, Any],
        learning_style: dict[str, Any],
    ) -> str:
        """ç”Ÿæˆæ¨¡å¼æ€»ç»“."""
        try:
            summary_parts = []

            # å­¦ä¹ é£æ ¼æ€»ç»“
            primary_style = learning_style.get("primary_style", "unknown")
            if primary_style != "unknown":
                style_names = {
                    "visual_learner": "è§†è§‰å­¦ä¹ è€…",
                    "auditory_learner": "å¬è§‰å­¦ä¹ è€…",
                    "kinesthetic_learner": "åŠ¨è§‰å­¦ä¹ è€…",
                    "sequential_learner": "åºåˆ—å­¦ä¹ è€…",
                    "global_learner": "æ•´ä½“å­¦ä¹ è€…",
                    "analytical_learner": "åˆ†æå­¦ä¹ è€…",
                }
                summary_parts.append(
                    f"ä¸»è¦å­¦ä¹ é£æ ¼ï¼š{style_names.get(primary_style, primary_style)}"
                )

            # æ—¶é—´åå¥½æ€»ç»“
            optimal_time = time_patterns.get("optimal_learning_time", "unknown")
            if optimal_time != "unknown":
                time_names = {
                    "morning": "ä¸Šåˆ",
                    "afternoon": "ä¸‹åˆ",
                    "evening": "æ™šä¸Š",
                    "night": "æ·±å¤œ",
                }
                summary_parts.append(f"æœ€ä½³å­¦ä¹ æ—¶é—´ï¼š{time_names.get(optimal_time, optimal_time)}")

            # ç­”é¢˜ç‰¹å¾æ€»ç»“
            speed_pattern = behavior_patterns.get("answer_speed_pattern", "unknown")
            if speed_pattern != "unknown":
                speed_names = {
                    "fast_answerer": "å¿«é€Ÿç­”é¢˜å‹",
                    "moderate_answerer": "ç¨³å¥ç­”é¢˜å‹",
                    "careful_answerer": "è°¨æ…ç­”é¢˜å‹",
                }
                summary_parts.append(f"ç­”é¢˜ç‰¹å¾ï¼š{speed_names.get(speed_pattern, speed_pattern)}")

            # éš¾åº¦é€‚åº”æ€»ç»“
            adaptation = difficulty_patterns.get("adaptation_ability", "unknown")
            if adaptation != "unknown":
                adaptation_names = {
                    "excellent": "ä¼˜ç§€",
                    "good": "è‰¯å¥½",
                    "needs_improvement": "éœ€è¦æ”¹è¿›",
                }
                summary_parts.append(
                    f"éš¾åº¦é€‚åº”èƒ½åŠ›ï¼š{adaptation_names.get(adaptation, adaptation)}"
                )

            return "ï¼›".join(summary_parts) if summary_parts else "å­¦ä¹ æ¨¡å¼ç‰¹å¾ä¸æ˜æ˜¾"

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ¨¡å¼æ€»ç»“å¤±è´¥: {str(e)}")
            return "æ¨¡å¼åˆ†æå¼‚å¸¸"

    def _analyze_by_knowledge_points(self, records: list[Any]) -> dict[str, Any]:
        """æŒ‰çŸ¥è¯†ç‚¹åˆ†ç»„åˆ†æ."""
        try:
            if not records:
                return {}

            # æŒ‰çŸ¥è¯†ç‚¹åˆ†ç»„ï¼ˆè¿™é‡Œå‡è®¾æœ‰knowledge_pointå­—æ®µï¼Œå®é™…å¯èƒ½éœ€è¦æ ¹æ®é¢˜ç›®ç±»å‹æ¨æ–­ï¼‰
            knowledge_points: dict[str, dict[str, Any]] = {}

            for record in records:
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®é¢˜ç›®å†…å®¹æˆ–æ ‡ç­¾ç¡®å®šçŸ¥è¯†ç‚¹
                point = getattr(record, "knowledge_point", "general")
                if point not in knowledge_points:
                    knowledge_points[point] = {
                        "total_attempts": 0,
                        "correct_attempts": 0,
                        "total_time": 0,
                        "attempts": [],
                    }

                point_data = knowledge_points[point]
                point_data["total_attempts"] += 1
                if record.is_correct:
                    point_data["correct_attempts"] += 1

                if record.time_spent:
                    point_data["total_time"] += record.time_spent

                point_data["attempts"].append(
                    {
                        "is_correct": record.is_correct,
                        "time_spent": record.time_spent,
                        "created_at": record.created_at,
                    }
                )

            # è®¡ç®—æ¯ä¸ªçŸ¥è¯†ç‚¹çš„ç»Ÿè®¡ä¿¡æ¯
            for _point, data in knowledge_points.items():
                data["accuracy"] = (
                    data["correct_attempts"] / data["total_attempts"]
                    if data["total_attempts"] > 0
                    else 0
                )
                data["average_time"] = (
                    data["total_time"] / data["total_attempts"] if data["total_attempts"] > 0 else 0
                )

                # è®¡ç®—æŒæ¡åº¦
                if data["accuracy"] >= 0.85:
                    data["mastery_level"] = "mastered"
                elif data["accuracy"] >= 0.7:
                    data["mastery_level"] = "proficient"
                elif data["accuracy"] >= 0.5:
                    data["mastery_level"] = "developing"
                else:
                    data["mastery_level"] = "struggling"

            return knowledge_points

        except Exception as e:
            logger.error(f"æŒ‰çŸ¥è¯†ç‚¹åˆ†æå¤±è´¥: {str(e)}")
            return {}

    def _classify_mastery_levels(self, knowledge_point_analysis: dict[str, Any]) -> dict[str, Any]:
        """åˆ†ç±»æŒæ¡åº¦ç­‰çº§."""
        try:
            mastery_classification: dict[str, list[dict[str, Any]]] = {
                "mastered": [],
                "proficient": [],
                "developing": [],
                "struggling": [],
            }

            for point, data in knowledge_point_analysis.items():
                level = data.get("mastery_level", "struggling")
                mastery_classification[level].append(
                    {
                        "knowledge_point": point,
                        "accuracy": data.get("accuracy", 0),
                        "attempts": data.get("total_attempts", 0),
                        "average_time": data.get("average_time", 0),
                    }
                )

            # è®¡ç®—å„ç­‰çº§çš„ç»Ÿè®¡ä¿¡æ¯
            total_points = len(knowledge_point_analysis)
            mastery_stats = {}
            for level, points in mastery_classification.items():
                count = len(points)
                percentage = count / total_points if total_points > 0 else 0
                mastery_stats[level] = {
                    "count": count,
                    "percentage": percentage,
                    "points": points,
                }

            return {
                "classification": mastery_classification,
                "statistics": mastery_stats,
                "total_knowledge_points": total_points,
            }

        except Exception as e:
            logger.error(f"åˆ†ç±»æŒæ¡åº¦ç­‰çº§å¤±è´¥: {str(e)}")
            return {}

    def _analyze_knowledge_retention(self, records: list[Any]) -> dict[str, Any]:
        """åˆ†æçŸ¥è¯†ä¿æŒæƒ…å†µ."""
        try:
            if not records:
                return {"retention_score": 0, "decay_analysis": {}}

            # æŒ‰æ—¶é—´æ’åº
            sorted_records = sorted(records, key=lambda x: x.created_at)

            # åˆ†æçŸ¥è¯†é—å¿˜æ›²çº¿
            retention_data: dict[str, list[dict[str, Any]]] = {}
            decay_days = self.analysis_config["knowledge_mastery"]["knowledge_decay_days"]

            # æŒ‰çŸ¥è¯†ç‚¹åˆ†æä¿æŒæƒ…å†µ
            for record in sorted_records:
                point = getattr(record, "knowledge_point", "general")
                if point not in retention_data:
                    retention_data[point] = []

                retention_data[point].append(
                    {
                        "date": record.created_at.date(),
                        "is_correct": record.is_correct,
                        "time_spent": record.time_spent,
                    }
                )

            # è®¡ç®—ä¿æŒåˆ†æ•°
            retention_scores = {}
            for point, data in retention_data.items():
                if len(data) < 2:
                    retention_scores[point] = 0.5  # æ•°æ®ä¸è¶³
                    continue

                # è®¡ç®—æœ€è¿‘è¡¨ç°ä¸æ—©æœŸè¡¨ç°çš„å¯¹æ¯”
                recent_data = [
                    d for d in data if (datetime.now().date() - d["date"]).days <= decay_days
                ]
                early_data = [
                    d for d in data if (datetime.now().date() - d["date"]).days > decay_days
                ]

                if not recent_data or not early_data:
                    retention_scores[point] = 0.5
                    continue

                recent_accuracy = sum(1 for d in recent_data if d["is_correct"]) / len(recent_data)
                early_accuracy = sum(1 for d in early_data if d["is_correct"]) / len(early_data)

                # ä¿æŒåˆ†æ•° = æœ€è¿‘æ­£ç¡®ç‡ / æ—©æœŸæ­£ç¡®ç‡
                retention_scores[point] = (
                    recent_accuracy / early_accuracy if early_accuracy > 0 else 0.5
                )

            # è®¡ç®—æ•´ä½“ä¿æŒåˆ†æ•°
            overall_retention = (
                sum(retention_scores.values()) / len(retention_scores) if retention_scores else 0
            )

            return {
                "retention_score": overall_retention,
                "knowledge_point_retention": retention_scores,
                "decay_analysis": {
                    "decay_period_days": decay_days,
                    "points_with_decay": [k for k, v in retention_scores.items() if v < 0.8],
                    "points_well_retained": [k for k, v in retention_scores.items() if v >= 0.8],
                },
            }

        except Exception as e:
            logger.error(f"åˆ†æçŸ¥è¯†ä¿æŒå¤±è´¥: {str(e)}")
            return {"retention_score": 0, "decay_analysis": {}}

    def _assess_learning_progress(self, records: list[Any]) -> dict[str, Any]:
        """è¯„ä¼°å­¦ä¹ è¿›åº¦."""
        try:
            if not records:
                return {"progress_rate": 0, "trend": "unknown"}

            # æŒ‰æ—¶é—´æ’åº
            sorted_records = sorted(records, key=lambda x: x.created_at)

            # åˆ†æ—¶é—´æ®µåˆ†æè¿›æ­¥æƒ…å†µ
            total_days = (sorted_records[-1].created_at - sorted_records[0].created_at).days
            if total_days < 1:
                return {"progress_rate": 0, "trend": "insufficient_data"}

            # å°†è®°å½•åˆ†ä¸ºå‰åŠæ®µå’ŒååŠæ®µ
            mid_point = len(sorted_records) // 2
            early_records: list[Any] = sorted_records[:mid_point]
            recent_records: list[Any] = sorted_records[mid_point:]

            # è®¡ç®—å„é˜¶æ®µçš„è¡¨ç°
            early_accuracy = (
                sum(1 for r in early_records if r.is_correct) / len(early_records)
                if early_records
                else 0
            )
            recent_accuracy = (
                sum(1 for r in recent_records if r.is_correct) / len(recent_records)
                if recent_records
                else 0
            )

            early_avg_time = (
                sum(r.time_spent for r in early_records if r.time_spent)
                / len([r for r in early_records if r.time_spent])
                if any(r.time_spent for r in early_records)
                else 0
            )
            recent_avg_time = (
                sum(r.time_spent for r in recent_records if r.time_spent)
                / len([r for r in recent_records if r.time_spent])
                if any(r.time_spent for r in recent_records)
                else 0
            )

            # è®¡ç®—è¿›æ­¥ç‡
            accuracy_improvement = recent_accuracy - early_accuracy
            speed_improvement = (
                (early_avg_time - recent_avg_time) / early_avg_time if early_avg_time > 0 else 0
            )

            # ç»¼åˆè¿›æ­¥è¯„åˆ†
            progress_rate = (accuracy_improvement + speed_improvement * 0.5) / 1.5

            # ç¡®å®šè¶‹åŠ¿
            if progress_rate > 0.1:
                trend = "improving"
            elif progress_rate < -0.1:
                trend = "declining"
            else:
                trend = "stable"

            return {
                "progress_rate": progress_rate,
                "trend": trend,
                "accuracy_improvement": accuracy_improvement,
                "speed_improvement": speed_improvement,
                "early_performance": {
                    "accuracy": early_accuracy,
                    "average_time": early_avg_time,
                },
                "recent_performance": {
                    "accuracy": recent_accuracy,
                    "average_time": recent_avg_time,
                },
            }

        except Exception as e:
            logger.error(f"è¯„ä¼°å­¦ä¹ è¿›åº¦å¤±è´¥: {str(e)}")
            return {"progress_rate": 0, "trend": "unknown"}

    def _identify_weak_areas(self, knowledge_point_analysis: dict[str, Any]) -> list[str]:
        """è¯†åˆ«è–„å¼±ç¯èŠ‚."""
        try:
            weak_areas = []

            for point, data in knowledge_point_analysis.items():
                accuracy = data.get("accuracy", 0)
                attempts = data.get("total_attempts", 0)

                # åˆ¤æ–­è–„å¼±ç¯èŠ‚çš„æ¡ä»¶ï¼šæ­£ç¡®ç‡ä½äº60%ä¸”å°è¯•æ¬¡æ•°è¶³å¤Ÿ
                if accuracy < 0.6 and attempts >= 3:
                    weak_areas.append(point)
                # æˆ–è€…æ­£ç¡®ç‡ä½äº40%ï¼ˆæ— è®ºå°è¯•æ¬¡æ•°ï¼‰
                elif accuracy < 0.4:
                    weak_areas.append(point)

            return weak_areas

        except Exception as e:
            logger.error(f"è¯†åˆ«è–„å¼±ç¯èŠ‚å¤±è´¥: {str(e)}")
            return []

    def _calculate_mastery_confidence(
        self,
        knowledge_point_analysis: dict[str, Any],
        retention_analysis: dict[str, Any],
        progress_assessment: dict[str, Any],
    ) -> float:
        """è®¡ç®—æŒæ¡åº¦ç½®ä¿¡åº¦."""
        try:
            confidence_factors = []

            # çŸ¥è¯†ç‚¹åˆ†æç½®ä¿¡åº¦
            if knowledge_point_analysis:
                total_attempts = sum(
                    data.get("total_attempts", 0) for data in knowledge_point_analysis.values()
                )
                if total_attempts >= 20:
                    confidence_factors.append(0.9)
                elif total_attempts >= 10:
                    confidence_factors.append(0.7)
                else:
                    confidence_factors.append(0.5)
            else:
                confidence_factors.append(0.3)

            # ä¿æŒåˆ†æç½®ä¿¡åº¦
            retention_score = retention_analysis.get("retention_score", 0)
            if retention_score > 0:
                confidence_factors.append(0.8)
            else:
                confidence_factors.append(0.4)

            # è¿›åº¦è¯„ä¼°ç½®ä¿¡åº¦
            trend = progress_assessment.get("trend", "unknown")
            if trend != "unknown":
                confidence_factors.append(0.8)
            else:
                confidence_factors.append(0.4)

            return sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0

        except Exception as e:
            logger.error(f"è®¡ç®—æŒæ¡åº¦ç½®ä¿¡åº¦å¤±è´¥: {str(e)}")
            return 0.5

    def _calculate_overall_mastery_score(self, mastery_levels: dict[str, Any]) -> float:
        """è®¡ç®—æ•´ä½“æŒæ¡åº¦åˆ†æ•°."""
        try:
            stats = mastery_levels.get("statistics", {})

            # æƒé‡åˆ†é…
            weights = {
                "mastered": 1.0,
                "proficient": 0.8,
                "developing": 0.6,
                "struggling": 0.3,
            }

            total_score = 0
            total_points = 0

            for level, weight in weights.items():
                level_stats = stats.get(level, {})
                count = level_stats.get("count", 0)
                total_score += count * weight
                total_points += count

            return total_score / total_points if total_points > 0 else 0

        except Exception as e:
            logger.error(f"è®¡ç®—æ•´ä½“æŒæ¡åº¦åˆ†æ•°å¤±è´¥: {str(e)}")
            return 0

    def _analyze_accuracy_efficiency(self, records: list[Any]) -> dict[str, Any]:
        """åˆ†æå‡†ç¡®ç‡æ•ˆç‡."""
        try:
            if not records:
                return {"efficiency_score": 0, "accuracy_trend": "unknown"}

            # è®¡ç®—æ•´ä½“å‡†ç¡®ç‡
            correct_count = sum(1 for r in records if r.is_correct)
            total_count = len(records)
            overall_accuracy = correct_count / total_count if total_count > 0 else 0

            # åˆ†æå‡†ç¡®ç‡è¶‹åŠ¿
            if len(records) >= 10:
                recent_10 = records[:10]
                earlier_10 = records[10:20] if len(records) >= 20 else records[10:]

                recent_accuracy = sum(1 for r in recent_10 if r.is_correct) / len(recent_10)
                earlier_accuracy = (
                    sum(1 for r in earlier_10 if r.is_correct) / len(earlier_10)
                    if earlier_10
                    else recent_accuracy
                )

                if recent_accuracy > earlier_accuracy + 0.1:
                    trend = "improving"
                elif recent_accuracy < earlier_accuracy - 0.1:
                    trend = "declining"
                else:
                    trend = "stable"
            else:
                trend = "insufficient_data"

            # è®¡ç®—æ•ˆç‡åˆ†æ•° (å‡†ç¡®ç‡ * ä¸€è‡´æ€§å› å­)
            accuracy_variance = 0
            if len(records) >= 5:
                # è®¡ç®—æœ€è¿‘5æ¬¡çš„å‡†ç¡®ç‡æ–¹å·®
                recent_results = [1 if r.is_correct else 0 for r in records[:5]]
                mean_recent = sum(recent_results) / len(recent_results)
                accuracy_variance = sum((x - mean_recent) ** 2 for x in recent_results) / len(
                    recent_results
                )

            consistency_factor = max(0.5, 1 - accuracy_variance)
            efficiency_score = overall_accuracy * consistency_factor

            return {
                "efficiency_score": efficiency_score,
                "overall_accuracy": overall_accuracy,
                "accuracy_trend": trend,
                "consistency_factor": consistency_factor,
                "recent_accuracy": (
                    sum(1 for r in records[:10] if r.is_correct) / min(10, len(records))
                    if records
                    else 0
                ),
                "confidence": min(1.0, len(records) / 20),
            }

        except Exception as e:
            logger.error(f"åˆ†æå‡†ç¡®ç‡æ•ˆç‡å¤±è´¥: {str(e)}")
            return {"efficiency_score": 0, "accuracy_trend": "unknown"}

    def _analyze_speed_efficiency(self, records: list[Any]) -> dict[str, Any]:
        """åˆ†æé€Ÿåº¦æ•ˆç‡."""
        try:
            if not records:
                return {"efficiency_score": 0, "speed_trend": "unknown"}

            # è·å–æœ‰æ•ˆçš„ç­”é¢˜æ—¶é—´
            valid_times = [r.time_spent for r in records if r.time_spent and r.time_spent > 0]
            if not valid_times:
                return {"efficiency_score": 0, "speed_trend": "unknown"}

            # è®¡ç®—å¹³å‡ç­”é¢˜æ—¶é—´
            avg_time = sum(valid_times) / len(valid_times)

            # åˆ†æé€Ÿåº¦è¶‹åŠ¿
            if len(valid_times) >= 10:
                recent_times = valid_times[:10]
                earlier_times = valid_times[10:20] if len(valid_times) >= 20 else valid_times[10:]

                recent_avg = sum(recent_times) / len(recent_times)
                earlier_avg = (
                    sum(earlier_times) / len(earlier_times) if earlier_times else recent_avg
                )

                if recent_avg < earlier_avg * 0.9:
                    trend = "accelerating"
                elif recent_avg > earlier_avg * 1.1:
                    trend = "decelerating"
                else:
                    trend = "stable"
            else:
                trend = "insufficient_data"

            # è®¡ç®—é€Ÿåº¦æ•ˆç‡åˆ†æ•° (åŸºäºç†æƒ³æ—¶é—´èŒƒå›´)
            ideal_time_range = (30, 120)  # 30-120ç§’ä¸ºç†æƒ³èŒƒå›´
            if ideal_time_range[0] <= avg_time <= ideal_time_range[1]:
                speed_efficiency = 1.0
            elif avg_time < ideal_time_range[0]:
                # å¤ªå¿«å¯èƒ½å½±å“å‡†ç¡®ç‡
                speed_efficiency = 0.8
            else:
                # å¤ªæ…¢æ•ˆç‡ä½
                speed_efficiency = max(0.3, ideal_time_range[1] / avg_time)

            # è€ƒè™‘ä¸€è‡´æ€§
            if len(valid_times) >= 5:
                import statistics

                time_std = statistics.stdev(valid_times[:5])
                consistency_factor = max(0.5, 1 - min(1.0, time_std / avg_time))
            else:
                consistency_factor = 0.8

            efficiency_score = speed_efficiency * consistency_factor

            return {
                "efficiency_score": efficiency_score,
                "average_time": avg_time,
                "speed_trend": trend,
                "consistency_factor": consistency_factor,
                "ideal_range": ideal_time_range,
                "confidence": min(1.0, len(valid_times) / 20),
            }

        except Exception as e:
            logger.error(f"åˆ†æé€Ÿåº¦æ•ˆç‡å¤±è´¥: {str(e)}")
            return {"efficiency_score": 0, "speed_trend": "unknown"}

    def _analyze_learning_consistency(self, records: list[Any]) -> dict[str, Any]:
        """åˆ†æå­¦ä¹ ä¸€è‡´æ€§."""
        try:
            if not records:
                return {"consistency_score": 0, "pattern": "unknown"}

            # åˆ†æç­”é¢˜æ—¶é—´ä¸€è‡´æ€§
            valid_times = [r.time_spent for r in records if r.time_spent and r.time_spent > 0]
            time_consistency = 0
            if len(valid_times) >= 3:
                import statistics

                avg_time = sum(valid_times) / len(valid_times)
                time_std = statistics.stdev(valid_times)
                time_consistency = max(0, 1 - min(1.0, time_std / avg_time)) if avg_time > 0 else 0

            # åˆ†ææ­£ç¡®ç‡ä¸€è‡´æ€§
            accuracy_consistency = 0
            if len(records) >= 10:
                # å°†è®°å½•åˆ†ä¸º5ä¸ªåŒºé—´ï¼Œè®¡ç®—æ¯ä¸ªåŒºé—´çš„æ­£ç¡®ç‡
                chunk_size = len(records) // 5
                chunk_accuracies = []
                for i in range(5):
                    start_idx = i * chunk_size
                    end_idx = start_idx + chunk_size if i < 4 else len(records)
                    chunk = records[start_idx:end_idx]
                    chunk_accuracy = (
                        sum(1 for r in chunk if r.is_correct) / len(chunk) if chunk else 0
                    )
                    chunk_accuracies.append(chunk_accuracy)

                if chunk_accuracies:
                    import statistics

                    accuracy_std = statistics.stdev(chunk_accuracies)
                    accuracy_consistency = max(0, 1 - min(1.0, accuracy_std / 0.5))  # æ ‡å‡†åŒ–åˆ°0.5

            # ç»¼åˆä¸€è‡´æ€§åˆ†æ•°
            consistency_score = time_consistency * 0.4 + accuracy_consistency * 0.6

            # ç¡®å®šä¸€è‡´æ€§æ¨¡å¼
            if consistency_score >= 0.8:
                pattern = "highly_consistent"
            elif consistency_score >= 0.6:
                pattern = "moderately_consistent"
            elif consistency_score >= 0.4:
                pattern = "somewhat_inconsistent"
            else:
                pattern = "highly_inconsistent"

            return {
                "consistency_score": consistency_score,
                "time_consistency": time_consistency,
                "accuracy_consistency": accuracy_consistency,
                "pattern": pattern,
                "confidence": min(1.0, len(records) / 30),
            }

        except Exception as e:
            logger.error(f"åˆ†æå­¦ä¹ ä¸€è‡´æ€§å¤±è´¥: {str(e)}")
            return {"consistency_score": 0, "pattern": "unknown"}

    def _analyze_progress_rate(self, records: list[Any], sessions: list[Any]) -> dict[str, Any]:
        """åˆ†æè¿›æ­¥é€Ÿåº¦."""
        try:
            if not records or not sessions:
                return {"progress_rate": 0, "trend": "unknown"}

            # æŒ‰æ—¶é—´æ’åº
            sorted_records = sorted(records, key=lambda x: x.created_at)
            # sorted_sessions = sorted(sessions, key=lambda x: x.created_at)  # æš‚æ—¶ä¸ä½¿ç”¨

            # è®¡ç®—æ—¶é—´è·¨åº¦
            if len(sorted_records) < 2:
                return {"progress_rate": 0, "trend": "insufficient_data"}

            time_span = (
                sorted_records[-1].created_at - sorted_records[0].created_at
            ).total_seconds() / 86400  # å¤©æ•°

            # åˆ†æ—¶é—´æ®µåˆ†æè¿›æ­¥
            if len(sorted_records) >= 20:
                # åˆ†ä¸ºå‰åä¸¤åŠ
                mid_point = len(sorted_records) // 2
                early_records = sorted_records[:mid_point]
                recent_records = sorted_records[mid_point:]

                # è®¡ç®—å„é˜¶æ®µè¡¨ç°
                early_accuracy = sum(1 for r in early_records if r.is_correct) / len(early_records)
                recent_accuracy = sum(1 for r in recent_records if r.is_correct) / len(
                    recent_records
                )

                # è®¡ç®—å¹³å‡ç­”é¢˜æ—¶é—´å˜åŒ–
                early_times = [
                    r.time_spent for r in early_records if r.time_spent and r.time_spent > 0
                ]
                recent_times = [
                    r.time_spent for r in recent_records if r.time_spent and r.time_spent > 0
                ]

                early_avg_time = sum(early_times) / len(early_times) if early_times else 0
                recent_avg_time = sum(recent_times) / len(recent_times) if recent_times else 0

                # è®¡ç®—è¿›æ­¥ç‡
                accuracy_improvement = recent_accuracy - early_accuracy
                speed_improvement = (
                    (early_avg_time - recent_avg_time) / early_avg_time if early_avg_time > 0 else 0
                )

                # ç»¼åˆè¿›æ­¥ç‡ (å‡†ç¡®ç‡æƒé‡0.7ï¼Œé€Ÿåº¦æƒé‡0.3)
                progress_rate = accuracy_improvement * 0.7 + speed_improvement * 0.3

                # ç¡®å®šè¶‹åŠ¿
                if progress_rate > 0.05:
                    trend = "rapid_improvement"
                elif progress_rate > 0.02:
                    trend = "steady_improvement"
                elif progress_rate > -0.02:
                    trend = "stable"
                elif progress_rate > -0.05:
                    trend = "slight_decline"
                else:
                    trend = "significant_decline"

                # è®¡ç®—æ—¥å‡è¿›æ­¥ç‡
                daily_progress_rate = progress_rate / max(1, time_span) if time_span > 0 else 0

                return {
                    "progress_rate": progress_rate,
                    "daily_progress_rate": daily_progress_rate,
                    "trend": trend,
                    "accuracy_improvement": accuracy_improvement,
                    "speed_improvement": speed_improvement,
                    "time_span_days": time_span,
                    "confidence": min(1.0, len(sorted_records) / 50),
                }
            else:
                return {"progress_rate": 0, "trend": "insufficient_data"}

        except Exception as e:
            logger.error(f"åˆ†æè¿›æ­¥é€Ÿåº¦å¤±è´¥: {str(e)}")
            return {"progress_rate": 0, "trend": "unknown"}

    def _determine_efficiency_level(self, overall_efficiency: float) -> str:
        """ç¡®å®šæ•ˆç‡ç­‰çº§."""
        try:
            if overall_efficiency >= 0.9:
                return "excellent"
            elif overall_efficiency >= 0.8:
                return "very_good"
            elif overall_efficiency >= 0.7:
                return "good"
            elif overall_efficiency >= 0.6:
                return "fair"
            elif overall_efficiency >= 0.5:
                return "poor"
            else:
                return "very_poor"

        except Exception as e:
            logger.error(f"ç¡®å®šæ•ˆç‡ç­‰çº§å¤±è´¥: {str(e)}")
            return "unknown"

    def _generate_efficiency_improvement_suggestions(
        self,
        accuracy_efficiency: dict[str, Any],
        speed_efficiency: dict[str, Any],
        consistency_analysis: dict[str, Any],
        progress_rate: dict[str, Any],
    ) -> list[dict[str, Any]]:
        """ç”Ÿæˆæ•ˆç‡æ”¹è¿›å»ºè®®."""
        try:
            suggestions = []

            # åŸºäºå‡†ç¡®ç‡æ•ˆç‡çš„å»ºè®®
            accuracy_score = accuracy_efficiency.get("efficiency_score", 0)
            accuracy_trend = accuracy_efficiency.get("accuracy_trend", "unknown")

            if accuracy_score < 0.6:
                suggestions.append(
                    {
                        "type": "accuracy_improvement",
                        "priority": "high",
                        "suggestion": "å»ºè®®åŠ å¼ºåŸºç¡€çŸ¥è¯†ç»ƒä¹ ï¼Œé‡ç‚¹å…³æ³¨é”™è¯¯ç‡è¾ƒé«˜çš„é¢˜å‹",
                        "reason": f"å½“å‰å‡†ç¡®ç‡æ•ˆç‡è¾ƒä½ ({accuracy_score:.2f})",
                        "action_items": [
                            "å¤ä¹ åŸºç¡€è¯­æ³•å’Œè¯æ±‡",
                            "é’ˆå¯¹é”™è¯¯é¢˜å‹è¿›è¡Œä¸“é¡¹ç»ƒä¹ ",
                            "é™ä½ç­”é¢˜é€Ÿåº¦ï¼Œæé«˜å‡†ç¡®ç‡",
                        ],
                    }
                )

            if accuracy_trend == "declining":
                suggestions.append(
                    {
                        "type": "accuracy_trend",
                        "priority": "high",
                        "suggestion": "æ³¨æ„åˆ°å‡†ç¡®ç‡å‘ˆä¸‹é™è¶‹åŠ¿ï¼Œå»ºè®®æš‚åœæ–°å†…å®¹å­¦ä¹ ï¼Œå·©å›ºå·²å­¦çŸ¥è¯†",
                        "reason": "å‡†ç¡®ç‡è¶‹åŠ¿ä¸‹é™",
                        "action_items": [
                            "å›é¡¾æœ€è¿‘çš„é”™è¯¯é¢˜ç›®",
                            "åŠ å¼ºè–„å¼±çŸ¥è¯†ç‚¹ç»ƒä¹ ",
                            "é€‚å½“ä¼‘æ¯ï¼Œé¿å…ç–²åŠ³å­¦ä¹ ",
                        ],
                    }
                )

            # åŸºäºé€Ÿåº¦æ•ˆç‡çš„å»ºè®®
            speed_score = speed_efficiency.get("efficiency_score", 0)
            avg_time = speed_efficiency.get("average_time", 0)
            ideal_range = speed_efficiency.get("ideal_range", (30, 120))

            if speed_score < 0.6:
                if avg_time > ideal_range[1]:
                    suggestions.append(
                        {
                            "type": "speed_improvement",
                            "priority": "medium",
                            "suggestion": "ç­”é¢˜é€Ÿåº¦åæ…¢ï¼Œå»ºè®®è¿›è¡Œé™æ—¶ç»ƒä¹ æé«˜ç­”é¢˜æ•ˆç‡",
                            "reason": f"å¹³å‡ç­”é¢˜æ—¶é—´ {avg_time:.1f}ç§’ï¼Œè¶…å‡ºç†æƒ³èŒƒå›´",
                            "action_items": [
                                "è¿›è¡Œé™æ—¶ç»ƒä¹ ",
                                "æé«˜é˜…è¯»é€Ÿåº¦",
                                "ç†Ÿç»ƒæŒæ¡å¸¸è§é¢˜å‹è§£é¢˜æŠ€å·§",
                            ],
                        }
                    )
                elif avg_time < ideal_range[0]:
                    suggestions.append(
                        {
                            "type": "accuracy_focus",
                            "priority": "medium",
                            "suggestion": "ç­”é¢˜é€Ÿåº¦è¿‡å¿«ï¼Œå»ºè®®æ”¾æ…¢é€Ÿåº¦ï¼Œä»”ç»†å®¡é¢˜",
                            "reason": f"å¹³å‡ç­”é¢˜æ—¶é—´ {avg_time:.1f}ç§’ï¼Œè¿‡äºåŒ†å¿™",
                            "action_items": [
                                "ä»”ç»†é˜…è¯»é¢˜ç›®è¦æ±‚",
                                "æ£€æŸ¥ç­”æ¡ˆå‡†ç¡®æ€§",
                                "é¿å…æ€¥èºæƒ…ç»ª",
                            ],
                        }
                    )

            # åŸºäºä¸€è‡´æ€§çš„å»ºè®®
            consistency_score = consistency_analysis.get("consistency_score", 0)
            pattern = consistency_analysis.get("pattern", "unknown")

            if consistency_score < 0.5:
                suggestions.append(
                    {
                        "type": "consistency_improvement",
                        "priority": "medium",
                        "suggestion": "å­¦ä¹ è¡¨ç°ä¸å¤Ÿç¨³å®šï¼Œå»ºè®®å»ºç«‹è§„å¾‹çš„å­¦ä¹ ä¹ æƒ¯",
                        "reason": f"ä¸€è‡´æ€§åˆ†æ•°è¾ƒä½ ({consistency_score:.2f})ï¼Œè¡¨ç°ä¸º{pattern}",
                        "action_items": [
                            "åˆ¶å®šå›ºå®šçš„å­¦ä¹ æ—¶é—´è¡¨",
                            "ä¿æŒè‰¯å¥½çš„å­¦ä¹ ç¯å¢ƒ",
                            "é¿å…åœ¨ç–²åŠ³æˆ–åˆ†å¿ƒæ—¶å­¦ä¹ ",
                        ],
                    }
                )

            # åŸºäºè¿›æ­¥ç‡çš„å»ºè®®
            progress_trend = progress_rate.get("trend", "unknown")
            progress_value = progress_rate.get("progress_rate", 0)

            if progress_trend in ["stable", "slight_decline", "significant_decline"]:
                suggestions.append(
                    {
                        "type": "progress_enhancement",
                        "priority": (
                            "high" if progress_trend == "significant_decline" else "medium"
                        ),
                        "suggestion": "å­¦ä¹ è¿›æ­¥ç¼“æ…¢ï¼Œå»ºè®®è°ƒæ•´å­¦ä¹ ç­–ç•¥å’Œæ–¹æ³•",
                        "reason": f"è¿›æ­¥è¶‹åŠ¿: {progress_trend}ï¼Œè¿›æ­¥ç‡: {progress_value:.3f}",
                        "action_items": [
                            "å°è¯•æ–°çš„å­¦ä¹ æ–¹æ³•",
                            "å¢åŠ ç»ƒä¹ éš¾åº¦æˆ–é¢˜é‡",
                            "å¯»æ±‚è€å¸ˆæˆ–åŒå­¦çš„å¸®åŠ©",
                        ],
                    }
                )

            # å¦‚æœæ²¡æœ‰æ˜æ˜¾é—®é¢˜ï¼Œç»™å‡ºç§¯æå»ºè®®
            if not suggestions:
                suggestions.append(
                    {
                        "type": "maintenance",
                        "priority": "low",
                        "suggestion": "å½“å‰å­¦ä¹ æ•ˆç‡è‰¯å¥½ï¼Œå»ºè®®ä¿æŒç°æœ‰å­¦ä¹ èŠ‚å¥",
                        "reason": "å„é¡¹æ•ˆç‡æŒ‡æ ‡è¡¨ç°è‰¯å¥½",
                        "action_items": [
                            "ç»§ç»­ä¿æŒå½“å‰å­¦ä¹ æ–¹æ³•",
                            "é€‚å½“å¢åŠ æŒ‘æˆ˜æ€§å†…å®¹",
                            "å®šæœŸå›é¡¾å’Œæ€»ç»“",
                        ],
                    }
                )

            return suggestions

        except Exception as e:
            logger.error(f"ç”Ÿæˆæ•ˆç‡æ”¹è¿›å»ºè®®å¤±è´¥: {str(e)}")
            return []
