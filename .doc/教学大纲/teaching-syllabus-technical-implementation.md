# æ•™å­¦å¤§çº²ç”Ÿæˆç³»ç»ŸæŠ€æœ¯å®ç°è¯¦ç»†è®¾è®¡

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†æ•™å­¦å¤§çº²ç”Ÿæˆç³»ç»Ÿçš„æŠ€æœ¯å®ç°æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ ¸å¿ƒç®—æ³•å®ç°ã€æœåŠ¡æ¶æ„è®¾è®¡ã€æ€§èƒ½ä¼˜åŒ–ç­–ç•¥å’Œéƒ¨ç½²æ–¹æ¡ˆã€‚

## ğŸ”§ æ ¸å¿ƒç®—æ³•å®ç°

### 1. å¤šè½®è¿­ä»£åˆ†æç®—æ³•

#### è¿­ä»£åˆ†æå¼•æ“æ ¸å¿ƒå®ç°

```python
class IterativeAnalysisEngine:
    """å¤šè½®è¿­ä»£AIåˆ†æå¼•æ“"""

    def __init__(self):
        self.ai_service = DeepSeekAPIService()
        self.quality_evaluator = AnalysisQualityEvaluator()
        self.context_builder = AnalysisContextBuilder()
        self.termination_detector = TerminationDetector()

    async def start_analysis_session(
        self,
        document: UploadedFile,
        config: AnalysisConfig
    ) -> AnalysisSession:
        """å¯åŠ¨å¤šè½®åˆ†æä¼šè¯"""

        # 1. åˆ›å»ºåˆ†æä¼šè¯
        session = AnalysisSession.create(
            document=document,
            config=config,
            max_rounds=config.max_rounds,
            quality_threshold=config.quality_threshold
        )

        # 2. æ–‡æ¡£é¢„å¤„ç†
        processed_document = await self._preprocess_document(document)
        session.set_processed_document(processed_document)

        # 3. ç¬¬ä¸€è½®æ¡†æ¶åˆ†æ
        framework_result = await self._analyze_framework(processed_document)
        session.add_round_result(1, framework_result)

        # 4. è¿­ä»£åˆ†æå¾ªç¯
        while not self.termination_detector.should_terminate(session):
            await self._execute_next_round(session)

        # 5. æœ€ç»ˆæ•´åˆå’Œä¼˜åŒ–
        final_result = await self._integrate_and_optimize(session)
        session.mark_completed(final_result)

        return session

    async def _execute_next_round(self, session: AnalysisSession):
        """æ‰§è¡Œä¸‹ä¸€è½®åˆ†æ"""
        round_number = session.current_round + 1

        # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
        context = self.context_builder.build_context(session)

        # ç¡®å®šåˆ†æç„¦ç‚¹
        focus_strategy = self._determine_focus_strategy(session, context)
        focus_areas = focus_strategy.get_focus_areas()

        # æ„å»ºå¢å¼ºæç¤ºè¯
        enhanced_prompt = self._build_enhanced_prompt(
            session.processed_document,
            context,
            focus_areas,
            round_number
        )

        # æ‰§è¡ŒAIåˆ†æ
        ai_response = await self.ai_service.generate_response(
            prompt=enhanced_prompt,
            model=self._select_optimal_model(session, round_number),
            temperature=self._calculate_optimal_temperature(session),
            max_tokens=self._calculate_optimal_tokens(session)
        )

        # è§£æå’ŒéªŒè¯ç»“æœ
        parsed_result = self._parse_ai_response(ai_response, round_number)
        validated_result = self._validate_analysis_result(parsed_result, context)

        # è´¨é‡è¯„ä¼°
        quality_metrics = self.quality_evaluator.evaluate_round(
            session, validated_result
        )
        validated_result.quality_metrics = quality_metrics

        # æ·»åŠ åˆ°ä¼šè¯
        session.add_round_result(round_number, validated_result)

        # æ›´æ–°ä¸Šä¸‹æ–‡
        self.context_builder.update_context(session, validated_result)

    def _determine_focus_strategy(
        self,
        session: AnalysisSession,
        context: AnalysisContext
    ) -> FocusStrategy:
        """ç¡®å®šåˆ†æç„¦ç‚¹ç­–ç•¥"""

        # åˆ†æå†å²è´¨é‡è¶‹åŠ¿
        quality_trend = self._analyze_quality_trend(session)

        # è¯†åˆ«å†…å®¹ç¼ºå£
        content_gaps = self._identify_content_gaps(session, context)

        # è¯„ä¼°åˆ†ææ·±åº¦
        current_depth = self._evaluate_analysis_depth(session)

        # é€‰æ‹©ç„¦ç‚¹ç­–ç•¥
        if quality_trend.is_improving and content_gaps.has_major_gaps:
            return ContentGapFocusStrategy(content_gaps)
        elif current_depth < context.required_depth:
            return DepthEnhancementStrategy(current_depth, context.required_depth)
        elif quality_trend.is_plateauing:
            return QualityRefinementStrategy(session.latest_result)
        else:
            return BalancedFocusStrategy(session, context)

    def _build_enhanced_prompt(
        self,
        document: ProcessedDocument,
        context: AnalysisContext,
        focus_areas: List[str],
        round_number: int
    ) -> str:
        """æ„å»ºå¢å¼ºçš„AIæç¤ºè¯"""

        prompt_builder = EnhancedPromptBuilder()

        # æ·»åŠ æ–‡æ¡£å†…å®¹
        prompt_builder.add_document_content(
            document.content,
            max_length=self._calculate_content_length(round_number)
        )

        # æ·»åŠ å†å²ä¸Šä¸‹æ–‡
        prompt_builder.add_historical_context(
            context.previous_results,
            context.identified_patterns,
            context.quality_evolution
        )

        # æ·»åŠ ç„¦ç‚¹æŒ‡å¯¼
        prompt_builder.add_focus_guidance(
            focus_areas,
            context.analysis_objectives,
            round_number
        )

        # æ·»åŠ è´¨é‡è¦æ±‚
        prompt_builder.add_quality_requirements(
            context.quality_standards,
            context.expected_improvements
        )

        # æ·»åŠ è¾“å‡ºæ ¼å¼è§„èŒƒ
        prompt_builder.add_output_format(
            self._get_output_schema(round_number),
            context.validation_rules
        )

        return prompt_builder.build()
```

#### è´¨é‡è¯„ä¼°ç®—æ³•

```python
class AnalysisQualityEvaluator:
    """åˆ†æè´¨é‡è¯„ä¼°å™¨"""

    def __init__(self):
        self.metrics_calculators = {
            'completeness': CompletenessCalculator(),
            'accuracy': AccuracyCalculator(),
            'consistency': ConsistencyCalculator(),
            'depth': DepthCalculator(),
            'novelty': NoveltyCalculator()
        }

    def evaluate_round(
        self,
        session: AnalysisSession,
        round_result: RoundResult
    ) -> QualityMetrics:
        """è¯„ä¼°å•è½®åˆ†æè´¨é‡"""

        metrics = {}

        # è®¡ç®—å„ç»´åº¦è´¨é‡æŒ‡æ ‡
        for metric_name, calculator in self.metrics_calculators.items():
            score = calculator.calculate(session, round_result)
            metrics[metric_name] = score

        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        overall_score = self._calculate_overall_score(metrics)

        # è®¡ç®—æ”¹è¿›ç‡
        improvement_rate = self._calculate_improvement_rate(session, overall_score)

        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        quality_report = self._generate_quality_report(
            metrics, overall_score, improvement_rate
        )

        return QualityMetrics(
            individual_scores=metrics,
            overall_score=overall_score,
            improvement_rate=improvement_rate,
            quality_report=quality_report,
            recommendations=self._generate_recommendations(metrics)
        )

    def _calculate_overall_score(self, metrics: Dict[str, float]) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°"""

        # æƒé‡é…ç½®
        weights = {
            'completeness': 0.25,
            'accuracy': 0.30,
            'consistency': 0.20,
            'depth': 0.15,
            'novelty': 0.10
        }

        weighted_sum = sum(
            metrics.get(metric, 0) * weight
            for metric, weight in weights.items()
        )

        return min(weighted_sum, 1.0)

    def _calculate_improvement_rate(
        self,
        session: AnalysisSession,
        current_score: float
    ) -> float:
        """è®¡ç®—ç›¸æ¯”å‰è½®çš„æ”¹è¿›ç‡"""

        if session.current_round <= 1:
            return 0.0

        previous_result = session.get_round_result(session.current_round - 1)
        if not previous_result:
            return 0.0

        previous_score = previous_result.quality_metrics.overall_score

        if previous_score == 0:
            return 1.0 if current_score > 0 else 0.0

        return (current_score - previous_score) / previous_score

class CompletenessCalculator:
    """å®Œæ•´æ€§è®¡ç®—å™¨"""

    def calculate(self, session: AnalysisSession, round_result: RoundResult) -> float:
        """è®¡ç®—å†…å®¹å®Œæ•´æ€§åˆ†æ•°"""

        # è·å–é¢„æœŸå†…å®¹è¦ç´ 
        expected_elements = self._get_expected_elements(session.document_type)

        # æ£€æŸ¥å·²æå–çš„å†…å®¹è¦ç´ 
        extracted_elements = self._extract_content_elements(round_result)

        # è®¡ç®—è¦†ç›–ç‡
        coverage_rate = len(extracted_elements & expected_elements) / len(expected_elements)

        # è€ƒè™‘å†…å®¹æ·±åº¦
        depth_bonus = self._calculate_depth_bonus(round_result)

        # è€ƒè™‘å†…å®¹è´¨é‡
        quality_factor = self._calculate_quality_factor(round_result)

        return min(coverage_rate + depth_bonus * quality_factor, 1.0)

    def _get_expected_elements(self, document_type: str) -> Set[str]:
        """è·å–é¢„æœŸçš„å†…å®¹è¦ç´ """

        element_templates = {
            'syllabus': {
                'course_objectives', 'learning_outcomes', 'content_structure',
                'assessment_methods', 'skill_requirements', 'difficulty_levels',
                'time_allocation', 'prerequisites', 'resources'
            },
            'textbook': {
                'chapter_structure', 'learning_objectives', 'key_concepts',
                'exercises', 'examples', 'difficulty_progression',
                'skill_coverage', 'vocabulary_lists'
            }
        }

        return element_templates.get(document_type, set())
```

### 2. çŸ¥è¯†ç‚¹æ˜ å°„ç®—æ³•

#### è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—

```python
class SemanticSimilarityCalculator:
    """è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—å™¨"""

    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.similarity_cache = SimilarityCache()

    async def calculate_similarity_matrix(
        self,
        syllabus_points: List[KnowledgePoint],
        textbook_points: List[KnowledgePoint]
    ) -> np.ndarray:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ"""

        # è·å–åµŒå…¥å‘é‡
        syllabus_embeddings = await self._get_embeddings(syllabus_points)
        textbook_embeddings = await self._get_embeddings(textbook_points)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = np.zeros((len(syllabus_points), len(textbook_points)))

        for i, s_embedding in enumerate(syllabus_embeddings):
            for j, t_embedding in enumerate(textbook_embeddings):
                # å¤šç§ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•
                cosine_sim = self._cosine_similarity(s_embedding, t_embedding)
                euclidean_sim = self._euclidean_similarity(s_embedding, t_embedding)
                jaccard_sim = self._jaccard_similarity(
                    syllabus_points[i], textbook_points[j]
                )

                # åŠ æƒç»„åˆ
                combined_sim = (
                    cosine_sim * 0.5 +
                    euclidean_sim * 0.3 +
                    jaccard_sim * 0.2
                )

                similarity_matrix[i][j] = combined_sim

        return similarity_matrix

    async def _get_embeddings(self, knowledge_points: List[KnowledgePoint]) -> List[np.ndarray]:
        """è·å–çŸ¥è¯†ç‚¹çš„åµŒå…¥å‘é‡"""

        embeddings = []

        for point in knowledge_points:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(point)
            cached_embedding = self.similarity_cache.get(cache_key)

            if cached_embedding is not None:
                embeddings.append(cached_embedding)
                continue

            # æ„å»ºæ–‡æœ¬è¡¨ç¤º
            text_representation = self._build_text_representation(point)

            # è·å–åµŒå…¥å‘é‡
            embedding = await self.embedding_service.get_embedding(text_representation)

            # ç¼“å­˜ç»“æœ
            self.similarity_cache.set(cache_key, embedding)
            embeddings.append(embedding)

        return embeddings

    def _build_text_representation(self, point: KnowledgePoint) -> str:
        """æ„å»ºçŸ¥è¯†ç‚¹çš„æ–‡æœ¬è¡¨ç¤º"""

        components = [
            point.name,
            point.description,
            ' '.join(point.keywords),
            point.category,
            f"éš¾åº¦:{point.difficulty_level}"
        ]

        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        if point.parent:
            components.append(f"çˆ¶çº§:{point.parent.name}")

        if point.children:
            child_names = [child.name for child in point.children]
            components.append(f"å­çº§:{','.join(child_names)}")

        return ' | '.join(filter(None, components))

class OptimalMatchingAlgorithm:
    """æœ€ä¼˜åŒ¹é…ç®—æ³•"""

    def find_optimal_matches(
        self,
        similarity_matrix: np.ndarray,
        syllabus_points: List[KnowledgePoint],
        textbook_points: List[KnowledgePoint],
        threshold: float = 0.3
    ) -> List[KnowledgePointMatch]:
        """å¯»æ‰¾æœ€ä¼˜çŸ¥è¯†ç‚¹åŒ¹é…"""

        matches = []

        # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•è¿›è¡Œæœ€ä¼˜åˆ†é…
        row_indices, col_indices = linear_sum_assignment(-similarity_matrix)

        for i, j in zip(row_indices, col_indices):
            similarity_score = similarity_matrix[i][j]

            if similarity_score >= threshold:
                match = KnowledgePointMatch(
                    syllabus_point=syllabus_points[i],
                    textbook_point=textbook_points[j],
                    similarity_score=similarity_score,
                    match_type=self._determine_match_type(similarity_score),
                    confidence=self._calculate_confidence(similarity_score),
                    mapping_rationale=self._generate_rationale(
                        syllabus_points[i], textbook_points[j], similarity_score
                    )
                )
                matches.append(match)

        # å¤„ç†æœªåŒ¹é…çš„çŸ¥è¯†ç‚¹
        unmatched_syllabus = self._find_unmatched_syllabus_points(
            syllabus_points, matches
        )
        unmatched_textbook = self._find_unmatched_textbook_points(
            textbook_points, matches
        )

        # å°è¯•å¤šå¯¹ä¸€å’Œä¸€å¯¹å¤šåŒ¹é…
        additional_matches = self._find_complex_matches(
            unmatched_syllabus, unmatched_textbook, similarity_matrix
        )
        matches.extend(additional_matches)

        return matches

    def _determine_match_type(self, similarity_score: float) -> str:
        """ç¡®å®šåŒ¹é…ç±»å‹"""

        if similarity_score >= 0.8:
            return 'exact'
        elif similarity_score >= 0.6:
            return 'strong'
        elif similarity_score >= 0.4:
            return 'moderate'
        else:
            return 'weak'

    def _calculate_confidence(self, similarity_score: float) -> str:
        """è®¡ç®—åŒ¹é…ç½®ä¿¡åº¦"""

        if similarity_score >= 0.9:
            return 'very_high'
        elif similarity_score >= 0.7:
            return 'high'
        elif similarity_score >= 0.5:
            return 'medium'
        else:
            return 'low'
```

### 3. æ™ºèƒ½è¯¾æ—¶åˆ†é…ç®—æ³•

#### é‡è¦æ€§æƒé‡è®¡ç®—

```python
class ImportanceWeightCalculator:
    """é‡è¦æ€§æƒé‡è®¡ç®—å™¨"""

    def __init__(self):
        self.weight_factors = {
            'syllabus_emphasis': 0.4,      # è€ƒçº²å¼ºè°ƒç¨‹åº¦
            'exam_frequency': 0.3,         # è€ƒè¯•å‡ºç°é¢‘ç‡
            'difficulty_level': 0.2,       # éš¾åº¦ç­‰çº§
            'prerequisite_importance': 0.1  # å‰ç½®çŸ¥è¯†é‡è¦æ€§
        }

    def calculate_weights(
        self,
        knowledge_hierarchy: KnowledgeHierarchy,
        syllabus_analysis: SyllabusAnalysis
    ) -> Dict[str, float]:
        """è®¡ç®—çŸ¥è¯†ç‚¹é‡è¦æ€§æƒé‡"""

        weights = {}

        for point in knowledge_hierarchy.knowledge_points:
            # è®¡ç®—å„å› å­åˆ†æ•°
            syllabus_score = self._calculate_syllabus_emphasis_score(
                point, syllabus_analysis
            )
            frequency_score = self._calculate_exam_frequency_score(point)
            difficulty_score = self._calculate_difficulty_score(point)
            prerequisite_score = self._calculate_prerequisite_score(
                point, knowledge_hierarchy
            )

            # åŠ æƒè®¡ç®—æœ€ç»ˆæƒé‡
            final_weight = (
                syllabus_score * self.weight_factors['syllabus_emphasis'] +
                frequency_score * self.weight_factors['exam_frequency'] +
                difficulty_score * self.weight_factors['difficulty_level'] +
                prerequisite_score * self.weight_factors['prerequisite_importance']
            )

            weights[point.id] = min(max(final_weight, 0.0), 1.0)

        # å½’ä¸€åŒ–æƒé‡
        return self._normalize_weights(weights)

    def _calculate_syllabus_emphasis_score(
        self,
        point: KnowledgePoint,
        syllabus_analysis: SyllabusAnalysis
    ) -> float:
        """è®¡ç®—è€ƒçº²å¼ºè°ƒç¨‹åº¦åˆ†æ•°"""

        # æŸ¥æ‰¾åœ¨è€ƒçº²ä¸­çš„æåŠé¢‘ç‡
        mention_count = syllabus_analysis.get_mention_count(point.name)

        # æŸ¥æ‰¾å…³é”®è¯å¯†åº¦
        keyword_density = syllabus_analysis.get_keyword_density(point.keywords)

        # æŸ¥æ‰¾åœ¨é‡è¦ç« èŠ‚ä¸­çš„ä½ç½®
        section_importance = syllabus_analysis.get_section_importance(point)

        # ç»¼åˆè®¡ç®—
        emphasis_score = (
            min(mention_count / 10, 1.0) * 0.4 +
            keyword_density * 0.3 +
            section_importance * 0.3
        )

        return emphasis_score

    def _calculate_exam_frequency_score(self, point: KnowledgePoint) -> float:
        """è®¡ç®—è€ƒè¯•å‡ºç°é¢‘ç‡åˆ†æ•°"""

        # ä»å†å²è€ƒè¯•æ•°æ®ä¸­è·å–å‡ºç°é¢‘ç‡
        historical_frequency = self._get_historical_frequency(point)

        # ä»é¢˜å‹åˆ†å¸ƒä¸­è·å–æƒé‡
        question_type_weight = self._get_question_type_weight(point)

        # ä»åˆ†å€¼åˆ†å¸ƒä¸­è·å–æƒé‡
        score_weight = self._get_score_weight(point)

        frequency_score = (
            historical_frequency * 0.5 +
            question_type_weight * 0.3 +
            score_weight * 0.2
        )

        return frequency_score

class HourAllocationOptimizer:
    """è¯¾æ—¶åˆ†é…ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.allocation_strategies = {
            'importance_weighted': ImportanceWeightedStrategy(),
            'difficulty_adjusted': DifficultyAdjustedStrategy(),
            'balanced': BalancedStrategy(),
            'learning_path_optimized': LearningPathOptimizedStrategy()
        }

    def optimize_allocation(
        self,
        knowledge_hierarchy: KnowledgeHierarchy,
        importance_weights: Dict[str, float],
        constraints: AllocationConstraints
    ) -> OptimizedAllocation:
        """ä¼˜åŒ–è¯¾æ—¶åˆ†é…"""

        # 1. åŸºç¡€åˆ†é…è®¡ç®—
        base_allocation = self._calculate_base_allocation(
            knowledge_hierarchy, importance_weights, constraints
        )

        # 2. çº¦æŸæ¡ä»¶åº”ç”¨
        constrained_allocation = self._apply_constraints(
            base_allocation, constraints
        )

        # 3. å­¦ä¹ è·¯å¾„ä¼˜åŒ–
        path_optimized_allocation = self._optimize_learning_path(
            constrained_allocation, knowledge_hierarchy
        )

        # 4. è¯¾æ—¶æ¨¡å¼è°ƒæ•´
        mode_adjusted_allocation = self._adjust_hour_modes(
            path_optimized_allocation, constraints.hour_modes
        )

        # 5. è´¨é‡è¯„ä¼°å’Œå¾®è°ƒ
        final_allocation = self._fine_tune_allocation(
            mode_adjusted_allocation, knowledge_hierarchy, constraints
        )

        return final_allocation

    def _calculate_base_allocation(
        self,
        knowledge_hierarchy: KnowledgeHierarchy,
        importance_weights: Dict[str, float],
        constraints: AllocationConstraints
    ) -> BaseAllocation:
        """è®¡ç®—åŸºç¡€è¯¾æ—¶åˆ†é…"""

        total_hours = constraints.total_hours
        knowledge_points = knowledge_hierarchy.knowledge_points

        # è®¡ç®—æƒé‡æ€»å’Œ
        total_weight = sum(importance_weights.values())

        allocations = {}

        for point in knowledge_points:
            # åŸºäºé‡è¦æ€§æƒé‡çš„åŸºç¡€åˆ†é…
            weight_ratio = importance_weights.get(point.id, 0) / total_weight
            base_hours = total_hours * weight_ratio

            # éš¾åº¦è°ƒæ•´
            difficulty_multiplier = self._get_difficulty_multiplier(point.difficulty_level)
            adjusted_hours = base_hours * difficulty_multiplier

            # æœ€å°è¯¾æ—¶ä¿è¯
            min_hours = constraints.min_hours_per_point
            final_hours = max(adjusted_hours, min_hours)

            allocations[point.id] = {
                'knowledge_point': point,
                'base_hours': base_hours,
                'difficulty_adjusted_hours': adjusted_hours,
                'final_hours': final_hours,
                'allocation_rationale': self._generate_allocation_rationale(
                    point, weight_ratio, difficulty_multiplier
                )
            }

        return BaseAllocation(allocations)

    def _optimize_learning_path(
        self,
        allocation: ConstrainedAllocation,
        knowledge_hierarchy: KnowledgeHierarchy
    ) -> PathOptimizedAllocation:
        """ä¼˜åŒ–å­¦ä¹ è·¯å¾„"""

        # æ„å»ºä¾èµ–å…³ç³»å›¾
        dependency_graph = knowledge_hierarchy.dependency_graph

        # æ‹“æ‰‘æ’åºç¡®å®šå­¦ä¹ é¡ºåº
        learning_order = self._topological_sort(dependency_graph)

        # æ ¹æ®ä¾èµ–å…³ç³»è°ƒæ•´è¯¾æ—¶åˆ†é…
        optimized_allocations = {}

        for point_id in learning_order:
            point_allocation = allocation.allocations[point_id]

            # æ£€æŸ¥å‰ç½®çŸ¥è¯†ç‚¹çš„è¯¾æ—¶åˆ†é…
            prerequisites = dependency_graph.get_prerequisites(point_id)
            prerequisite_hours = sum(
                optimized_allocations.get(prereq_id, {}).get('final_hours', 0)
                for prereq_id in prerequisites
            )

            # æ ¹æ®å‰ç½®çŸ¥è¯†ç‚¹è°ƒæ•´å½“å‰çŸ¥è¯†ç‚¹çš„è¯¾æ—¶
            if prerequisite_hours > 0:
                # å¦‚æœå‰ç½®çŸ¥è¯†ç‚¹è¯¾æ—¶è¾ƒå¤šï¼Œå¯ä»¥é€‚å½“å‡å°‘å½“å‰çŸ¥è¯†ç‚¹è¯¾æ—¶
                adjustment_factor = min(1.0, 1.0 - (prerequisite_hours / 20) * 0.1)
                adjusted_hours = point_allocation['final_hours'] * adjustment_factor
            else:
                adjusted_hours = point_allocation['final_hours']

            optimized_allocations[point_id] = {
                **point_allocation,
                'path_optimized_hours': adjusted_hours,
                'learning_order': learning_order.index(point_id)
            }

        return PathOptimizedAllocation(optimized_allocations, learning_order)
```

## ğŸ—ï¸ æœåŠ¡æ¶æ„è®¾è®¡

### å¾®æœåŠ¡æ¶æ„

```python
# æœåŠ¡æ³¨å†Œå’Œå‘ç°
class ServiceRegistry:
    """æœåŠ¡æ³¨å†Œä¸­å¿ƒ"""

    def __init__(self):
        self.services = {}
        self.health_checkers = {}

    def register_service(self, service_name: str, endpoint: str, health_check_url: str):
        """æ³¨å†ŒæœåŠ¡"""
        self.services[service_name] = {
            'endpoint': endpoint,
            'health_check_url': health_check_url,
            'status': 'healthy',
            'last_check': timezone.now()
        }

    async def get_healthy_endpoint(self, service_name: str) -> Optional[str]:
        """è·å–å¥åº·çš„æœåŠ¡ç«¯ç‚¹"""
        service_info = self.services.get(service_name)

        if not service_info:
            return None

        # æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
        if await self._check_service_health(service_name):
            return service_info['endpoint']

        return None

    async def _check_service_health(self, service_name: str) -> bool:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        service_info = self.services[service_name]

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    service_info['health_check_url'],
                    timeout=aiohttp.ClientTimeout(total=5)
                ) as response:
                    is_healthy = response.status == 200

            service_info['status'] = 'healthy' if is_healthy else 'unhealthy'
            service_info['last_check'] = timezone.now()

            return is_healthy

        except Exception:
            service_info['status'] = 'unhealthy'
            service_info['last_check'] = timezone.now()
            return False

# æœåŠ¡é—´é€šä¿¡
class ServiceCommunicator:
    """æœåŠ¡é—´é€šä¿¡ç®¡ç†å™¨"""

    def __init__(self, service_registry: ServiceRegistry):
        self.service_registry = service_registry
        self.circuit_breakers = {}
        self.retry_policies = {}

    async def call_service(
        self,
        service_name: str,
        method: str,
        endpoint: str,
        data: Optional[Dict] = None,
        timeout: int = 30
    ) -> Dict:
        """è°ƒç”¨æœåŠ¡"""

        # è·å–æœåŠ¡ç«¯ç‚¹
        base_url = await self.service_registry.get_healthy_endpoint(service_name)
        if not base_url:
            raise ServiceUnavailableError(f"Service {service_name} is not available")

        # æ£€æŸ¥ç†”æ–­å™¨çŠ¶æ€
        circuit_breaker = self.circuit_breakers.get(service_name)
        if circuit_breaker and circuit_breaker.is_open():
            raise CircuitBreakerOpenError(f"Circuit breaker is open for {service_name}")

        # æ‰§è¡Œè¯·æ±‚
        full_url = f"{base_url}{endpoint}"

        try:
            async with aiohttp.ClientSession() as session:
                async with session.request(
                    method,
                    full_url,
                    json=data,
                    timeout=aiohttp.ClientTimeout(total=timeout)
                ) as response:

                    if response.status >= 400:
                        raise ServiceError(f"Service error: {response.status}")

                    result = await response.json()

                    # è®°å½•æˆåŠŸè°ƒç”¨
                    if circuit_breaker:
                        circuit_breaker.record_success()

                    return result

        except Exception as e:
            # è®°å½•å¤±è´¥è°ƒç”¨
            if circuit_breaker:
                circuit_breaker.record_failure()

            # é‡è¯•é€»è¾‘
            retry_policy = self.retry_policies.get(service_name)
            if retry_policy and retry_policy.should_retry(e):
                await asyncio.sleep(retry_policy.get_delay())
                return await self.call_service(service_name, method, endpoint, data, timeout)

            raise
```

### å¼‚æ­¥ä»»åŠ¡å¤„ç†

```python
# Celeryä»»åŠ¡å®šä¹‰
from celery import Celery

app = Celery('teaching_syllabus_system')

@app.task(bind=True, max_retries=3, default_retry_delay=60)
def process_iterative_analysis(self, session_id: str, config: Dict):
    """å¼‚æ­¥å¤„ç†å¤šè½®è¿­ä»£åˆ†æ"""

    try:
        # è·å–åˆ†æä¼šè¯
        session = IterativeAnalysisSession.objects.get(id=session_id)

        # åˆ›å»ºåˆ†æå¼•æ“
        engine = IterativeAnalysisEngine()

        # æ‰§è¡Œåˆ†æ
        result = asyncio.run(engine.start_analysis_session(session, config))

        # æ›´æ–°ä¼šè¯çŠ¶æ€
        session.mark_completed(result)

        # å‘é€å®Œæˆé€šçŸ¥
        send_analysis_completion_notification.delay(session_id)

        return {
            'success': True,
            'session_id': session_id,
            'result_summary': result.get_summary()
        }

    except Exception as exc:
        # è®°å½•é”™è¯¯
        logger.error(f"Analysis failed for session {session_id}: {exc}")

        # æ›´æ–°ä¼šè¯çŠ¶æ€
        session = IterativeAnalysisSession.objects.get(id=session_id)
        session.mark_failed(str(exc))

        # é‡è¯•é€»è¾‘
        if self.request.retries < self.max_retries:
            raise self.retry(countdown=60 * (2 ** self.request.retries))
        else:
            # å‘é€å¤±è´¥é€šçŸ¥
            send_analysis_failure_notification.delay(session_id, str(exc))
            raise

@app.task
def calculate_knowledge_mapping(syllabus_session_id: str, textbook_session_id: str):
    """å¼‚æ­¥è®¡ç®—çŸ¥è¯†ç‚¹æ˜ å°„"""

    try:
        # è·å–åˆ†æç»“æœ
        syllabus_session = IterativeAnalysisSession.objects.get(id=syllabus_session_id)
        textbook_session = IterativeAnalysisSession.objects.get(id=textbook_session_id)

        # åˆ›å»ºæ˜ å°„å¼•æ“
        mapping_engine = KnowledgePointMappingEngine()

        # æ‰§è¡Œæ˜ å°„è®¡ç®—
        mapping_result = asyncio.run(
            mapping_engine.create_knowledge_mapping(
                syllabus_session.final_result,
                textbook_session.final_result
            )
        )

        # ä¿å­˜æ˜ å°„ç»“æœ
        knowledge_hierarchy = KnowledgePointHierarchy.objects.create(
            workflow=syllabus_session.workflow,
            source_analysis_session=syllabus_session,
            hierarchy_type='integrated',
            hierarchy_data=mapping_result.hierarchy,
            importance_weights=mapping_result.weights,
            dependency_graph=mapping_result.dependencies
        )

        # è§¦å‘è¯¾æ—¶åˆ†é…è®¡ç®—
        calculate_hour_allocation.delay(knowledge_hierarchy.id)

        return {
            'success': True,
            'mapping_id': knowledge_hierarchy.id
        }

    except Exception as exc:
        logger.error(f"Knowledge mapping failed: {exc}")
        raise

@app.task
def calculate_hour_allocation(knowledge_hierarchy_id: str):
    """å¼‚æ­¥è®¡ç®—è¯¾æ—¶åˆ†é…"""

    try:
        # è·å–çŸ¥è¯†ç‚¹å±‚çº§
        knowledge_hierarchy = KnowledgePointHierarchy.objects.get(id=knowledge_hierarchy_id)

        # è·å–è¯¾ç¨‹é…ç½®
        workflow = knowledge_hierarchy.workflow
        course_config = workflow.workflow_config.get('course_config', {})

        # åˆ›å»ºåˆ†é…å¼•æ“
        allocation_engine = SmartHourAllocationEngine()

        # è®¡ç®—æœ€ä¼˜åˆ†é…
        allocation_result = asyncio.run(
            allocation_engine.calculate_optimal_allocation(
                knowledge_hierarchy,
                course_config
            )
        )

        # ä¿å­˜åˆ†é…ç»“æœ
        hour_allocation = CourseHourAllocation.objects.create(
            workflow=workflow,
            knowledge_hierarchy=knowledge_hierarchy,
            allocation_strategy=course_config.get('allocation_strategy', 'importance_weighted'),
            total_hours=course_config.get('total_hours', 48),
            hour_distribution=allocation_result.allocation_data,
            allocation_rationale=allocation_result.rationale,
            allocation_quality_score=allocation_result.quality_score
        )

        # æ¨è¿›å·¥ä½œæµåˆ°ä¸‹ä¸€é˜¶æ®µ
        advance_workflow_stage.delay(workflow.id)

        return {
            'success': True,
            'allocation_id': hour_allocation.id
        }

    except Exception as exc:
        logger.error(f"Hour allocation failed: {exc}")
        raise
```

## ğŸš€ DeepSeekæ¨¡å‹ä¼˜åŒ–é›†æˆ

### ä¼˜åŒ–åçš„AIåˆ†ææœåŠ¡

åŸºäºDeepSeekæ¨¡å‹ç‰¹æ€§ï¼Œæˆ‘ä»¬å¯¹AIåˆ†ææœåŠ¡è¿›è¡Œäº†å…¨é¢ä¼˜åŒ–ï¼š

```python
class OptimizedAISyllabusAnalyzer:
    """ä¼˜åŒ–çš„AIè€ƒçº²åˆ†æå™¨"""

    def __init__(self):
        self.deepseek_service = OptimizedDeepSeekService()
        self.prompt_optimizer = DeepSeekPromptOptimizer()

    async def analyze_syllabus_optimized(
        self,
        file: UploadedFile,
        config: AnalysisConfig
    ) -> AnalysisResult:
        """ä¼˜åŒ–çš„è€ƒçº²åˆ†æ"""

        # 1. æ–‡æ¡£é¢„å¤„ç†å’Œé•¿ä¸Šä¸‹æ–‡ä¼˜åŒ–
        processed_content = await self._preprocess_for_long_context(file)

        # 2. ç¬¬ä¸€è½®ï¼šä½¿ç”¨æ¨ç†æ¨¡å‹è¿›è¡Œæ¡†æ¶åˆ†æ
        framework_analysis = await self.deepseek_service.analyze_with_reasoning(
            content=processed_content,
            analysis_type="structure_analysis",
            context={"document_type": "syllabus", "analysis_depth": "comprehensive"}
        )

        # 3. åŸºäºæ¡†æ¶ç»“æœçš„å¤šè½®æ·±åº¦åˆ†æ
        detailed_analyses = await asyncio.gather(
            self._analyze_knowledge_points_optimized(processed_content, framework_analysis),
            self._analyze_difficulty_levels_optimized(processed_content, framework_analysis),
            self._analyze_skill_requirements_optimized(processed_content, framework_analysis),
            self._assess_teaching_objectives_optimized(processed_content, framework_analysis)
        )

        # 4. æ•´åˆåˆ†æç»“æœ
        integrated_result = await self._integrate_analysis_results(
            framework_analysis,
            detailed_analyses
        )

        return integrated_result

    async def _analyze_knowledge_points_optimized(
        self,
        content: str,
        framework: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„çŸ¥è¯†ç‚¹åˆ†æ"""

        # æ„å»ºåŸºäºæ¡†æ¶çš„çŸ¥è¯†ç‚¹åˆ†ææç¤ºè¯
        analysis_prompt = self.prompt_optimizer.build_reasoning_prompt(
            problem_statement="åŸºäºæ–‡æ¡£æ¡†æ¶åˆ†æï¼Œæ·±åº¦æå–å’Œåˆ†æçŸ¥è¯†ç‚¹",
            analysis_context={
                "document_content": content[:30000],  # åˆ©ç”¨é•¿ä¸Šä¸‹æ–‡
                "framework_structure": framework.get("reasoning_process", {}),
                "identified_sections": framework.get("final_answer", {}).get("sections", [])
            },
            expected_output="""
            {
                "knowledge_points": [
                    {
                        "id": "kp_001",
                        "name": "çŸ¥è¯†ç‚¹åç§°",
                        "category": "åˆ†ç±»",
                        "importance_level": "high/medium/low",
                        "difficulty_level": "beginner/intermediate/advanced",
                        "prerequisites": ["å‰ç½®çŸ¥è¯†ç‚¹"],
                        "learning_objectives": ["å­¦ä¹ ç›®æ ‡"],
                        "assessment_methods": ["è¯„ä¼°æ–¹å¼"]
                    }
                ],
                "knowledge_hierarchy": {...},
                "cross_references": {...}
            }
            """
        )

        return await self.deepseek_service.analyze_with_reasoning(
            content=analysis_prompt,
            analysis_type="knowledge_extraction"
        )
```

### æ™ºèƒ½æ¨¡å‹é€‰æ‹©ç­–ç•¥

```python
class IntelligentModelSelector:
    """æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨"""

    MODEL_SELECTION_MATRIX = {
        ("analysis", "high_complexity"): {
            "model": "deepseek-reasoner",
            "temperature": 0.6,
            "max_tokens": 4000,
            "reasoning_required": True
        },
        ("generation", "structured"): {
            "model": "deepseek-chat",
            "temperature": 0.3,
            "max_tokens": 6000,
            "reasoning_required": False
        },
        ("mapping", "semantic"): {
            "model": "deepseek-reasoner",
            "temperature": 0.5,
            "max_tokens": 3500,
            "reasoning_required": True
        },
        ("allocation", "optimization"): {
            "model": "deepseek-reasoner",
            "temperature": 0.4,
            "max_tokens": 4000,
            "reasoning_required": True
        }
    }

    def select_optimal_configuration(
        self,
        task_type: str,
        complexity: str,
        content_length: int
    ) -> Dict[str, Any]:
        """é€‰æ‹©æœ€ä¼˜é…ç½®"""

        # åŸºç¡€é…ç½®é€‰æ‹©
        base_config = self.MODEL_SELECTION_MATRIX.get(
            (task_type, complexity),
            self.MODEL_SELECTION_MATRIX[("generation", "structured")]
        )

        # åŸºäºå†…å®¹é•¿åº¦çš„åŠ¨æ€è°ƒæ•´
        if content_length > 50000:  # é•¿æ–‡æ¡£
            adjusted_config = base_config.copy()
            adjusted_config["temperature"] = max(0.2, base_config["temperature"] - 0.1)
            adjusted_config["max_tokens"] = min(8000, base_config["max_tokens"] + 1000)
            return adjusted_config

        return base_config
```

### é•¿ä¸Šä¸‹æ–‡ä¼˜åŒ–ç­–ç•¥

```python
class LongContextProcessor:
    """é•¿ä¸Šä¸‹æ–‡å¤„ç†å™¨"""

    MAX_CONTEXT_TOKENS = 120000  # DeepSeek-V3çš„128Kä¸Šä¸‹æ–‡é™åˆ¶

    async def optimize_context_usage(
        self,
        primary_content: str,
        supporting_context: Dict[str, Any],
        task_requirements: str
    ) -> str:
        """ä¼˜åŒ–ä¸Šä¸‹æ–‡ä½¿ç”¨"""

        # 1. è®¡ç®—tokenä½¿ç”¨åˆ†é…
        allocation = self._calculate_token_allocation(
            len(primary_content),
            len(str(supporting_context)),
            len(task_requirements)
        )

        # 2. æ™ºèƒ½å†…å®¹æˆªå–å’Œå‹ç¼©
        optimized_primary = self._smart_truncate(
            primary_content,
            allocation["primary_tokens"]
        )

        compressed_context = self._compress_supporting_context(
            supporting_context,
            allocation["context_tokens"]
        )

        # 3. æ„å»ºæœ€ç»ˆä¸Šä¸‹æ–‡
        final_context = self._build_optimized_context(
            optimized_primary,
            compressed_context,
            task_requirements
        )

        return final_context

    def _smart_truncate(self, content: str, max_tokens: int) -> str:
        """æ™ºèƒ½æˆªå–å†…å®¹"""

        if len(content) <= max_tokens:
            return content

        # åˆ†æ®µç­–ç•¥ï¼šä¿ç•™å¼€å¤´ã€ç»“å°¾å’Œå…³é”®ä¸­é—´æ®µè½
        sections = self._identify_key_sections(content)

        # æŒ‰é‡è¦æ€§æ’åºå¹¶é€‰æ‹©
        selected_sections = self._select_by_importance(sections, max_tokens)

        return self._reconstruct_content(selected_sections)

    def _identify_key_sections(self, content: str) -> List[Dict[str, Any]]:
        """è¯†åˆ«å…³é”®æ®µè½"""

        # ä½¿ç”¨å¯å‘å¼æ–¹æ³•è¯†åˆ«é‡è¦æ®µè½
        paragraphs = content.split('\n\n')
        key_sections = []

        for i, paragraph in enumerate(paragraphs):
            importance_score = self._calculate_paragraph_importance(paragraph, i, len(paragraphs))
            key_sections.append({
                "content": paragraph,
                "position": i,
                "importance": importance_score,
                "length": len(paragraph)
            })

        return sorted(key_sections, key=lambda x: x["importance"], reverse=True)
```

### ç¼“å­˜ä¼˜åŒ–å®ç°

```python
class DeepSeekCacheOptimizer:
    """DeepSeekç¼“å­˜ä¼˜åŒ–å™¨"""

    def __init__(self):
        self.cache_strategies = {
            "document_analysis": {"ttl": 86400, "compression": True},
            "knowledge_mapping": {"ttl": 43200, "compression": True},
            "hour_allocation": {"ttl": 21600, "compression": False}
        }

    async def get_or_compute_with_cache(
        self,
        operation_type: str,
        input_data: Dict[str, Any],
        compute_function: callable,
        model_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """å¸¦ç¼“å­˜çš„è®¡ç®—"""

        # 1. ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_intelligent_cache_key(
            operation_type,
            input_data,
            model_config
        )

        # 2. å°è¯•ä»ç¼“å­˜è·å–
        cached_result = await self._get_from_cache(cache_key, operation_type)
        if cached_result:
            return {
                **cached_result,
                "from_cache": True,
                "cache_hit": True,
                "cost_saved": self._calculate_cost_saved(operation_type)
            }

        # 3. æ‰§è¡Œè®¡ç®—
        start_time = time.time()
        result = await compute_function()
        computation_time = time.time() - start_time

        # 4. ç¼“å­˜ç»“æœ
        await self._store_to_cache(cache_key, result, operation_type)

        return {
            **result,
            "from_cache": False,
            "cache_hit": False,
            "computation_time": computation_time
        }

    def _generate_intelligent_cache_key(
        self,
        operation_type: str,
        input_data: Dict[str, Any],
        model_config: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆæ™ºèƒ½ç¼“å­˜é”®"""

        # å†…å®¹å“ˆå¸Œ
        content_parts = []
        for key in sorted(input_data.keys()):
            if key.endswith("_content") or key.endswith("_text"):
                # å¯¹é•¿æ–‡æœ¬å†…å®¹ç”Ÿæˆå“ˆå¸Œ
                content_hash = hashlib.md5(str(input_data[key]).encode()).hexdigest()[:16]
                content_parts.append(f"{key}:{content_hash}")
            else:
                content_parts.append(f"{key}:{input_data[key]}")

        # æ¨¡å‹é…ç½®å“ˆå¸Œ
        model_signature = hashlib.md5(
            json.dumps(model_config, sort_keys=True).encode()
        ).hexdigest()[:8]

        # ç»„åˆç¼“å­˜é”®
        cache_key = f"deepseek:{operation_type}:{':'.join(content_parts)}:model:{model_signature}"

        return cache_key
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [DeepSeekä¼˜åŒ–ç­–ç•¥](./deepseek-optimization-strategy.md)
- [DeepSeekå®ç°ç¤ºä¾‹](./deepseek-implementation-examples.md)
- [å‰ç«¯ç•Œé¢è®¾è®¡](./teaching-syllabus-frontend-design.md)
- [å®æ–½è®¡åˆ’å’Œé‡Œç¨‹ç¢‘](./teaching-syllabus-implementation-plan.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.1
**åˆ›å»ºæ—¥æœŸ**: 2025-01-22
**æœ€åæ›´æ–°**: 2025-01-22
