# æ•™å­¦å¤§çº²ç”Ÿæˆç³»ç»Ÿæ¶æ„æ”¹è¿›æ–¹æ¡ˆ

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†æ•™å­¦å¤§çº²ç”Ÿæˆç³»ç»Ÿçš„æ¶æ„æ”¹è¿›æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ–°æ¶æ„è®¾è®¡ã€æ ¸å¿ƒç»„ä»¶è§„åˆ’ã€æŠ€æœ¯é€‰å‹å’Œé›†æˆç­–ç•¥ã€‚

## ğŸ—ï¸ æ–°æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "ç”¨æˆ·ç•Œé¢å±‚"
        A[å·¥ä½œæµå¯è§†åŒ–ç•Œé¢]
        B[å¤šè½®åˆ†æç›‘æ§ç•Œé¢]
        C[çŸ¥è¯†ç‚¹æ˜ å°„å¯è§†åŒ–]
        D[è¯¾æ—¶åˆ†é…äº¤äº’ç•Œé¢]
        E[æ•™æ¡ˆç¼–è¾‘å™¨]
    end

    subgraph "ä¸šåŠ¡é€»è¾‘å±‚"
        F[å·¥ä½œæµå¼•æ“]
        G[å¤šè½®è¿­ä»£AIåˆ†æå¼•æ“]
        H[çŸ¥è¯†ç‚¹æ˜ å°„å¼•æ“]
        I[æ™ºèƒ½è¯¾æ—¶åˆ†é…å¼•æ“]
        J[ä¸ªæ€§åŒ–å†…å®¹ç”Ÿæˆå¼•æ“]
    end

    subgraph "æœåŠ¡å±‚"
        K[è´¨é‡æ§åˆ¶æœåŠ¡]
        L[çŠ¶æ€ç®¡ç†æœåŠ¡]
        M[äººå·¥ç›‘ç£æœåŠ¡]
        N[é€šçŸ¥æœåŠ¡]
    end

    subgraph "æ•°æ®å±‚"
        O[å·¥ä½œæµçŠ¶æ€å­˜å‚¨]
        P[åˆ†æç»“æœå­˜å‚¨]
        Q[çŸ¥è¯†ç‚¹å›¾è°±å­˜å‚¨]
        R[ç¼“å­˜å±‚]
    end

    subgraph "å¤–éƒ¨æœåŠ¡"
        S[DeepSeek AI API]
        T[æ–‡æ¡£å¤„ç†æœåŠ¡]
        U[æ—¶æ”¿å†…å®¹API]
    end

    A --> F
    B --> G
    C --> H
    D --> I
    E --> J

    F --> K
    G --> K
    H --> K
    I --> K
    J --> K

    K --> L
    K --> M
    K --> N

    L --> O
    G --> P
    H --> Q
    All --> R

    G --> S
    J --> S
    G --> T
    J --> U
```

### æ ¸å¿ƒæ¶æ„åŸåˆ™

1. **åˆ†å±‚è§£è€¦** - æ¸…æ™°çš„åˆ†å±‚æ¶æ„ï¼Œé™ä½ç»„ä»¶é—´è€¦åˆ
2. **å¾®æœåŠ¡åŒ–** - æ ¸å¿ƒåŠŸèƒ½æ¨¡å—åŒ–ï¼Œæ”¯æŒç‹¬ç«‹éƒ¨ç½²å’Œæ‰©å±•
3. **äº‹ä»¶é©±åŠ¨** - åŸºäºäº‹ä»¶çš„å¼‚æ­¥å¤„ç†ï¼Œæé«˜ç³»ç»Ÿå“åº”æ€§
4. **çŠ¶æ€ç®¡ç†** - é›†ä¸­å¼çŠ¶æ€ç®¡ç†ï¼Œæ”¯æŒé•¿æ—¶é—´ä»»åŠ¡å¤„ç†
5. **è´¨é‡ä¼˜å…ˆ** - å†…ç½®è´¨é‡æ§åˆ¶æœºåˆ¶ï¼Œç¡®ä¿è¾“å‡ºè´¨é‡

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è®¾è®¡

### 1. å·¥ä½œæµå¼•æ“ (Workflow Engine)

#### åŠŸèƒ½èŒè´£

- ç®¡ç†6é˜¶æ®µæ•™å­¦å‡†å¤‡å·¥ä½œæµç¨‹
- æ§åˆ¶é˜¶æ®µé—´çš„æµè½¬å’Œä¾èµ–å…³ç³»
- æ”¯æŒäººå·¥å¹²é¢„å’Œå®¡æ ¸èŠ‚ç‚¹
- æä¾›å·¥ä½œæµçŠ¶æ€æŸ¥è¯¢å’Œç®¡ç†

#### æŠ€æœ¯å®ç°

```python
class TeachingPreparationWorkflowEngine:
    """æ•™å­¦å‡†å¤‡å·¥ä½œæµå¼•æ“"""

    def __init__(self):
        self.state_manager = WorkflowStateManager()
        self.stage_processors = {
            'course_info': CourseInfoProcessor(),
            'resource_collection': ResourceCollectionProcessor(),
            'syllabus_analysis': SyllabusAnalysisProcessor(),
            'textbook_analysis': TextbookAnalysisProcessor(),
            'syllabus_generation': SyllabusGenerationProcessor(),
            'lesson_plan_generation': LessonPlanGenerationProcessor(),
        }
        self.quality_gates = QualityGateManager()

    async def start_workflow(self, course_assignment_id: int, config: Dict):
        """å¯åŠ¨å·¥ä½œæµ"""
        workflow = await self._create_workflow(course_assignment_id, config)
        await self._advance_to_first_stage(workflow)
        return workflow

    async def advance_stage(self, workflow_id: int, user_approval: bool = False):
        """æ¨è¿›åˆ°ä¸‹ä¸€é˜¶æ®µ"""
        workflow = await self.state_manager.get_workflow(workflow_id)

        # è´¨é‡é—¨æ§æ£€æŸ¥
        if not await self.quality_gates.check_stage_completion(workflow):
            raise WorkflowError("å½“å‰é˜¶æ®µè´¨é‡æ£€æŸ¥æœªé€šè¿‡")

        # äººå·¥å®¡æ ¸èŠ‚ç‚¹
        if self._requires_human_approval(workflow.current_stage):
            if not user_approval:
                raise WorkflowError("éœ€è¦äººå·¥å®¡æ ¸ç¡®è®¤")

        # æ¨è¿›åˆ°ä¸‹ä¸€é˜¶æ®µ
        next_stage = self._get_next_stage(workflow.current_stage)
        await self._transition_to_stage(workflow, next_stage)

    async def _transition_to_stage(self, workflow, stage):
        """é˜¶æ®µè½¬æ¢å¤„ç†"""
        # æ›´æ–°å·¥ä½œæµçŠ¶æ€
        workflow.current_stage = stage
        workflow.stage_start_time = timezone.now()

        # å¯åŠ¨é˜¶æ®µå¤„ç†å™¨
        processor = self.stage_processors[stage]
        await processor.start_processing(workflow)

        # ä¿å­˜çŠ¶æ€
        await self.state_manager.save_workflow_state(workflow)

        # å‘é€é€šçŸ¥
        await self._notify_stage_transition(workflow, stage)
```

#### çŠ¶æ€ç®¡ç†è®¾è®¡

```python
class WorkflowStateManager:
    """å·¥ä½œæµçŠ¶æ€ç®¡ç†å™¨"""

    def __init__(self):
        self.redis_client = redis.Redis()
        self.db_manager = DatabaseManager()

    async def save_workflow_state(self, workflow):
        """ä¿å­˜å·¥ä½œæµçŠ¶æ€"""
        # æŒä¹…åŒ–åˆ°æ•°æ®åº“
        await self.db_manager.save_workflow(workflow)

        # ç¼“å­˜åˆ°Redis
        state_key = f"workflow:{workflow.id}:state"
        await self.redis_client.setex(
            state_key,
            3600,  # 1å°æ—¶è¿‡æœŸ
            json.dumps(workflow.to_dict())
        )

    async def restore_workflow_state(self, workflow_id):
        """æ¢å¤å·¥ä½œæµçŠ¶æ€"""
        # å…ˆå°è¯•ä»ç¼“å­˜è·å–
        state_key = f"workflow:{workflow_id}:state"
        cached_state = await self.redis_client.get(state_key)

        if cached_state:
            return Workflow.from_dict(json.loads(cached_state))

        # ä»æ•°æ®åº“æ¢å¤
        return await self.db_manager.get_workflow(workflow_id)
```

### 2. å¤šè½®è¿­ä»£AIåˆ†æå¼•æ“ (Iterative AI Analysis Engine)

#### åŠŸèƒ½èŒè´£

- æ”¯æŒ3-5è½®é€’å½’åˆ†æ
- åŸºäºä¸Šä¸‹æ–‡çš„æ™ºèƒ½åˆ†æ
- è´¨é‡é©±åŠ¨çš„è¿­ä»£ç»ˆæ­¢
- åˆ†æç»“æœçš„æ•´åˆå’Œä¼˜åŒ–

#### æ ¸å¿ƒç®—æ³•è®¾è®¡

```python
class IterativeAnalysisEngine:
    """å¤šè½®è¿­ä»£AIåˆ†æå¼•æ“"""

    def __init__(self):
        self.ai_service = DeepSeekAPIService()
        self.quality_evaluator = AnalysisQualityEvaluator()
        self.context_builder = AnalysisContextBuilder()

    async def start_analysis_session(self, document, config):
        """å¯åŠ¨åˆ†æä¼šè¯"""
        session = AnalysisSession.create(
            document=document,
            max_rounds=config.get('max_rounds', 5),
            quality_threshold=config.get('quality_threshold', 0.85)
        )

        # ç¬¬ä¸€è½®ï¼šæ¡†æ¶åˆ†æ
        round1_result = await self._analyze_framework(document)
        session.add_round_result(1, round1_result)

        # è¿­ä»£åˆ†æå¾ªç¯
        while not self._should_terminate_analysis(session):
            await self._execute_next_round(session, document)

        # æœ€ç»ˆæ•´åˆ
        final_result = await self._integrate_analysis_results(session)
        session.mark_completed(final_result)

        return session

    async def _execute_next_round(self, session, document):
        """æ‰§è¡Œä¸‹ä¸€è½®åˆ†æ"""
        round_number = session.current_round + 1

        # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
        context = self.context_builder.build_context(session)

        # ç¡®å®šåˆ†æç„¦ç‚¹
        focus_areas = self._determine_focus_areas(session, context)

        # æ‰§è¡Œåˆ†æ
        round_result = await self._analyze_with_context(
            document, context, focus_areas, round_number
        )

        # è´¨é‡è¯„ä¼°
        quality_score = self.quality_evaluator.evaluate_round(
            session, round_result
        )
        round_result.quality_score = quality_score

        # æ·»åŠ åˆ°ä¼šè¯
        session.add_round_result(round_number, round_result)

    def _should_terminate_analysis(self, session):
        """åˆ¤æ–­æ˜¯å¦ç»ˆæ­¢åˆ†æ"""
        # è¾¾åˆ°æœ€å¤§è½®æ¬¡
        if session.current_round >= session.max_rounds:
            session.termination_reason = 'max_rounds_reached'
            return True

        # è´¨é‡é˜ˆå€¼æ»¡è¶³
        if session.get_latest_quality_score() >= session.quality_threshold:
            session.termination_reason = 'quality_threshold_met'
            return True

        # è´¨é‡æ”¶æ•›æ£€æŸ¥
        if self._is_quality_converged(session):
            session.termination_reason = 'quality_converged'
            return True

        # å†…å®¹é¥±å’Œåº¦æ£€æŸ¥
        if self._is_content_saturated(session):
            session.termination_reason = 'content_saturated'
            return True

        return False

    def _is_quality_converged(self, session):
        """æ£€æŸ¥è´¨é‡æ˜¯å¦æ”¶æ•›"""
        if len(session.round_results) < 3:
            return False

        recent_scores = [
            r.quality_score for r in session.round_results[-3:]
        ]

        # è®¡ç®—è´¨é‡æ”¹è¿›ç‡
        improvement_rate = (recent_scores[-1] - recent_scores[0]) / recent_scores[0]
        return improvement_rate < 0.05  # æ”¹è¿›ç‡ä½äº5%

    def _is_content_saturated(self, session):
        """æ£€æŸ¥å†…å®¹æ˜¯å¦é¥±å’Œ"""
        if len(session.round_results) < 2:
            return False

        # è®¡ç®—æ–°å¢å†…å®¹æ¯”ä¾‹
        latest_content = session.round_results[-1].extracted_content
        previous_content = session.round_results[-2].extracted_content

        new_content_ratio = self._calculate_new_content_ratio(
            latest_content, previous_content
        )

        return new_content_ratio < 0.1  # æ–°å¢å†…å®¹å°‘äº10%
```

### 3. çŸ¥è¯†ç‚¹æ˜ å°„å¼•æ“ (Knowledge Mapping Engine)

#### åŠŸèƒ½èŒè´£

- è€ƒçº²ä¸æ•™æçš„æ™ºèƒ½åŒ¹é…
- çŸ¥è¯†ç‚¹å±‚çº§å…³ç³»æ„å»º
- é‡è¦æ€§æƒé‡è®¡ç®—
- ä¾èµ–å…³ç³»åˆ†æ

#### æ˜ å°„ç®—æ³•è®¾è®¡

```python
class KnowledgePointMappingEngine:
    """çŸ¥è¯†ç‚¹æ˜ å°„å¼•æ“"""

    def __init__(self):
        self.similarity_calculator = SemanticSimilarityCalculator()
        self.hierarchy_builder = KnowledgeHierarchyBuilder()
        self.weight_calculator = ImportanceWeightCalculator()

    async def create_knowledge_mapping(self, syllabus_analysis, textbook_analysis):
        """åˆ›å»ºçŸ¥è¯†ç‚¹æ˜ å°„"""
        # 1. æå–çŸ¥è¯†ç‚¹
        syllabus_points = self._extract_knowledge_points(syllabus_analysis)
        textbook_points = self._extract_knowledge_points(textbook_analysis)

        # 2. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = await self._calculate_similarity_matrix(
            syllabus_points, textbook_points
        )

        # 3. æ‰§è¡Œæœ€ä¼˜åŒ¹é…
        optimal_matches = self._find_optimal_matches(similarity_matrix)

        # 4. æ„å»ºå±‚çº§å…³ç³»
        hierarchy = self.hierarchy_builder.build_hierarchy(
            syllabus_points, textbook_points, optimal_matches
        )

        # 5. è®¡ç®—é‡è¦æ€§æƒé‡
        importance_weights = self.weight_calculator.calculate_weights(
            hierarchy, syllabus_analysis.importance_indicators
        )

        # 6. åˆ†æä¾èµ–å…³ç³»
        dependencies = self._analyze_dependencies(hierarchy)

        return KnowledgeMapping(
            hierarchy=hierarchy,
            matches=optimal_matches,
            weights=importance_weights,
            dependencies=dependencies
        )

    async def _calculate_similarity_matrix(self, syllabus_points, textbook_points):
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ"""
        matrix = np.zeros((len(syllabus_points), len(textbook_points)))

        for i, s_point in enumerate(syllabus_points):
            for j, t_point in enumerate(textbook_points):
                # ä½¿ç”¨å¤šç§ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•
                semantic_sim = await self.similarity_calculator.semantic_similarity(
                    s_point.content, t_point.content
                )
                keyword_sim = self.similarity_calculator.keyword_similarity(
                    s_point.keywords, t_point.keywords
                )
                structure_sim = self.similarity_calculator.structure_similarity(
                    s_point.structure, t_point.structure
                )

                # åŠ æƒç»„åˆ
                combined_sim = (
                    semantic_sim * 0.5 +
                    keyword_sim * 0.3 +
                    structure_sim * 0.2
                )

                matrix[i][j] = combined_sim

        return matrix

    def _find_optimal_matches(self, similarity_matrix):
        """å¯»æ‰¾æœ€ä¼˜åŒ¹é…"""
        # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æˆ–è´ªå¿ƒç®—æ³•
        from scipy.optimize import linear_sum_assignment

        row_indices, col_indices = linear_sum_assignment(-similarity_matrix)

        matches = []
        for i, j in zip(row_indices, col_indices):
            if similarity_matrix[i][j] > 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                matches.append({
                    'syllabus_point_id': i,
                    'textbook_point_id': j,
                    'similarity_score': similarity_matrix[i][j],
                    'match_type': self._determine_match_type(similarity_matrix[i][j])
                })

        return matches
```

### 4. æ™ºèƒ½è¯¾æ—¶åˆ†é…å¼•æ“ (Smart Hour Allocation Engine)

#### åŠŸèƒ½èŒè´£

- åŸºäºçŸ¥è¯†ç‚¹é‡è¦æ€§çš„è¯¾æ—¶åˆ†é…
- æ”¯æŒå¤šç§åˆ†é…ç­–ç•¥
- è¯¾æ—¶æ¨¡å¼çº¦æŸå¤„ç†
- äººå·¥è°ƒæ•´å’ŒAIé‡æ–°ä¼˜åŒ–

#### åˆ†é…ç®—æ³•è®¾è®¡

```python
class SmartHourAllocationEngine:
    """æ™ºèƒ½è¯¾æ—¶åˆ†é…å¼•æ“"""

    def __init__(self):
        self.allocation_strategies = {
            'importance_weighted': ImportanceWeightedStrategy(),
            'difficulty_adjusted': DifficultyAdjustedStrategy(),
            'balanced': BalancedStrategy(),
            'custom': CustomStrategy()
        }

    async def calculate_optimal_allocation(self, knowledge_mapping, course_config):
        """è®¡ç®—æœ€ä¼˜è¯¾æ—¶åˆ†é…"""
        total_hours = course_config['total_hours']
        strategy_name = course_config.get('allocation_strategy', 'importance_weighted')
        hour_modes = course_config.get('hour_modes', [2, 4])

        # 1. åŸºç¡€åˆ†é…è®¡ç®—
        strategy = self.allocation_strategies[strategy_name]
        base_allocation = strategy.calculate_base_allocation(
            knowledge_mapping, total_hours
        )

        # 2. åº”ç”¨çº¦æŸæ¡ä»¶
        constrained_allocation = self._apply_constraints(
            base_allocation, hour_modes, course_config
        )

        # 3. ä¼˜åŒ–è°ƒæ•´
        optimized_allocation = self._optimize_allocation(
            constrained_allocation, knowledge_mapping
        )

        # 4. ç”Ÿæˆåˆ†é…ä¾æ®
        allocation_rationale = self._generate_rationale(
            optimized_allocation, knowledge_mapping
        )

        return CourseHourAllocation(
            allocation_data=optimized_allocation,
            rationale=allocation_rationale,
            strategy=strategy_name,
            quality_score=self._evaluate_allocation_quality(optimized_allocation)
        )

    def _apply_constraints(self, base_allocation, hour_modes, config):
        """åº”ç”¨çº¦æŸæ¡ä»¶"""
        constrained = base_allocation.copy()

        for knowledge_point in constrained['knowledge_points']:
            original_hours = knowledge_point['allocated_hours']

            # åº”ç”¨æœ€å°è¯¾æ—¶çº¦æŸ
            min_hours = knowledge_point.get('min_hours', 2)
            adjusted_hours = max(original_hours, min_hours)

            # åº”ç”¨è¯¾æ—¶æ¨¡å¼çº¦æŸ
            mode_adjusted_hours = self._adjust_to_hour_modes(
                adjusted_hours, hour_modes
            )

            knowledge_point['final_hours'] = mode_adjusted_hours
            knowledge_point['adjustment_reason'] = self._get_adjustment_reason(
                original_hours, mode_adjusted_hours
            )

        # é‡æ–°å¹³è¡¡æ€»è¯¾æ—¶
        self._rebalance_total_hours(constrained, config['total_hours'])

        return constrained

    def _adjust_to_hour_modes(self, target_hours, available_modes):
        """è°ƒæ•´åˆ°å¯ç”¨çš„è¯¾æ—¶æ¨¡å¼"""
        best_combination = None
        min_difference = float('inf')

        # åŠ¨æ€è§„åˆ’å¯»æ‰¾æœ€ä¼˜ç»„åˆ
        for combination in self._generate_hour_combinations(target_hours, available_modes):
            total_hours = sum(mode * count for mode, count in combination.items())
            difference = abs(total_hours - target_hours)

            if difference < min_difference:
                min_difference = difference
                best_combination = combination

        return best_combination

    def _generate_hour_combinations(self, target_hours, available_modes):
        """ç”Ÿæˆå¯èƒ½çš„è¯¾æ—¶ç»„åˆ"""
        combinations = []
        max_sessions = int(target_hours / min(available_modes)) + 1

        def backtrack(current_combination, remaining_hours, mode_index):
            if mode_index >= len(available_modes):
                if remaining_hours >= 0:
                    combinations.append(current_combination.copy())
                return

            mode = available_modes[mode_index]
            max_count = int(remaining_hours / mode) + 1

            for count in range(max_count):
                current_combination[mode] = count
                backtrack(
                    current_combination,
                    remaining_hours - mode * count,
                    mode_index + 1
                )

        backtrack({}, target_hours, 0)
        return combinations
```

## ğŸ”— ç»„ä»¶é›†æˆç­–ç•¥

### 1. äº‹ä»¶é©±åŠ¨é›†æˆ

```python
class SystemEventBus:
    """ç³»ç»Ÿäº‹ä»¶æ€»çº¿"""

    def __init__(self):
        self.subscribers = defaultdict(list)

    def subscribe(self, event_type, handler):
        """è®¢é˜…äº‹ä»¶"""
        self.subscribers[event_type].append(handler)

    async def publish(self, event):
        """å‘å¸ƒäº‹ä»¶"""
        handlers = self.subscribers[event.type]
        await asyncio.gather(*[handler(event) for handler in handlers])

# äº‹ä»¶å®šä¹‰
class WorkflowStageCompletedEvent:
    def __init__(self, workflow_id, stage, result):
        self.type = 'workflow_stage_completed'
        self.workflow_id = workflow_id
        self.stage = stage
        self.result = result
        self.timestamp = timezone.now()

# äº‹ä»¶å¤„ç†å™¨
class AnalysisEventHandler:
    async def handle_stage_completed(self, event):
        if event.stage == 'syllabus_analysis':
            # å¯åŠ¨æ•™æåˆ†æ
            await self.start_textbook_analysis(event.workflow_id)
```

### 2. æœåŠ¡é—´é€šä¿¡

```python
class ServiceCommunicationManager:
    """æœåŠ¡é—´é€šä¿¡ç®¡ç†å™¨"""

    def __init__(self):
        self.service_registry = ServiceRegistry()
        self.message_queue = MessageQueue()

    async def call_service(self, service_name, method, params):
        """è°ƒç”¨æœåŠ¡"""
        service_endpoint = self.service_registry.get_endpoint(service_name)

        try:
            response = await self._make_service_call(service_endpoint, method, params)
            return response
        except ServiceUnavailableError:
            # é™çº§å¤„ç†
            return await self._handle_service_degradation(service_name, method, params)

    async def _handle_service_degradation(self, service_name, method, params):
        """æœåŠ¡é™çº§å¤„ç†"""
        if service_name == 'ai_analysis_service':
            # ä½¿ç”¨ç¼“å­˜ç»“æœæˆ–ç®€åŒ–åˆ†æ
            return await self._get_cached_analysis_result(params)
        elif service_name == 'quality_control_service':
            # ä½¿ç”¨åŸºç¡€è´¨é‡æ£€æŸ¥
            return await self._basic_quality_check(params)
```

## ğŸ“Š æ€§èƒ½å’Œæ‰©å±•æ€§è®¾è®¡

### 1. ç¼“å­˜ç­–ç•¥

```python
class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self):
        self.redis_client = redis.Redis()
        self.local_cache = {}

    async def get_analysis_result(self, document_hash, analysis_config):
        """è·å–åˆ†æç»“æœç¼“å­˜"""
        cache_key = f"analysis:{document_hash}:{hash(str(analysis_config))}"

        # L1ç¼“å­˜ï¼šæœ¬åœ°å†…å­˜
        if cache_key in self.local_cache:
            return self.local_cache[cache_key]

        # L2ç¼“å­˜ï¼šRedis
        cached_result = await self.redis_client.get(cache_key)
        if cached_result:
            result = json.loads(cached_result)
            self.local_cache[cache_key] = result
            return result

        return None

    async def cache_analysis_result(self, document_hash, analysis_config, result):
        """ç¼“å­˜åˆ†æç»“æœ"""
        cache_key = f"analysis:{document_hash}:{hash(str(analysis_config))}"

        # ç¼“å­˜åˆ°Redisï¼ˆ24å°æ—¶è¿‡æœŸï¼‰
        await self.redis_client.setex(
            cache_key,
            86400,
            json.dumps(result, default=str)
        )

        # ç¼“å­˜åˆ°æœ¬åœ°å†…å­˜
        self.local_cache[cache_key] = result
```

### 2. å¼‚æ­¥ä»»åŠ¡å¤„ç†

```python
from celery import Celery

app = Celery('teaching_syllabus_system')

@app.task(bind=True, max_retries=3)
def process_iterative_analysis(self, document_id, analysis_config):
    """å¼‚æ­¥å¤„ç†å¤šè½®è¿­ä»£åˆ†æ"""
    try:
        engine = IterativeAnalysisEngine()
        result = asyncio.run(
            engine.start_analysis_session(document_id, analysis_config)
        )
        return result
    except Exception as exc:
        # é‡è¯•æœºåˆ¶
        if self.request.retries < self.max_retries:
            raise self.retry(countdown=60 * (2 ** self.request.retries))
        else:
            # è®°å½•å¤±è´¥å¹¶é€šçŸ¥
            logger.error(f"Analysis failed after {self.max_retries} retries: {exc}")
            raise
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [æŠ€æœ¯å®ç°è¯¦ç»†è®¾è®¡](./teaching-syllabus-technical-implementation.md)
- [æ•°æ®æ¨¡å‹è®¾è®¡æ–‡æ¡£](./teaching-syllabus-data-models.md)
- [APIè®¾è®¡è§„èŒƒ](./teaching-syllabus-api-design.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025-01-22  
**æœ€åæ›´æ–°**: 2025-01-22
