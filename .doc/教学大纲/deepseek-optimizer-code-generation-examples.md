# DeepSeekä¼˜åŒ–æ¨¡å—ä»£ç ç”Ÿæˆç¤ºä¾‹

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›ä½¿ç”¨AIç”ŸæˆDeepSeekä¼˜åŒ–æ¨¡å—æ ¸å¿ƒä»£ç çš„å…·ä½“ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•å……åˆ†åˆ©ç”¨AIçš„ä»£ç ç”Ÿæˆèƒ½åŠ›æ¥å¿«é€Ÿæ„å»ºé«˜è´¨é‡çš„æ¨¡å—ä»£ç ã€‚

## ğŸ¤– AIä»£ç ç”Ÿæˆç¤ºä¾‹

### 1. æ ¸å¿ƒæ•°æ®ç±»ç”Ÿæˆ

**AIæç¤ºè¯**ï¼š

```
è¯·ç”ŸæˆDeepSeekä¼˜åŒ–æ¨¡å—çš„æ ¸å¿ƒæ•°æ®ç±»ï¼Œè¦æ±‚ï¼š
1. ä½¿ç”¨@dataclassè£…é¥°å™¨å’Œç±»å‹æç¤º
2. åŒ…å«éªŒè¯æ–¹æ³•å’Œåºåˆ—åŒ–æ”¯æŒ
3. æ”¯æŒé»˜è®¤å€¼å’Œå¯é€‰å­—æ®µ
4. æ·»åŠ è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²
5. åŒ…å«__post_init__æ–¹æ³•è¿›è¡ŒéªŒè¯

éœ€è¦ç”Ÿæˆçš„ç±»ï¼š
- OptimizationRequestï¼ˆä¼˜åŒ–è¯·æ±‚ï¼‰
- OptimizationResultï¼ˆä¼˜åŒ–ç»“æœï¼‰
- ModelConfigï¼ˆæ¨¡å‹é…ç½®ï¼‰
- CacheConfigï¼ˆç¼“å­˜é…ç½®ï¼‰
```

**AIç”Ÿæˆçš„ä»£ç **ï¼š

```python
# deepseek_optimizer/core/models.py

from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List, Union
from enum import Enum
import json
import hashlib
from datetime import datetime

class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    ANALYSIS = "analysis"
    GENERATION = "generation"
    REASONING = "reasoning"
    EXTRACTION = "extraction"
    MAPPING = "mapping"
    ALLOCATION = "allocation"

class ComplexityLevel(Enum):
    """å¤æ‚åº¦çº§åˆ«æšä¸¾"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"

class ModelType(Enum):
    """æ¨¡å‹ç±»å‹æšä¸¾"""
    DEEPSEEK_CHAT = "deepseek-chat"
    DEEPSEEK_REASONER = "deepseek-reasoner"

@dataclass
class OptimizationRequest:
    """
    ä¼˜åŒ–è¯·æ±‚æ•°æ®ç±»

    åŒ…å«æ‰§è¡ŒDeepSeekä¼˜åŒ–æ‰€éœ€çš„æ‰€æœ‰å‚æ•°å’Œé…ç½®ä¿¡æ¯
    """
    content: str
    task_type: TaskType
    complexity: ComplexityLevel
    context: Optional[Dict[str, Any]] = None
    preferences: Optional[Dict[str, Any]] = None
    user_id: Optional[str] = None
    tenant_id: Optional[str] = None
    request_id: Optional[str] = None
    timestamp: Optional[datetime] = None

    def __post_init__(self):
        """åˆå§‹åŒ–åéªŒè¯å’Œè®¾ç½®é»˜è®¤å€¼"""
        if self.timestamp is None:
            self.timestamp = datetime.now()

        if self.request_id is None:
            self.request_id = self._generate_request_id()

        if self.context is None:
            self.context = {}

        if self.preferences is None:
            self.preferences = {}

        self._validate()

    def _generate_request_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„è¯·æ±‚ID"""
        content_hash = hashlib.md5(self.content.encode()).hexdigest()[:8]
        timestamp_str = self.timestamp.strftime("%Y%m%d%H%M%S") if self.timestamp else ""
        return f"req_{timestamp_str}_{content_hash}"

    def _validate(self):
        """éªŒè¯è¯·æ±‚æ•°æ®çš„æœ‰æ•ˆæ€§"""
        if not self.content or not self.content.strip():
            raise ValueError("Content cannot be empty")

        if len(self.content) > 200000:  # 200Kå­—ç¬¦é™åˆ¶
            raise ValueError("Content exceeds maximum length of 200K characters")

        if self.user_id and len(self.user_id) > 100:
            raise ValueError("User ID exceeds maximum length")

        if self.tenant_id and len(self.tenant_id) > 100:
            raise ValueError("Tenant ID exceeds maximum length")

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "content": self.content,
            "task_type": self.task_type.value,
            "complexity": self.complexity.value,
            "context": self.context,
            "preferences": self.preferences,
            "user_id": self.user_id,
            "tenant_id": self.tenant_id,
            "request_id": self.request_id,
            "timestamp": self.timestamp.isoformat() if self.timestamp else None
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'OptimizationRequest':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(
            content=data["content"],
            task_type=TaskType(data["task_type"]),
            complexity=ComplexityLevel(data["complexity"]),
            context=data.get("context"),
            preferences=data.get("preferences"),
            user_id=data.get("user_id"),
            tenant_id=data.get("tenant_id"),
            request_id=data.get("request_id"),
            timestamp=datetime.fromisoformat(data["timestamp"]) if data.get("timestamp") else None
        )

@dataclass
class OptimizationResult:
    """
    ä¼˜åŒ–ç»“æœæ•°æ®ç±»

    åŒ…å«DeepSeekä¼˜åŒ–å¤„ç†çš„å®Œæ•´ç»“æœä¿¡æ¯
    """
    success: bool
    content: str
    request_id: str
    reasoning_process: Optional[str] = None
    model_used: str = ""
    processing_time: float = 0.0
    tokens_used: int = 0
    cost: float = 0.0
    quality_score: float = 0.0
    from_cache: bool = False
    cache_key: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    timestamp: Optional[datetime] = None

    def __post_init__(self):
        """åˆå§‹åŒ–åè®¾ç½®é»˜è®¤å€¼"""
        if self.timestamp is None:
            self.timestamp = datetime.now()

        if self.metadata is None:
            self.metadata = {}

        self._validate()

    def _validate(self):
        """éªŒè¯ç»“æœæ•°æ®çš„æœ‰æ•ˆæ€§"""
        if not self.request_id:
            raise ValueError("Request ID is required")

        if self.success and not self.content:
            raise ValueError("Content is required for successful results")

        if not self.success and not self.error_message:
            raise ValueError("Error message is required for failed results")

        if self.processing_time < 0:
            raise ValueError("Processing time cannot be negative")

        if self.tokens_used < 0:
            raise ValueError("Tokens used cannot be negative")

        if self.cost < 0:
            raise ValueError("Cost cannot be negative")

        if not 0 <= self.quality_score <= 1:
            raise ValueError("Quality score must be between 0 and 1")

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "success": self.success,
            "content": self.content,
            "request_id": self.request_id,
            "reasoning_process": self.reasoning_process,
            "model_used": self.model_used,
            "processing_time": self.processing_time,
            "tokens_used": self.tokens_used,
            "cost": self.cost,
            "quality_score": self.quality_score,
            "from_cache": self.from_cache,
            "cache_key": self.cache_key,
            "metadata": self.metadata,
            "error_message": self.error_message,
            "timestamp": self.timestamp.isoformat() if self.timestamp else None
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'OptimizationResult':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(
            success=data["success"],
            content=data["content"],
            request_id=data["request_id"],
            reasoning_process=data.get("reasoning_process"),
            model_used=data.get("model_used", ""),
            processing_time=data.get("processing_time", 0.0),
            tokens_used=data.get("tokens_used", 0),
            cost=data.get("cost", 0.0),
            quality_score=data.get("quality_score", 0.0),
            from_cache=data.get("from_cache", False),
            cache_key=data.get("cache_key"),
            metadata=data.get("metadata"),
            error_message=data.get("error_message"),
            timestamp=datetime.fromisoformat(data["timestamp"]) if data.get("timestamp") else None
        )

@dataclass
class ModelConfig:
    """
    æ¨¡å‹é…ç½®æ•°æ®ç±»

    å®šä¹‰DeepSeekæ¨¡å‹çš„å‚æ•°é…ç½®
    """
    name: str
    temperature: float = 0.7
    max_tokens: int = 4000
    top_p: float = 0.9
    use_reasoning: bool = False
    timeout: int = 30
    retry_count: int = 3

    def __post_init__(self):
        """åˆå§‹åŒ–åéªŒè¯é…ç½®"""
        self._validate()

    def _validate(self):
        """éªŒè¯é…ç½®å‚æ•°çš„æœ‰æ•ˆæ€§"""
        if not self.name:
            raise ValueError("Model name is required")

        if not 0 <= self.temperature <= 2:
            raise ValueError("Temperature must be between 0 and 2")

        if not 1 <= self.max_tokens <= 8000:
            raise ValueError("Max tokens must be between 1 and 8000")

        if not 0 <= self.top_p <= 1:
            raise ValueError("Top_p must be between 0 and 1")

        if self.timeout <= 0:
            raise ValueError("Timeout must be positive")

        if self.retry_count < 0:
            raise ValueError("Retry count cannot be negative")

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "name": self.name,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p,
            "use_reasoning": self.use_reasoning,
            "timeout": self.timeout,
            "retry_count": self.retry_count
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ModelConfig':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(**data)

@dataclass
class CacheConfig:
    """
    ç¼“å­˜é…ç½®æ•°æ®ç±»

    å®šä¹‰ç¼“å­˜ç³»ç»Ÿçš„é…ç½®å‚æ•°
    """
    backend: str = "memory"
    default_ttl: int = 3600
    max_size: int = 1000
    compression: bool = True
    redis_url: Optional[str] = None
    redis_db: int = 0
    key_prefix: str = "deepseek_optimizer"

    def __post_init__(self):
        """åˆå§‹åŒ–åéªŒè¯é…ç½®"""
        self._validate()

    def _validate(self):
        """éªŒè¯ç¼“å­˜é…ç½®çš„æœ‰æ•ˆæ€§"""
        valid_backends = ["memory", "redis", "file"]
        if self.backend not in valid_backends:
            raise ValueError(f"Backend must be one of {valid_backends}")

        if self.default_ttl <= 0:
            raise ValueError("Default TTL must be positive")

        if self.max_size <= 0:
            raise ValueError("Max size must be positive")

        if self.backend == "redis" and not self.redis_url:
            raise ValueError("Redis URL is required for redis backend")

        if self.redis_db < 0:
            raise ValueError("Redis DB must be non-negative")

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "backend": self.backend,
            "default_ttl": self.default_ttl,
            "max_size": self.max_size,
            "compression": self.compression,
            "redis_url": self.redis_url,
            "redis_db": self.redis_db,
            "key_prefix": self.key_prefix
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'CacheConfig':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(**data)
```

### 2. æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨ç”Ÿæˆ

**AIæç¤ºè¯**ï¼š

```
è¯·ç”Ÿæˆæ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨ç±»ï¼Œè¦æ±‚ï¼š
1. å®ç°åŸºäºä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦çš„æ™ºèƒ½é€‰æ‹©é€»è¾‘
2. æ”¯æŒåŠ¨æ€å‚æ•°è°ƒæ•´
3. åŒ…å«æ€§èƒ½ç»Ÿè®¡å’Œå­¦ä¹ èƒ½åŠ›
4. æä¾›é…ç½®éªŒè¯å’Œé”™è¯¯å¤„ç†
5. æ”¯æŒè‡ªå®šä¹‰é€‰æ‹©ç­–ç•¥

åŠŸèƒ½éœ€æ±‚ï¼š
- æ ¹æ®TaskTypeå’ŒComplexityLevelé€‰æ‹©æœ€ä¼˜æ¨¡å‹
- åŸºäºå†å²æ€§èƒ½æ•°æ®ä¼˜åŒ–é€‰æ‹©
- æ”¯æŒA/Bæµ‹è¯•å’Œç­–ç•¥åˆ‡æ¢
- æä¾›é€‰æ‹©ç†ç”±å’Œç½®ä¿¡åº¦
```

**AIç”Ÿæˆçš„ä»£ç **ï¼š

```python
# deepseek_optimizer/core/model_selector.py

from typing import Dict, Any, Optional, Tuple, List
import logging
from datetime import datetime, timedelta
from collections import defaultdict
import statistics

from .models import TaskType, ComplexityLevel, ModelConfig, ModelType
from ..utils.exceptions import ModelSelectionError, ConfigurationError

logger = logging.getLogger(__name__)

class IntelligentModelSelector:
    """
    æ™ºèƒ½æ¨¡å‹é€‰æ‹©å™¨

    åŸºäºä»»åŠ¡ç‰¹å¾ã€å†å²æ€§èƒ½å’Œé…ç½®ç­–ç•¥æ™ºèƒ½é€‰æ‹©æœ€ä¼˜çš„DeepSeekæ¨¡å‹
    """

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ¨¡å‹é€‰æ‹©å™¨

        Args:
            config: é€‰æ‹©å™¨é…ç½®å­—å…¸
        """
        self.config = config
        self.performance_history: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        self.selection_matrix = self._build_selection_matrix()
        self.learning_enabled = config.get("enable_learning", True)
        self.min_samples_for_learning = config.get("min_samples_for_learning", 10)

        logger.info("Intelligent model selector initialized")

    def _build_selection_matrix(self) -> Dict[Tuple[TaskType, ComplexityLevel], Dict[str, Any]]:
        """æ„å»ºåŸºç¡€é€‰æ‹©çŸ©é˜µ"""
        return {
            # åˆ†æä»»åŠ¡
            (TaskType.ANALYSIS, ComplexityLevel.LOW): {
                "model": ModelType.DEEPSEEK_CHAT,
                "temperature": 0.3,
                "max_tokens": 3000,
                "top_p": 0.8,
                "use_reasoning": False,
                "confidence": 0.9
            },
            (TaskType.ANALYSIS, ComplexityLevel.MEDIUM): {
                "model": ModelType.DEEPSEEK_CHAT,
                "temperature": 0.5,
                "max_tokens": 4000,
                "top_p": 0.85,
                "use_reasoning": False,
                "confidence": 0.8
            },
            (TaskType.ANALYSIS, ComplexityLevel.HIGH): {
                "model": ModelType.DEEPSEEK_REASONER,
                "temperature": 0.6,
                "max_tokens": 4000,
                "top_p": 0.9,
                "use_reasoning": True,
                "confidence": 0.95
            },

            # æ¨ç†ä»»åŠ¡
            (TaskType.REASONING, ComplexityLevel.LOW): {
                "model": ModelType.DEEPSEEK_REASONER,
                "temperature": 0.5,
                "max_tokens": 3000,
                "top_p": 0.85,
                "use_reasoning": True,
                "confidence": 0.9
            },
            (TaskType.REASONING, ComplexityLevel.MEDIUM): {
                "model": ModelType.DEEPSEEK_REASONER,
                "temperature": 0.6,
                "max_tokens": 4000,
                "top_p": 0.9,
                "use_reasoning": True,
                "confidence": 0.95
            },
            (TaskType.REASONING, ComplexityLevel.HIGH): {
                "model": ModelType.DEEPSEEK_REASONER,
                "temperature": 0.6,
                "max_tokens": 5000,
                "top_p": 0.9,
                "use_reasoning": True,
                "confidence": 0.98
            },

            # ç”Ÿæˆä»»åŠ¡
            (TaskType.GENERATION, ComplexityLevel.LOW): {
                "model": ModelType.DEEPSEEK_CHAT,
                "temperature": 0.7,
                "max_tokens": 4000,
                "top_p": 0.9,
                "use_reasoning": False,
                "confidence": 0.85
            },
            (TaskType.GENERATION, ComplexityLevel.MEDIUM): {
                "model": ModelType.DEEPSEEK_CHAT,
                "temperature": 0.7,
                "max_tokens": 6000,
                "top_p": 0.95,
                "use_reasoning": False,
                "confidence": 0.8
            },
            (TaskType.GENERATION, ComplexityLevel.HIGH): {
                "model": ModelType.DEEPSEEK_CHAT,
                "temperature": 0.8,
                "max_tokens": 8000,
                "top_p": 0.95,
                "use_reasoning": False,
                "confidence": 0.75
            },

            # æå–ä»»åŠ¡
            (TaskType.EXTRACTION, ComplexityLevel.LOW): {
                "model": ModelType.DEEPSEEK_CHAT,
                "temperature": 0.2,
                "max_tokens": 2000,
                "top_p": 0.8,
                "use_reasoning": False,
                "confidence": 0.9
            },
            (TaskType.EXTRACTION, ComplexityLevel.MEDIUM): {
                "model": ModelType.DEEPSEEK_CHAT,
                "temperature": 0.3,
                "max_tokens": 3000,
                "top_p": 0.8,
                "use_reasoning": False,
                "confidence": 0.85
            },
            (TaskType.EXTRACTION, ComplexityLevel.HIGH): {
                "model": ModelType.DEEPSEEK_REASONER,
                "temperature": 0.4,
                "max_tokens": 4000,
                "top_p": 0.85,
                "use_reasoning": True,
                "confidence": 0.9
            }
        }

    def select_model(
        self,
        task_type: TaskType,
        complexity: ComplexityLevel,
        content_length: int,
        context: Optional[Dict[str, Any]] = None
    ) -> Tuple[ModelConfig, Dict[str, Any]]:
        """
        é€‰æ‹©æœ€ä¼˜æ¨¡å‹é…ç½®

        Args:
            task_type: ä»»åŠ¡ç±»å‹
            complexity: å¤æ‚åº¦çº§åˆ«
            content_length: å†…å®¹é•¿åº¦
            context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            Tuple[ModelConfig, Dict[str, Any]]: æ¨¡å‹é…ç½®å’Œé€‰æ‹©å…ƒæ•°æ®
        """
        try:
            # è·å–åŸºç¡€é…ç½®
            base_config = self._get_base_config(task_type, complexity)

            # åŸºäºå†…å®¹é•¿åº¦è°ƒæ•´
            adjusted_config = self._adjust_for_content_length(base_config, content_length)

            # åŸºäºå†å²æ€§èƒ½ä¼˜åŒ–
            if self.learning_enabled:
                optimized_config = self._optimize_with_history(
                    adjusted_config, task_type, complexity
                )
            else:
                optimized_config = adjusted_config

            # åˆ›å»ºæ¨¡å‹é…ç½®
            model_config = ModelConfig(
                name=optimized_config["model"].value,
                temperature=optimized_config["temperature"],
                max_tokens=optimized_config["max_tokens"],
                top_p=optimized_config["top_p"],
                use_reasoning=optimized_config["use_reasoning"]
            )

            # é€‰æ‹©å…ƒæ•°æ®
            metadata = {
                "selection_reason": self._generate_selection_reason(
                    task_type, complexity, content_length
                ),
                "confidence": optimized_config["confidence"],
                "base_strategy": "matrix_lookup",
                "adjustments_applied": self._get_applied_adjustments(
                    base_config, optimized_config
                ),
                "timestamp": datetime.now().isoformat()
            }

            logger.info(
                f"Selected model {model_config.name} for {task_type.value} "
                f"task with {complexity.value} complexity"
            )

            return model_config, metadata

        except Exception as e:
            logger.error(f"Model selection failed: {e}")
            raise ModelSelectionError(f"Failed to select model: {e}")

    def _get_base_config(
        self,
        task_type: TaskType,
        complexity: ComplexityLevel
    ) -> Dict[str, Any]:
        """è·å–åŸºç¡€é…ç½®"""
        key = (task_type, complexity)
        if key not in self.selection_matrix:
            # å›é€€åˆ°é»˜è®¤é…ç½®
            logger.warning(f"No specific config for {key}, using default")
            return {
                "model": ModelType.DEEPSEEK_CHAT,
                "temperature": 0.7,
                "max_tokens": 4000,
                "top_p": 0.9,
                "use_reasoning": False,
                "confidence": 0.5
            }
        return self.selection_matrix[key].copy()

    def _adjust_for_content_length(
        self,
        config: Dict[str, Any],
        content_length: int
    ) -> Dict[str, Any]:
        """åŸºäºå†…å®¹é•¿åº¦è°ƒæ•´é…ç½®"""
        adjusted = config.copy()

        if content_length > 50000:  # é•¿æ–‡æ¡£
            # é™ä½æ¸©åº¦æé«˜ç¨³å®šæ€§
            adjusted["temperature"] = max(0.2, adjusted["temperature"] - 0.1)
            # å¢åŠ è¾“å‡ºé•¿åº¦
            adjusted["max_tokens"] = min(8000, adjusted["max_tokens"] + 1000)
            # è°ƒæ•´ç½®ä¿¡åº¦
            adjusted["confidence"] *= 0.9

        elif content_length < 1000:  # çŸ­æ–‡æ¡£
            # å¯ä»¥ç¨å¾®æé«˜æ¸©åº¦
            adjusted["temperature"] = min(1.0, adjusted["temperature"] + 0.1)
            # å‡å°‘è¾“å‡ºé•¿åº¦
            adjusted["max_tokens"] = max(1000, adjusted["max_tokens"] - 500)

        return adjusted

    def _optimize_with_history(
        self,
        config: Dict[str, Any],
        task_type: TaskType,
        complexity: ComplexityLevel
    ) -> Dict[str, Any]:
        """åŸºäºå†å²æ€§èƒ½ä¼˜åŒ–é…ç½®"""
        history_key = f"{task_type.value}_{complexity.value}"
        history = self.performance_history.get(history_key, [])

        if len(history) < self.min_samples_for_learning:
            return config

        # åˆ†æå†å²æ€§èƒ½
        recent_history = history[-20:]  # æœ€è¿‘20æ¬¡

        # è®¡ç®—å¹³å‡è´¨é‡åˆ†æ•°
        quality_scores = [h["quality_score"] for h in recent_history if h.get("quality_score")]
        if quality_scores:
            avg_quality = statistics.mean(quality_scores)

            # å¦‚æœè´¨é‡åˆ†æ•°ä½äºé˜ˆå€¼ï¼Œè°ƒæ•´å‚æ•°
            if avg_quality < 0.8:
                optimized = config.copy()

                # å¦‚æœå½“å‰ä½¿ç”¨chatæ¨¡å‹ä¸”è´¨é‡ä¸ä½³ï¼Œè€ƒè™‘åˆ‡æ¢åˆ°reasoner
                if config["model"] == ModelType.DEEPSEEK_CHAT:
                    optimized["model"] = ModelType.DEEPSEEK_REASONER
                    optimized["use_reasoning"] = True
                    optimized["temperature"] = max(0.3, optimized["temperature"] - 0.1)

                # é™ä½æ¸©åº¦æé«˜ç¨³å®šæ€§
                optimized["temperature"] = max(0.2, optimized["temperature"] - 0.05)

                logger.info(f"Optimized config based on history for {history_key}")
                return optimized

        return config

    def record_performance(
        self,
        task_type: TaskType,
        complexity: ComplexityLevel,
        model_used: str,
        quality_score: float,
        processing_time: float,
        success: bool
    ):
        """è®°å½•æ€§èƒ½æ•°æ®ç”¨äºå­¦ä¹ """
        if not self.learning_enabled:
            return

        history_key = f"{task_type.value}_{complexity.value}"

        performance_record = {
            "timestamp": datetime.now().isoformat(),
            "model_used": model_used,
            "quality_score": quality_score,
            "processing_time": processing_time,
            "success": success
        }

        self.performance_history[history_key].append(performance_record)

        # ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
        max_history = self.config.get("max_history_records", 100)
        if len(self.performance_history[history_key]) > max_history:
            self.performance_history[history_key] = \
                self.performance_history[history_key][-max_history:]

        logger.debug(f"Recorded performance for {history_key}")

    def _generate_selection_reason(
        self,
        task_type: TaskType,
        complexity: ComplexityLevel,
        content_length: int
    ) -> str:
        """ç”Ÿæˆé€‰æ‹©ç†ç”±"""
        reasons = []

        if task_type == TaskType.REASONING:
            reasons.append("æ¨ç†ä»»åŠ¡éœ€è¦ä½¿ç”¨æ¨ç†æ¨¡å‹")
        elif complexity == ComplexityLevel.HIGH:
            reasons.append("é«˜å¤æ‚åº¦ä»»åŠ¡éœ€è¦æ›´å¼ºçš„æ¨¡å‹èƒ½åŠ›")

        if content_length > 50000:
            reasons.append("é•¿æ–‡æ¡£éœ€è¦è°ƒæ•´å‚æ•°ä»¥æé«˜ç¨³å®šæ€§")
        elif content_length < 1000:
            reasons.append("çŸ­æ–‡æ¡£å¯ä»¥ä½¿ç”¨æ›´çµæ´»çš„å‚æ•°")

        if not reasons:
            reasons.append("åŸºäºä»»åŠ¡ç±»å‹å’Œå¤æ‚åº¦çš„æ ‡å‡†é€‰æ‹©")

        return "; ".join(reasons)

    def _get_applied_adjustments(
        self,
        base_config: Dict[str, Any],
        final_config: Dict[str, Any]
    ) -> List[str]:
        """è·å–åº”ç”¨çš„è°ƒæ•´åˆ—è¡¨"""
        adjustments = []

        if base_config["temperature"] != final_config["temperature"]:
            adjustments.append(
                f"temperature: {base_config['temperature']} -> {final_config['temperature']}"
            )

        if base_config["max_tokens"] != final_config["max_tokens"]:
            adjustments.append(
                f"max_tokens: {base_config['max_tokens']} -> {final_config['max_tokens']}"
            )

        if base_config["model"] != final_config["model"]:
            adjustments.append(
                f"model: {base_config['model'].value} -> {final_config['model'].value}"
            )

        return adjustments

    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}

        for key, history in self.performance_history.items():
            if not history:
                continue

            recent_history = history[-20:]

            quality_scores = [h["quality_score"] for h in recent_history if h.get("quality_score")]
            processing_times = [h["processing_time"] for h in recent_history]
            success_rate = sum(1 for h in recent_history if h["success"]) / len(recent_history)

            stats[key] = {
                "sample_count": len(recent_history),
                "avg_quality_score": statistics.mean(quality_scores) if quality_scores else 0,
                "avg_processing_time": statistics.mean(processing_times) if processing_times else 0,
                "success_rate": success_rate,
                "last_updated": recent_history[-1]["timestamp"] if recent_history else None
            }

        return stats
```

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [DeepSeekä¼˜åŒ–æ¨¡å—å¼€å‘è®¡åˆ’](./deepseek-optimizer-module-development-plan.md)
- [DeepSeekä¼˜åŒ–ç­–ç•¥](./deepseek-optimization-strategy.md)
- [å®ç°ä»£ç ç¤ºä¾‹](./deepseek-implementation-examples.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025-01-22  
**æœ€åæ›´æ–°**: 2025-01-22
