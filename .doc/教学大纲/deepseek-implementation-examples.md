# DeepSeekä¼˜åŒ–å®ç°ä»£ç ç¤ºä¾‹

## ğŸ“‹ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›åŸºäºDeepSeekæ¨¡å‹ç‰¹æ€§ä¼˜åŒ–çš„å…·ä½“ä»£ç å®ç°ç¤ºä¾‹ï¼Œå¯ç›´æ¥åº”ç”¨äºæ•™å­¦å¤§çº²ç”Ÿæˆç³»ç»Ÿçš„æ”¹è¿›ã€‚

## ğŸ”§ ä¼˜åŒ–åçš„DeepSeekæœåŠ¡å®ç°

### 1. æ ¸å¿ƒæœåŠ¡ç±»ä¼˜åŒ–

````python
# backend/ai_services/optimized_deepseek_service.py

import asyncio
import hashlib
import json
import time
from typing import Any, Dict, List, Optional
from dataclasses import dataclass
from decimal import Decimal

from django.conf import settings
from django.core.cache import cache
import openai

from .services import DeepSeekAPIService


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    model: str
    temperature: float
    max_tokens: int
    top_p: float
    use_reasoning: bool = False


class OptimizedDeepSeekService(DeepSeekAPIService):
    """ä¼˜åŒ–çš„DeepSeekæœåŠ¡"""

    # ä¼˜åŒ–çš„æ¨¡å‹é…ç½®
    OPTIMIZED_MODEL_CONFIGS = {
        "reasoning_analysis": ModelConfig(
            model="deepseek-reasoner",
            temperature=0.6,
            max_tokens=4000,
            top_p=0.9,
            use_reasoning=True
        ),
        "structured_generation": ModelConfig(
            model="deepseek-chat",
            temperature=0.3,
            max_tokens=6000,
            top_p=0.8,
            use_reasoning=False
        ),
        "creative_generation": ModelConfig(
            model="deepseek-chat",
            temperature=0.7,
            max_tokens=8000,
            top_p=0.95,
            use_reasoning=False
        ),
        "quality_assessment": ModelConfig(
            model="deepseek-reasoner",
            temperature=0.5,
            max_tokens=3000,
            top_p=0.85,
            use_reasoning=True
        )
    }

    def __init__(self):
        super().__init__()
        self.cache_manager = IntelligentCacheManager()
        self.prompt_optimizer = DeepSeekPromptOptimizer()

    async def analyze_with_reasoning(
        self,
        content: str,
        analysis_type: str,
        context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨æ¨ç†æ¨¡å‹è¿›è¡Œåˆ†æ"""

        # 1. é€‰æ‹©æœ€ä¼˜é…ç½®
        config = self._select_optimal_config(analysis_type, len(content))

        # 2. æ„å»ºæ¨ç†æç¤ºè¯
        prompt = self.prompt_optimizer.build_reasoning_prompt(
            problem_statement=f"è¯·å¯¹ä»¥ä¸‹{analysis_type}å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æ",
            analysis_context={"content": content, "context": context or {}},
            expected_output=self._get_expected_output_format(analysis_type)
        )

        # 3. ç¼“å­˜æ£€æŸ¥
        cache_key = self._generate_cache_key(prompt, config)
        cached_result = await self.cache_manager.get_cached_result(cache_key)

        if cached_result:
            return {**cached_result, "from_cache": True}

        # 4. æ‰§è¡Œåˆ†æ
        result = await self._call_optimized_api(prompt, config)

        # 5. ç¼“å­˜ç»“æœ
        await self.cache_manager.cache_result(cache_key, result, ttl=3600)

        return result

    async def generate_with_context(
        self,
        task_description: str,
        context_data: Dict[str, Any],
        output_format: str,
        generation_type: str = "structured"
    ) -> Dict[str, Any]:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå†…å®¹"""

        # 1. é€‰æ‹©ç”Ÿæˆé…ç½®
        config_key = f"{generation_type}_generation"
        config = self.OPTIMIZED_MODEL_CONFIGS.get(
            config_key,
            self.OPTIMIZED_MODEL_CONFIGS["structured_generation"]
        )

        # 2. æ„å»ºç»“æ„åŒ–æç¤ºè¯
        prompt = self.prompt_optimizer.build_structured_prompt(
            task_description=task_description,
            context_data=context_data,
            output_format=output_format,
            reasoning_required=(generation_type == "reasoning")
        )

        # 3. æ‰§è¡Œç”Ÿæˆ
        result = await self._call_optimized_api(prompt, config)

        # 4. è´¨é‡éªŒè¯
        validated_result = self._validate_generation_quality(result, output_format)

        return validated_result

    def _select_optimal_config(self, task_type: str, content_length: int) -> ModelConfig:
        """é€‰æ‹©æœ€ä¼˜æ¨¡å‹é…ç½®"""

        # åŸºäºä»»åŠ¡ç±»å‹é€‰æ‹©
        if task_type in ["structure_analysis", "knowledge_extraction", "difficulty_assessment"]:
            base_config = self.OPTIMIZED_MODEL_CONFIGS["reasoning_analysis"]
        elif task_type in ["json_generation", "data_extraction"]:
            base_config = self.OPTIMIZED_MODEL_CONFIGS["structured_generation"]
        elif task_type in ["content_generation", "lesson_plan"]:
            base_config = self.OPTIMIZED_MODEL_CONFIGS["creative_generation"]
        else:
            base_config = self.OPTIMIZED_MODEL_CONFIGS["structured_generation"]

        # åŸºäºå†…å®¹é•¿åº¦è°ƒæ•´
        if content_length > 50000:  # é•¿æ–‡æ¡£
            adjusted_config = ModelConfig(
                model=base_config.model,
                temperature=max(0.3, base_config.temperature - 0.1),  # é™ä½æ¸©åº¦æé«˜ç¨³å®šæ€§
                max_tokens=min(8000, base_config.max_tokens + 1000),  # å¢åŠ è¾“å‡ºé•¿åº¦
                top_p=base_config.top_p,
                use_reasoning=base_config.use_reasoning
            )
            return adjusted_config

        return base_config

    async def _call_optimized_api(
        self,
        prompt: str,
        config: ModelConfig,
        retry_count: int = 0
    ) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„APIè°ƒç”¨"""

        messages = [
            {
                "role": "system",
                "content": self._get_optimized_system_prompt(config.use_reasoning)
            },
            {
                "role": "user",
                "content": prompt
            }
        ]

        try:
            start_time = time.time()

            response = self._call_api(
                messages=messages,
                model=config.model,
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )

            processing_time = time.time() - start_time

            # æå–æ¨ç†è¿‡ç¨‹ï¼ˆå¦‚æœä½¿ç”¨æ¨ç†æ¨¡å‹ï¼‰
            if config.use_reasoning and config.model == "deepseek-reasoner":
                reasoning_content = self._extract_reasoning_content(response["content"])
                final_answer = self._extract_final_answer(response["content"])

                return {
                    "reasoning_process": reasoning_content,
                    "final_answer": final_answer,
                    "raw_content": response["content"],
                    "model_used": config.model,
                    "processing_time": processing_time,
                    "tokens_used": response["usage"]["total_tokens"],
                    "cost": response.get("cost", 0)
                }
            else:
                return {
                    "content": response["content"],
                    "model_used": config.model,
                    "processing_time": processing_time,
                    "tokens_used": response["usage"]["total_tokens"],
                    "cost": response.get("cost", 0)
                }

        except Exception as e:
            if retry_count < 2:  # æœ€å¤šé‡è¯•2æ¬¡
                # è°ƒæ•´å‚æ•°é‡è¯•
                adjusted_config = self._adjust_config_for_retry(config, retry_count)
                return await self._call_optimized_api(prompt, adjusted_config, retry_count + 1)
            else:
                raise e

    def _get_optimized_system_prompt(self, use_reasoning: bool) -> str:
        """è·å–ä¼˜åŒ–çš„ç³»ç»Ÿæç¤ºè¯"""

        base_prompt = """ä½ æ˜¯ä¸“ä¸šçš„æ•™è‚²å†…å®¹åˆ†æå’Œç”Ÿæˆä¸“å®¶ï¼Œå…·å¤‡æ·±åšçš„æ•™è‚²å­¦ç†è®ºçŸ¥è¯†å’Œä¸°å¯Œçš„å®è·µç»éªŒã€‚"""

        if use_reasoning:
            return base_prompt + """
è¯·ä½¿ç”¨ä»¥ä¸‹ç»“æ„è¿›è¡Œåˆ†æï¼š
1. åœ¨<think>æ ‡ç­¾ä¸­è¿›è¡Œè¯¦ç»†çš„æ€è€ƒå’Œæ¨ç†
2. åœ¨<answer>æ ‡ç­¾ä¸­æä¾›æœ€ç»ˆçš„åˆ†æç»“æœ
3. ç¡®ä¿æ¨ç†è¿‡ç¨‹é€»è¾‘æ¸…æ™°ã€ç»“è®ºå‡†ç¡®å¯é 
"""
        else:
            return base_prompt + """
è¯·æä¾›å‡†ç¡®ã€ç»“æ„åŒ–çš„åˆ†æç»“æœï¼Œç¡®ä¿è¾“å‡ºæ ¼å¼ç¬¦åˆè¦æ±‚ã€‚
"""

    def _extract_reasoning_content(self, content: str) -> str:
        """æå–æ¨ç†è¿‡ç¨‹"""
        import re

        think_pattern = r'<think>(.*?)</think>'
        match = re.search(think_pattern, content, re.DOTALL)

        if match:
            return match.group(1).strip()
        return ""

    def _extract_final_answer(self, content: str) -> str:
        """æå–æœ€ç»ˆç­”æ¡ˆ"""
        import re

        answer_pattern = r'<answer>(.*?)</answer>'
        match = re.search(answer_pattern, content, re.DOTALL)

        if match:
            return match.group(1).strip()

        # å¦‚æœæ²¡æœ‰answeræ ‡ç­¾ï¼Œè¿”å›æ•´ä¸ªå†…å®¹
        return content

    def _adjust_config_for_retry(self, config: ModelConfig, retry_count: int) -> ModelConfig:
        """è°ƒæ•´é…ç½®ç”¨äºé‡è¯•"""

        return ModelConfig(
            model=config.model,
            temperature=max(0.1, config.temperature - 0.1 * retry_count),
            max_tokens=min(8000, config.max_tokens + 500 * retry_count),
            top_p=max(0.7, config.top_p - 0.05 * retry_count),
            use_reasoning=config.use_reasoning
        )

    def _generate_cache_key(self, prompt: str, config: ModelConfig) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""

        content_hash = hashlib.md5(prompt.encode()).hexdigest()[:16]
        config_hash = hashlib.md5(
            f"{config.model}:{config.temperature}:{config.max_tokens}".encode()
        ).hexdigest()[:8]

        return f"deepseek:optimized:{content_hash}:{config_hash}"


class IntelligentCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self):
        self.cache_prefix = "deepseek_optimized"

    async def get_cached_result(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜ç»“æœ"""

        full_key = f"{self.cache_prefix}:{cache_key}"
        cached_data = cache.get(full_key)

        if cached_data:
            # æ›´æ–°ç¼“å­˜å‘½ä¸­ç»Ÿè®¡
            self._update_cache_stats("hit")
            return cached_data

        self._update_cache_stats("miss")
        return None

    async def cache_result(
        self,
        cache_key: str,
        result: Dict[str, Any],
        ttl: int = 3600
    ) -> None:
        """ç¼“å­˜ç»“æœ"""

        full_key = f"{self.cache_prefix}:{cache_key}"

        # åªç¼“å­˜æˆåŠŸçš„ç»“æœ
        if result.get("success", True):
            cache.set(full_key, result, ttl)
            self._update_cache_stats("store")

    def _update_cache_stats(self, operation: str) -> None:
        """æ›´æ–°ç¼“å­˜ç»Ÿè®¡"""

        stats_key = f"{self.cache_prefix}:stats"
        stats = cache.get(stats_key, {"hits": 0, "misses": 0, "stores": 0})

        if operation in stats:
            stats[operation] += 1
            cache.set(stats_key, stats, 86400)  # 24å°æ—¶


class DeepSeekPromptOptimizer:
    """DeepSeekæç¤ºè¯ä¼˜åŒ–å™¨"""

    def build_reasoning_prompt(
        self,
        problem_statement: str,
        analysis_context: Dict[str, Any],
        expected_output: str
    ) -> str:
        """æ„å»ºæ¨ç†æç¤ºè¯"""

        context_str = self._format_context(analysis_context)

        return f"""# æ·±åº¦åˆ†æä»»åŠ¡

## é—®é¢˜æè¿°
{problem_statement}

## åˆ†æææ–™
{context_str}

## åˆ†æè¦æ±‚
è¯·ä½¿ç”¨ä»¥ä¸‹ç»“æ„è¿›è¡Œæ·±åº¦åˆ†æï¼š

<think>
è¯·åœ¨è¿™é‡Œè¿›è¡Œè¯¦ç»†çš„æ€è€ƒå’Œæ¨ç†ï¼š
1. é—®é¢˜ç†è§£å’Œå…³é”®ä¿¡æ¯è¯†åˆ«
2. åˆ†ææ–¹æ³•é€‰æ‹©å’Œåº”ç”¨
3. é€»è¾‘æ¨ç†è¿‡ç¨‹
4. ç»“è®ºéªŒè¯å’Œè´¨é‡æ£€æŸ¥
</think>

<answer>
{expected_output}
</answer>

## è´¨é‡æ ‡å‡†
- æ¨ç†è¿‡ç¨‹è¦é€»è¾‘æ¸…æ™°ã€æ­¥éª¤å®Œæ•´
- æœ€ç»ˆç­”æ¡ˆè¦å‡†ç¡®ã€å®Œæ•´ã€ç¬¦åˆæ ¼å¼è¦æ±‚
- å¦‚æœ‰ä¸ç¡®å®šæ€§ï¼Œè¯·æ˜ç¡®è¯´æ˜å¹¶æä¾›æœ€ä½³åˆ¤æ–­
"""

    def build_structured_prompt(
        self,
        task_description: str,
        context_data: Dict[str, Any],
        output_format: str,
        reasoning_required: bool = False
    ) -> str:
        """æ„å»ºç»“æ„åŒ–æç¤ºè¯"""

        prompt_parts = [
            f"# ä»»åŠ¡ï¼š{task_description}",
            "",
            "## è¾“å…¥ä¿¡æ¯"
        ]

        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        for key, value in context_data.items():
            if isinstance(value, (dict, list)):
                value_str = json.dumps(value, ensure_ascii=False, indent=2)
            else:
                value_str = str(value)

            prompt_parts.append(f"**{key}**:")
            prompt_parts.append(f"```\n{value_str}\n```")
            prompt_parts.append("")

        # æ·»åŠ å¤„ç†è¦æ±‚
        if reasoning_required:
            prompt_parts.extend([
                "## å¤„ç†è¦æ±‚",
                "è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è¿›è¡Œåˆ†æï¼š",
                "1. ç†è§£ä»»åŠ¡è¦æ±‚å’Œè¾“å…¥ä¿¡æ¯",
                "2. åˆ†æå…³é”®è¦ç´ å’Œå…³ç³»",
                "3. åº”ç”¨ä¸“ä¸šçŸ¥è¯†è¿›è¡Œæ¨ç†",
                "4. ç”Ÿæˆç¬¦åˆè¦æ±‚çš„è¾“å‡º",
                ""
            ])

        # æ·»åŠ è¾“å‡ºæ ¼å¼
        prompt_parts.extend([
            "## è¾“å‡ºæ ¼å¼",
            output_format,
            "",
            "## è´¨é‡è¦æ±‚",
            "- ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡º",
            "- ç¡®ä¿å†…å®¹å‡†ç¡®ã€å®Œæ•´",
            "- é€»è¾‘æ¸…æ™°ã€è¡¨è¿°è§„èŒƒ",
            "- é¿å…é‡å¤å’Œå†—ä½™ä¿¡æ¯"
        ])

        return "\n".join(prompt_parts)

    def _format_context(self, context: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯"""

        formatted_parts = []

        for key, value in context.items():
            if isinstance(value, str) and len(value) > 1000:
                # é•¿æ–‡æœ¬æˆªå–
                value = value[:1000] + "...[å†…å®¹æˆªå–]"
            elif isinstance(value, (dict, list)):
                value = json.dumps(value, ensure_ascii=False, indent=2)

            formatted_parts.append(f"**{key}**: {value}")

        return "\n".join(formatted_parts)
````

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [DeepSeekä¼˜åŒ–ç­–ç•¥](./deepseek-optimization-strategy.md)
- [æŠ€æœ¯å®ç°è¯¦ç»†è®¾è®¡](./teaching-syllabus-technical-implementation.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025-01-22  
**æœ€åæ›´æ–°**: 2025-01-22
