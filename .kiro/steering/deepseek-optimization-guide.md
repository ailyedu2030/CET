---
inclusion: always
---

# DeepSeek API æœ€ä½³å®è·µæŒ‡å—

## ğŸ¯ æœ€æ–°æ¨¡å‹ä¿¡æ¯ï¼ˆ2025å¹´1æœˆï¼‰

### å¯ç”¨æ¨¡å‹
1. **deepseek-chat** (DeepSeek-V3-0324)
   - ä¸Šä¸‹æ–‡é•¿åº¦ï¼š64K
   - è¾“å‡ºé•¿åº¦ï¼šé»˜è®¤4Kï¼Œæœ€å¤§8K
   - é€‚ç”¨åœºæ™¯ï¼šé€šç”¨å¯¹è¯ã€å†…å®¹ç”Ÿæˆã€ç»“æ„åŒ–è¾“å‡º

2. **deepseek-reasoner** (DeepSeek-R1-0528) ğŸ†•
   - ä¸Šä¸‹æ–‡é•¿åº¦ï¼š64K
   - è¾“å‡ºé•¿åº¦ï¼šé»˜è®¤32Kï¼Œæœ€å¤§64K
   - é€‚ç”¨åœºæ™¯ï¼šå¤æ‚æ¨ç†ã€æ·±åº¦åˆ†æã€å¤šæ­¥éª¤æ€è€ƒ
   - ç‰¹æ®ŠåŠŸèƒ½ï¼šåŒ…å«æ€ç»´é“¾è¾“å‡º

## ğŸ’° æœ€æ–°å®šä»·ç­–ç•¥

### æ ‡å‡†æ—¶æ®µä»·æ ¼ï¼ˆåŒ—äº¬æ—¶é—´ 08:30-00:30ï¼‰
| æ¨¡å‹ | è¾“å…¥ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰| è¾“å…¥ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰| è¾“å‡º |
|------|----------------|------------------|------|
| deepseek-chat | 0.5å…ƒ/ç™¾ä¸‡tokens | 2å…ƒ/ç™¾ä¸‡tokens | 8å…ƒ/ç™¾ä¸‡tokens |
| deepseek-reasoner | 1å…ƒ/ç™¾ä¸‡tokens | 4å…ƒ/ç™¾ä¸‡tokens | 16å…ƒ/ç™¾ä¸‡tokens |

### ğŸŒ™ é”™å³°ä¼˜æƒ æ—¶æ®µï¼ˆåŒ—äº¬æ—¶é—´ 00:30-08:30ï¼‰
| æ¨¡å‹ | è¾“å…¥ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰| è¾“å…¥ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰| è¾“å‡º |
|------|----------------|------------------|------|
| deepseek-chat | 0.25å…ƒï¼ˆ5æŠ˜ï¼‰| 1å…ƒï¼ˆ5æŠ˜ï¼‰| 4å…ƒï¼ˆ5æŠ˜ï¼‰|
| deepseek-reasoner | 0.25å…ƒï¼ˆ2.5æŠ˜ï¼‰| 1å…ƒï¼ˆ2.5æŠ˜ï¼‰| 4å…ƒï¼ˆ2.5æŠ˜ï¼‰|

## ğŸš€ æ™ºèƒ½æ¨¡å‹é€‰æ‹©ç­–ç•¥

### æ•™è‚²ç³»ç»Ÿåœºæ™¯ä¼˜åŒ–é…ç½®

```python
class OptimizedDeepSeekConfig:
    """ä¼˜åŒ–çš„DeepSeeké…ç½®"""
    
    # æ¨¡å‹é€‰æ‹©ç­–ç•¥
    MODEL_SELECTION = {
        # å¤æ‚åˆ†æä»»åŠ¡ - ä½¿ç”¨æ¨ç†æ¨¡å‹
        "complex_analysis": {
            "model": "deepseek-reasoner",
            "temperature": 1.0,  # æ•°æ®åˆ†æåœºæ™¯ï¼Œå®˜æ–¹æ¨èæ¸©åº¦
            "max_tokens": 8000,
            "top_p": 0.9,
            "use_cases": [
                "å­¦æƒ…æ·±åº¦åˆ†æ",
                "æ•™å­¦ç­–ç•¥æ¨ç†", 
                "å¤æ‚é¢˜ç›®ç”Ÿæˆ",
                "å¤šç»´åº¦è¯„ä¼°"
            ]
        },
        
        # ç»“æ„åŒ–ç”Ÿæˆ - ä½¿ç”¨æ ‡å‡†æ¨¡å‹åˆ†ææ¸©åº¦
        "structured_generation": {
            "model": "deepseek-chat",
            "temperature": 1.0,  # æ•°æ®åˆ†æåœºæ™¯ï¼Œå®˜æ–¹æ¨èæ¸©åº¦
            "max_tokens": 4000,
            "top_p": 0.8,
            "use_cases": [
                "JSONæ ¼å¼è¾“å‡º",
                "æ ‡å‡†åŒ–æ‰¹æ”¹",
                "æ•°æ®æå–",
                "æ ¼å¼è½¬æ¢"
            ]
        },
        
        # åˆ›æ„ç”Ÿæˆ - ä½¿ç”¨æ ‡å‡†æ¨¡å‹åˆ›æ„æ¸©åº¦
        "creative_generation": {
            "model": "deepseek-chat", 
            "temperature": 1.5,  # åˆ›æ„ç±»å†™ä½œåœºæ™¯ï¼Œå®˜æ–¹æ¨èæ¸©åº¦
            "max_tokens": 6000,
            "top_p": 0.95,
            "use_cases": [
                "é¢˜ç›®åˆ›æ„ç”Ÿæˆ",
                "æ•™æ¡ˆå†…å®¹åˆ›ä½œ",
                "å­¦ä¹ å»ºè®®ç”Ÿæˆ",
                "åé¦ˆæ–‡æœ¬ç”Ÿæˆ"
            ]
        }
    }
    
    # æˆæœ¬ä¼˜åŒ–ç­–ç•¥
    COST_OPTIMIZATION = {
        "peak_hours_avoidance": {
            "enabled": True,
            "off_peak_start": "00:30",  # åŒ—äº¬æ—¶é—´
            "off_peak_end": "08:30",
            "cost_savings": {
                "deepseek-chat": 0.5,      # 5æŠ˜
                "deepseek-reasoner": 0.25   # 2.5æŠ˜
            }
        },
        
        "caching_strategy": {
            "enabled": True,
            "cache_hit_cost_ratio": 0.25,  # ç¼“å­˜å‘½ä¸­ä»…25%æˆæœ¬
            "cache_ttl": {
                "question_generation": 3600,    # 1å°æ—¶
                "content_analysis": 7200,       # 2å°æ—¶  
                "student_assessment": 1800      # 30åˆ†é’Ÿ
            }
        }
    }
```

## ğŸ¯ æ•™è‚²åœºæ™¯ä¸“ç”¨é…ç½®

### 1. æ™ºèƒ½æ‰¹æ”¹ç³»ç»Ÿé…ç½®

```python
# å†™ä½œæ‰¹æ”¹ - ä½¿ç”¨æ¨ç†æ¨¡å‹ç¡®ä¿å‡†ç¡®æ€§
WRITING_GRADING_CONFIG = {
    "model": "deepseek-reasoner",
    "temperature": 0.5,  # ç¨ä½æ¸©åº¦ä¿è¯è¯„åˆ†ä¸€è‡´æ€§
    "max_tokens": 4000,
    "system_prompt": """ä½ æ˜¯ä¸“ä¸šçš„è‹±è¯­å››çº§å†™ä½œè¯„åˆ†ä¸“å®¶ã€‚
è¯·æŒ‰ç…§å››çº§è¯„åˆ†æ ‡å‡†è¿›è¡Œè¯¦ç»†åˆ†æå’Œè¯„åˆ†ã€‚

<think>
åœ¨è¿™é‡Œè¿›è¡Œè¯¦ç»†çš„è¯„åˆ†æ€è€ƒè¿‡ç¨‹ï¼š
1. å†…å®¹åˆ†æï¼ˆæ˜¯å¦åˆ‡é¢˜ã€è§‚ç‚¹æ˜¯å¦æ˜ç¡®ï¼‰
2. è¯­è¨€è´¨é‡ï¼ˆè¯­æ³•ã€è¯æ±‡ã€å¥å¼ï¼‰
3. ç»„ç»‡ç»“æ„ï¼ˆé€»è¾‘æ€§ã€è¿è´¯æ€§ï¼‰
4. æ ¼å¼è§„èŒƒï¼ˆå­—æ•°ã€æ ¼å¼è¦æ±‚ï¼‰
</think>

<answer>
æä¾›æœ€ç»ˆè¯„åˆ†å’Œè¯¦ç»†åé¦ˆ
</answer>"""
}

# é¢˜ç›®ç”Ÿæˆ - ä½¿ç”¨æ ‡å‡†æ¨¡å‹å¹³è¡¡åˆ›æ„å’Œè´¨é‡
QUESTION_GENERATION_CONFIG = {
    "model": "deepseek-chat",
    "temperature": 0.7,
    "max_tokens": 3000,
    "response_format": {"type": "json_object"}  # ç»“æ„åŒ–è¾“å‡º
}
```

### 2. å­¦æƒ…åˆ†æç³»ç»Ÿé…ç½®

```python
# æ·±åº¦å­¦æƒ…åˆ†æ - ä½¿ç”¨æ¨ç†æ¨¡å‹
LEARNING_ANALYSIS_CONFIG = {
    "model": "deepseek-reasoner", 
    "temperature": 0.6,
    "max_tokens": 6000,
    "system_prompt": """ä½ æ˜¯æ•™è‚²æ•°æ®åˆ†æä¸“å®¶ï¼Œè¯·å¯¹å­¦ç”Ÿå­¦ä¹ æ•°æ®è¿›è¡Œæ·±åº¦åˆ†æã€‚

<think>
åˆ†ææ€è·¯ï¼š
1. æ•°æ®æ¨¡å¼è¯†åˆ«
2. å­¦ä¹ è¶‹åŠ¿åˆ†æ  
3. è–„å¼±ç¯èŠ‚è¯Šæ–­
4. æ”¹è¿›ç­–ç•¥æ¨ç†
</think>

<answer>
æä¾›å…·ä½“çš„åˆ†æç»“è®ºå’Œå»ºè®®
</answer>"""
}
```

## âš¡ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. æ™ºèƒ½ç¼“å­˜ç®¡ç†

```python
class IntelligentCacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self.cache_strategies = {
            # é«˜é¢‘å¤ç”¨å†…å®¹ - é•¿æœŸç¼“å­˜
            "vocabulary_explanations": {
                "ttl": 86400,  # 24å°æ—¶
                "cost_savings": 0.75  # 75%æˆæœ¬èŠ‚çœ
            },
            
            # å­¦ç”Ÿåˆ†æ - ä¸­æœŸç¼“å­˜  
            "student_analysis": {
                "ttl": 3600,   # 1å°æ—¶
                "cost_savings": 0.75
            },
            
            # é¢˜ç›®ç”Ÿæˆ - çŸ­æœŸç¼“å­˜
            "question_generation": {
                "ttl": 1800,   # 30åˆ†é’Ÿ
                "cost_savings": 0.75
            }
        }
    
    def generate_cache_key(self, content: str, model_config: dict) -> str:
        """ç”Ÿæˆæ™ºèƒ½ç¼“å­˜é”®"""
        content_hash = hashlib.md5(content.encode()).hexdigest()[:16]
        config_hash = hashlib.md5(str(sorted(model_config.items())).encode()).hexdigest()[:8]
        return f"deepseek:{content_hash}:{config_hash}"
```

### 2. é”™å³°è°ƒåº¦ç³»ç»Ÿ

```python
class OffPeakScheduler:
    """é”™å³°è°ƒåº¦å™¨"""
    
    def __init__(self):
        self.off_peak_hours = {
            "start": "00:30",  # åŒ—äº¬æ—¶é—´
            "end": "08:30",
            "timezone": "Asia/Shanghai"
        }
    
    def is_off_peak_time(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé”™å³°æ—¶æ®µ"""
        beijing_tz = pytz.timezone('Asia/Shanghai')
        now = datetime.now(beijing_tz)
        current_time = now.time()
        
        off_peak_start = time(0, 30)  # 00:30
        off_peak_end = time(8, 30)    # 08:30
        
        return off_peak_start <= current_time <= off_peak_end
    
    async def schedule_api_call(self, api_call_func, priority: str = "normal"):
        """æ™ºèƒ½è°ƒåº¦APIè°ƒç”¨"""
        if priority == "urgent":
            # ç´§æ€¥ä»»åŠ¡ç«‹å³æ‰§è¡Œ
            return await api_call_func()
        
        elif priority == "cost_sensitive" and not self.is_off_peak_time():
            # æˆæœ¬æ•æ„Ÿä»»åŠ¡å»¶è¿Ÿåˆ°é”™å³°æ—¶æ®µ
            delay_seconds = self._calculate_delay_to_off_peak()
            await asyncio.sleep(delay_seconds)
        
        return await api_call_func()
```

## ğŸ”§ APIè°ƒç”¨æœ€ä½³å®è·µ

### 1. æ ‡å‡†APIè°ƒç”¨æ¨¡æ¿

```python
async def optimized_deepseek_call(
    messages: List[dict],
    task_type: str,
    priority: str = "normal"
) -> dict:
    """ä¼˜åŒ–çš„DeepSeek APIè°ƒç”¨"""
    
    # 1. æ™ºèƒ½æ¨¡å‹é€‰æ‹©
    config = OptimizedDeepSeekConfig.MODEL_SELECTION.get(
        task_type, 
        OptimizedDeepSeekConfig.MODEL_SELECTION["structured_generation"]
    )
    
    # 2. ç¼“å­˜æ£€æŸ¥
    cache_key = cache_manager.generate_cache_key(
        str(messages), config
    )
    cached_result = await cache_manager.get(cache_key)
    if cached_result:
        return cached_result
    
    # 3. é”™å³°è°ƒåº¦
    result = await off_peak_scheduler.schedule_api_call(
        lambda: _call_deepseek_api(messages, config),
        priority
    )
    
    # 4. ç»“æœç¼“å­˜
    await cache_manager.set(cache_key, result, ttl=3600)
    
    return result

async def _call_deepseek_api(messages: List[dict], config: dict) -> dict:
    """å®é™…çš„APIè°ƒç”¨"""
    try:
        response = await openai_client.chat.completions.create(
            model=config["model"],
            messages=messages,
            temperature=config["temperature"],
            max_tokens=config["max_tokens"],
            top_p=config.get("top_p", 1.0),
            stream=False
        )
        
        # å¤„ç†æ¨ç†æ¨¡å‹çš„ç‰¹æ®Šè¾“å‡º
        if config["model"] == "deepseek-reasoner":
            content = response.choices[0].message.content
            thinking, answer = extract_reasoning_parts(content)
            return {
                "thinking": thinking,
                "answer": answer,
                "usage": response.usage.dict(),
                "model": config["model"]
            }
        else:
            return {
                "content": response.choices[0].message.content,
                "usage": response.usage.dict(),
                "model": config["model"]
            }
            
    except Exception as e:
        logger.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {e}")
        raise
```

### 2. æ¨ç†æ¨¡å‹ä¸“ç”¨å¤„ç†

```python
def extract_reasoning_parts(content: str) -> tuple:
    """æå–æ¨ç†æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ"""
    import re
    
    # æå–æ€è€ƒè¿‡ç¨‹
    think_pattern = r'<think>(.*?)</think>'
    think_match = re.search(think_pattern, content, re.DOTALL)
    thinking = think_match.group(1).strip() if think_match else ""
    
    # æå–æœ€ç»ˆç­”æ¡ˆ
    answer_pattern = r'<answer>(.*?)</answer>'
    answer_match = re.search(answer_pattern, content, re.DOTALL)
    answer = answer_match.group(1).strip() if answer_match else content
    
    return thinking, answer
```

## ğŸ“Š æˆæœ¬æ§åˆ¶ç­–ç•¥

### é¢„æœŸæˆæœ¬ä¼˜åŒ–æ•ˆæœ

```python
COST_OPTIMIZATION_PROJECTION = {
    "current_monthly_cost": 500,  # å‡è®¾å½“å‰æœˆæˆæœ¬
    "optimizations": {
        "off_peak_scheduling": {
            "savings_ratio": 0.4,  # 40%è¯·æ±‚åœ¨é”™å³°æ—¶æ®µ
            "discount": 0.5,       # 5æŠ˜ä¼˜æƒ 
            "monthly_savings": 100  # èŠ‚çœ100å…ƒ
        },
        "intelligent_caching": {
            "cache_hit_rate": 0.3,  # 30%ç¼“å­˜å‘½ä¸­ç‡
            "cost_reduction": 0.75, # 75%æˆæœ¬é™ä½
            "monthly_savings": 75   # èŠ‚çœ75å…ƒ
        },
        "model_optimization": {
            "reasoner_usage_optimization": 0.2,  # 20%ä½¿ç”¨æ¨ç†æ¨¡å‹
            "chat_model_efficiency": 0.15,      # 15%æ•ˆç‡æå‡
            "monthly_savings": 50               # èŠ‚çœ50å…ƒ
        }
    },
    "total_monthly_savings": 225,  # æ€»è®¡èŠ‚çœ225å…ƒ
    "optimized_monthly_cost": 275, # ä¼˜åŒ–åæˆæœ¬275å…ƒ
    "cost_reduction_ratio": 0.45   # 45%æˆæœ¬é™ä½
}
```

## ğŸ¯ æ•™è‚²ç³»ç»Ÿä¸“ç”¨ä¼˜åŒ–

### 1. æ‰¹æ”¹ç³»ç»Ÿä¼˜åŒ–

```python
# æ™ºèƒ½æ‰¹æ”¹é…ç½®
GRADING_OPTIMIZATION = {
    "writing_assessment": {
        "model": "deepseek-chat",      # æ ‡å‡†æ¨¡å‹ï¼Œæ•°æ®åˆ†æåœºæ™¯
        "temperature": 1.0,            # æ•°æ®åˆ†æåœºæ™¯å®˜æ–¹æ¨èæ¸©åº¦
        "max_tokens": 4000,
        "cost_per_assessment": "çº¦0.2å…ƒ"  # é¢„ä¼°æˆæœ¬ï¼ˆä½¿ç”¨chatæ¨¡å‹æ›´ä¾¿å®œï¼‰
    },
    
    "multiple_choice_grading": {
        "model": "deepseek-chat",      # æ ‡å‡†æ¨¡å‹è¶³å¤Ÿ
        "temperature": 0.0,            # ä»£ç ç”Ÿæˆåœºæ™¯ï¼Œéœ€è¦ç²¾ç¡®æ€§
        "max_tokens": 1000,
        "cost_per_assessment": "çº¦0.03å…ƒ"
    }
}
```

### 2. ä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„

```python
# å­¦ä¹ è·¯å¾„ç”Ÿæˆé…ç½®
LEARNING_PATH_CONFIG = {
    "deep_analysis": {
        "model": "deepseek-reasoner",
        "temperature": 1.0,  # æ•°æ®åˆ†æåœºæ™¯ï¼Œæ·±åº¦å­¦æƒ…åˆ†æ
        "max_tokens": 6000,
        "schedule": "off_peak_preferred"  # ä¼˜å…ˆé”™å³°æ—¶æ®µ
    },
    
    "quick_recommendations": {
        "model": "deepseek-chat", 
        "temperature": 1.3,  # é€šç”¨å¯¹è¯åœºæ™¯ï¼Œæä¾›å­¦ä¹ å»ºè®®
        "max_tokens": 2000,
        "cache_ttl": 1800  # 30åˆ†é’Ÿç¼“å­˜
    }
}
```

è¿™ä¸ªé…ç½®æŒ‡å—ç¡®ä¿DeepSeek APIåœ¨æ•™è‚²ç³»ç»Ÿä¸­çš„æœ€ä¼˜ä½¿ç”¨ï¼Œå¹³è¡¡äº†æˆæœ¬ã€æ€§èƒ½å’Œè´¨é‡ã€‚