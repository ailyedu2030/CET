---
inclusion: always
---

# å®æ—¶æ€§èƒ½æµ‹è¯•é…ç½®æŒ‡å—

## ğŸ¯ æµ‹è¯•ç›®æ ‡

ç¡®ä¿è‹±è¯­å››çº§å­¦ä¹ ç³»ç»Ÿçš„å®æ—¶åŠŸèƒ½æ»¡è¶³ç”¨æˆ·ä½“éªŒè¦æ±‚ï¼Œç‰¹åˆ«æ˜¯æ™ºèƒ½æ‰¹æ”¹å’Œå®æ—¶è¾…åŠ©åŠŸèƒ½çš„å“åº”æ—¶é—´å’Œç¨³å®šæ€§ã€‚

## ğŸ“Š æ€§èƒ½åŸºå‡†è¦æ±‚

### å®æ—¶æ€§èƒ½æŒ‡æ ‡

| åŠŸèƒ½åœºæ™¯ | å“åº”æ—¶é—´è¦æ±‚ | æˆåŠŸç‡è¦æ±‚ | å¹¶å‘ç”¨æˆ· | æµ‹è¯•æŒç»­æ—¶é—´ |
|---------|-------------|-----------|---------|-------------|
| æ™ºèƒ½æ‰¹æ”¹ | <3ç§’ | >95% | 500ç”¨æˆ· | 30åˆ†é’Ÿ |
| å®æ—¶è¾…åŠ© | <1ç§’ | >98% | 200ç”¨æˆ· | 60åˆ†é’Ÿ |
| å¬åŠ›å­—å¹• | <1ç§’ | >99% | 100ç”¨æˆ· | 30åˆ†é’Ÿ |
| é¢˜ç›®ç”Ÿæˆ | <2ç§’ | >95% | 300ç”¨æˆ· | 15åˆ†é’Ÿ |
| å­¦æƒ…åˆ†æ | <5ç§’ | >90% | 100ç”¨æˆ· | 10åˆ†é’Ÿ |

### æµå¼è¾“å‡ºæ€§èƒ½è¦æ±‚

- **é¦–å­—èŠ‚æ—¶é—´**ï¼š<500ms
- **æµå¼é—´éš”**ï¼š<200ms
- **å®Œæ•´å“åº”æ—¶é—´**ï¼šç¬¦åˆä¸Šè¡¨è¦æ±‚
- **è¿æ¥ç¨³å®šæ€§**ï¼š>99.5%
- **é‡è¿æˆåŠŸç‡**ï¼š>95%

## ğŸ§ª æµ‹è¯•ç”¨ä¾‹è®¾è®¡

### 1. æµå¼æ‰¹æ”¹æ€§èƒ½æµ‹è¯•

```python
# tests/performance/test_streaming_grading.py
import asyncio
import time
import pytest
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict

class StreamingGradingPerformanceTest:
    """æµå¼æ‰¹æ”¹æ€§èƒ½æµ‹è¯•"""
    
    def __init__(self):
        self.base_url = "http://localhost:8000"
        self.test_writings = self._load_test_writings()
        self.performance_metrics = []
    
    async def test_single_grading_performance(self):
        """å•æ¬¡æ‰¹æ”¹æ€§èƒ½æµ‹è¯•"""
        
        writing_content = self.test_writings[0]
        start_time = time.time()
        
        # å»ºç«‹SSEè¿æ¥
        async with aiohttp.ClientSession() as session:
            async with session.get(
                f"{self.base_url}/api/grading/stream",
                params={
                    "submission_id": "test_001",
                    "question_type": "argumentative"
                }
            ) as response:
                
                first_byte_time = None
                chunks_received = 0
                chunk_intervals = []
                last_chunk_time = start_time
                
                async for line in response.content:
                    current_time = time.time()
                    
                    if first_byte_time is None:
                        first_byte_time = current_time - start_time
                    
                    if chunks_received > 0:
                        interval = current_time - last_chunk_time
                        chunk_intervals.append(interval)
                    
                    chunks_received += 1
                    last_chunk_time = current_time
                
                total_time = time.time() - start_time
                
                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                metrics = {
                    'total_time': total_time,
                    'first_byte_time': first_byte_time,
                    'chunks_received': chunks_received,
                    'avg_chunk_interval': sum(chunk_intervals) / len(chunk_intervals) if chunk_intervals else 0,
                    'max_chunk_interval': max(chunk_intervals) if chunk_intervals else 0
                }
                
                # éªŒè¯æ€§èƒ½è¦æ±‚
                assert total_time < 3.0, f"æ‰¹æ”¹æ€»æ—¶é—´è¶…æ ‡: {total_time}s"
                assert first_byte_time < 0.5, f"é¦–å­—èŠ‚æ—¶é—´è¶…æ ‡: {first_byte_time}s"
                assert metrics['avg_chunk_interval'] < 0.2, f"å¹³å‡æµå¼é—´éš”è¶…æ ‡: {metrics['avg_chunk_interval']}s"
                
                return metrics
    
    async def test_concurrent_grading_performance(self, concurrent_users: int = 100):
        """å¹¶å‘æ‰¹æ”¹æ€§èƒ½æµ‹è¯•"""
        
        async def single_user_test(user_id: int):
            try:
                start_time = time.time()
                
                # æ¨¡æ‹Ÿç”¨æˆ·æäº¤æ‰¹æ”¹è¯·æ±‚
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        f"{self.base_url}/api/grading/stream",
                        params={
                            "submission_id": f"test_{user_id:03d}",
                            "question_type": "argumentative"
                        }
                    ) as response:
                        
                        chunks = []
                        async for line in response.content:
                            chunks.append(line)
                        
                        total_time = time.time() - start_time
                        
                        return {
                            'user_id': user_id,
                            'success': True,
                            'response_time': total_time,
                            'chunks_count': len(chunks)
                        }
                        
            except Exception as e:
                return {
                    'user_id': user_id,
                    'success': False,
                    'error': str(e),
                    'response_time': time.time() - start_time
                }
        
        # å¹¶å‘æ‰§è¡Œæµ‹è¯•
        tasks = [single_user_test(i) for i in range(concurrent_users)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆ†æç»“æœ
        successful_results = [r for r in results if isinstance(r, dict) and r.get('success')]
        failed_results = [r for r in results if isinstance(r, dict) and not r.get('success')]
        
        success_rate = len(successful_results) / len(results)
        avg_response_time = sum(r['response_time'] for r in successful_results) / len(successful_results)
        
        # éªŒè¯å¹¶å‘æ€§èƒ½è¦æ±‚
        assert success_rate >= 0.95, f"å¹¶å‘æˆåŠŸç‡ä¸è¾¾æ ‡: {success_rate:.2%}"
        assert avg_response_time < 3.0, f"å¹¶å‘å¹³å‡å“åº”æ—¶é—´è¶…æ ‡: {avg_response_time}s"
        
        return {
            'concurrent_users': concurrent_users,
            'success_rate': success_rate,
            'avg_response_time': avg_response_time,
            'failed_count': len(failed_results)
        }
    
    def _load_test_writings(self) -> List[str]:
        """åŠ è½½æµ‹è¯•ç”¨çš„ä½œæ–‡å†…å®¹"""
        return [
            "In today's digital age, online learning has become increasingly popular...",
            "Environmental protection is one of the most pressing issues of our time...",
            "The role of artificial intelligence in education is rapidly evolving...",
            # æ›´å¤šæµ‹è¯•å†…å®¹...
        ]

### 2. å®æ—¶å†™ä½œè¾…åŠ©æ€§èƒ½æµ‹è¯•

```python
# tests/performance/test_realtime_assist.py
class RealtimeAssistPerformanceTest:
    """å®æ—¶å†™ä½œè¾…åŠ©æ€§èƒ½æµ‹è¯•"""
    
    async def test_typing_simulation(self):
        """æ¨¡æ‹Ÿç”¨æˆ·æ‰“å­—çš„å®æ—¶è¾…åŠ©æµ‹è¯•"""
        
        text_sequence = [
            "In today's",
            "In today's digital",
            "In today's digital age,",
            "In today's digital age, online",
            "In today's digital age, online learning",
            "In today's digital age, online learning has",
            "In today's digital age, online learning has become"
        ]
        
        response_times = []
        
        for i, text in enumerate(text_sequence):
            start_time = time.time()
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.base_url}/api/writing/realtime-assist",
                    json={
                        "text": text,
                        "cursor_position": len(text)
                    }
                ) as response:
                    
                    suggestion = ""
                    async for chunk in response.content.iter_chunked(1024):
                        suggestion += chunk.decode()
                    
                    response_time = time.time() - start_time
                    response_times.append(response_time)
                    
                    # éªŒè¯å®æ—¶è¾…åŠ©å“åº”æ—¶é—´
                    assert response_time < 1.0, f"å®æ—¶è¾…åŠ©å“åº”æ—¶é—´è¶…æ ‡: {response_time}s"
        
        avg_response_time = sum(response_times) / len(response_times)
        assert avg_response_time < 0.8, f"å¹³å‡å“åº”æ—¶é—´è¶…æ ‡: {avg_response_time}s"
        
        return {
            'sequence_count': len(text_sequence),
            'avg_response_time': avg_response_time,
            'max_response_time': max(response_times),
            'all_response_times': response_times
        }
    
    async def test_debounce_mechanism(self):
        """æµ‹è¯•é˜²æŠ–æœºåˆ¶æ€§èƒ½"""
        
        # æ¨¡æ‹Ÿå¿«é€Ÿè¿ç»­è¾“å…¥
        rapid_inputs = [
            "Hello",
            "Hello w",
            "Hello wo",
            "Hello wor",
            "Hello worl",
            "Hello world"
        ]
        
        # å¿«é€Ÿå‘é€è¯·æ±‚ï¼ˆé—´éš”100msï¼‰
        tasks = []
        for text in rapid_inputs:
            task = asyncio.create_task(self._send_assist_request(text))
            tasks.append(task)
            await asyncio.sleep(0.1)  # 100msé—´éš”
        
        # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # éªŒè¯é˜²æŠ–æ•ˆæœï¼ˆåº”è¯¥åªæœ‰æœ€åå‡ ä¸ªè¯·æ±‚æˆåŠŸï¼‰
        successful_requests = [r for r in results if not isinstance(r, Exception)]
        
        # é˜²æŠ–åº”è¯¥å‡å°‘å®é™…å¤„ç†çš„è¯·æ±‚æ•°é‡
        assert len(successful_requests) < len(rapid_inputs), "é˜²æŠ–æœºåˆ¶æœªç”Ÿæ•ˆ"
        
        return {
            'total_requests': len(rapid_inputs),
            'successful_requests': len(successful_requests),
            'debounce_effectiveness': 1 - (len(successful_requests) / len(rapid_inputs))
        }

### 3. ç³»ç»Ÿå‹åŠ›æµ‹è¯•

```python
# tests/performance/test_system_stress.py
class SystemStressTest:
    """ç³»ç»Ÿå‹åŠ›æµ‹è¯•"""
    
    async def test_mixed_workload_stress(self):
        """æ··åˆå·¥ä½œè´Ÿè½½å‹åŠ›æµ‹è¯•"""
        
        # å®šä¹‰ä¸åŒç±»å‹çš„å·¥ä½œè´Ÿè½½
        workloads = {
            'grading': {'weight': 0.4, 'concurrent': 200},      # 40%æ‰¹æ”¹è¯·æ±‚
            'assist': {'weight': 0.3, 'concurrent': 150},       # 30%å®æ—¶è¾…åŠ©
            'generation': {'weight': 0.2, 'concurrent': 100},   # 20%é¢˜ç›®ç”Ÿæˆ
            'analysis': {'weight': 0.1, 'concurrent': 50}       # 10%å­¦æƒ…åˆ†æ
        }
        
        async def execute_workload(workload_type: str, config: dict):
            tasks = []
            
            for i in range(config['concurrent']):
                if workload_type == 'grading':
                    task = self._execute_grading_request(i)
                elif workload_type == 'assist':
                    task = self._execute_assist_request(i)
                elif workload_type == 'generation':
                    task = self._execute_generation_request(i)
                elif workload_type == 'analysis':
                    task = self._execute_analysis_request(i)
                
                tasks.append(task)
            
            return await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰å·¥ä½œè´Ÿè½½
        all_tasks = []
        for workload_type, config in workloads.items():
            task = execute_workload(workload_type, config)
            all_tasks.append(task)
        
        start_time = time.time()
        all_results = await asyncio.gather(*all_tasks)
        total_time = time.time() - start_time
        
        # åˆ†æç»“æœ
        total_requests = sum(config['concurrent'] for config in workloads.values())
        successful_requests = 0
        
        for workload_results in all_results:
            for result in workload_results:
                if isinstance(result, dict) and result.get('success'):
                    successful_requests += 1
        
        success_rate = successful_requests / total_requests
        throughput = successful_requests / total_time
        
        # éªŒè¯ç³»ç»Ÿå‹åŠ›æµ‹è¯•è¦æ±‚
        assert success_rate >= 0.90, f"ç³»ç»Ÿå‹åŠ›æµ‹è¯•æˆåŠŸç‡ä¸è¾¾æ ‡: {success_rate:.2%}"
        assert throughput >= 50, f"ç³»ç»Ÿååé‡ä¸è¾¾æ ‡: {throughput:.1f} req/s"
        
        return {
            'total_requests': total_requests,
            'successful_requests': successful_requests,
            'success_rate': success_rate,
            'total_time': total_time,
            'throughput': throughput
        }

## ğŸ“ˆ æ€§èƒ½ç›‘æ§å’ŒæŠ¥å‘Š

### å®æ—¶æ€§èƒ½ç›‘æ§é¢æ¿

```python
# app/monitoring/realtime_dashboard.py
class RealtimePerformanceDashboard:
    """å®æ—¶æ€§èƒ½ç›‘æ§é¢æ¿"""
    
    def __init__(self):
        self.metrics_collector = RealtimePerformanceMonitor()
        self.alert_manager = AlertManager()
    
    async def get_realtime_metrics(self) -> Dict:
        """è·å–å®æ—¶æ€§èƒ½æŒ‡æ ‡"""
        
        current_metrics = await self.metrics_collector.get_performance_metrics()
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡
        dashboard_data = {
            'timestamp': datetime.now().isoformat(),
            'system_health': self._calculate_system_health(current_metrics),
            'realtime_functions': {
                'grading': {
                    'avg_response_time': current_metrics.get('writing_grading', {}).get('avg_response_time', 0),
                    'success_rate': current_metrics.get('writing_grading', {}).get('success_rate', 0),
                    'current_load': self._get_current_load('grading'),
                    'status': self._get_function_status('grading', current_metrics)
                },
                'assist': {
                    'avg_response_time': current_metrics.get('realtime_assist', {}).get('avg_response_time', 0),
                    'success_rate': current_metrics.get('realtime_assist', {}).get('success_rate', 0),
                    'current_load': self._get_current_load('assist'),
                    'status': self._get_function_status('assist', current_metrics)
                }
            },
            'alerts': await self.alert_manager.get_active_alerts(),
            'performance_trends': await self._get_performance_trends()
        }
        
        return dashboard_data
    
    def _calculate_system_health(self, metrics: Dict) -> str:
        """è®¡ç®—ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        
        critical_functions = ['writing_grading', 'realtime_assist']
        health_scores = []
        
        for func in critical_functions:
            func_metrics = metrics.get(func, {})
            response_time = func_metrics.get('avg_response_time', float('inf'))
            success_rate = func_metrics.get('success_rate', 0)
            
            # è®¡ç®—å¥åº·åˆ†æ•° (0-100)
            time_score = max(0, 100 - (response_time * 50))  # 2ç§’=0åˆ†
            success_score = success_rate * 100
            
            health_score = (time_score + success_score) / 2
            health_scores.append(health_score)
        
        avg_health = sum(health_scores) / len(health_scores) if health_scores else 0
        
        if avg_health >= 90:
            return 'excellent'
        elif avg_health >= 75:
            return 'good'
        elif avg_health >= 60:
            return 'warning'
        else:
            return 'critical'

### æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

```python
# tests/performance/report_generator.py
class PerformanceReportGenerator:
    """æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def generate_comprehensive_report(self, test_results: Dict) -> str:
        """ç”Ÿæˆç»¼åˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        
        report = f"""
# è‹±è¯­å››çº§å­¦ä¹ ç³»ç»Ÿå®æ—¶æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ
- æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- æµ‹è¯•ç¯å¢ƒ: {self._get_test_environment()}
- æµ‹è¯•æŒç»­æ—¶é—´: {test_results.get('total_duration', 0):.1f}åˆ†é’Ÿ

## æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡

### æ™ºèƒ½æ‰¹æ”¹ç³»ç»Ÿ
- å¹³å‡å“åº”æ—¶é—´: {test_results.get('grading', {}).get('avg_response_time', 0):.2f}s (è¦æ±‚: <3s)
- æˆåŠŸç‡: {test_results.get('grading', {}).get('success_rate', 0):.1%} (è¦æ±‚: >95%)
- å¹¶å‘å¤„ç†èƒ½åŠ›: {test_results.get('grading', {}).get('max_concurrent', 0)}ç”¨æˆ·
- é¦–å­—èŠ‚æ—¶é—´: {test_results.get('grading', {}).get('first_byte_time', 0):.3f}s (è¦æ±‚: <0.5s)

### å®æ—¶å†™ä½œè¾…åŠ©
- å¹³å‡å“åº”æ—¶é—´: {test_results.get('assist', {}).get('avg_response_time', 0):.2f}s (è¦æ±‚: <1s)
- æˆåŠŸç‡: {test_results.get('assist', {}).get('success_rate', 0):.1%} (è¦æ±‚: >98%)
- é˜²æŠ–æ•ˆæœ: {test_results.get('assist', {}).get('debounce_effectiveness', 0):.1%}

### ç³»ç»Ÿæ•´ä½“æ€§èƒ½
- æ€»ååé‡: {test_results.get('system', {}).get('throughput', 0):.1f} req/s
- ç³»ç»Ÿå¥åº·åº¦: {test_results.get('system', {}).get('health_score', 0):.1f}/100
- èµ„æºä½¿ç”¨ç‡: CPU {test_results.get('system', {}).get('cpu_usage', 0):.1f}%, å†…å­˜ {test_results.get('system', {}).get('memory_usage', 0):.1f}%

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

{self._generate_optimization_suggestions(test_results)}

## é£é™©è¯„ä¼°

{self._generate_risk_assessment(test_results)}
"""
        
        return report
```

## ğŸš¨ æ€§èƒ½å‘Šè­¦é…ç½®

### å‘Šè­¦è§„åˆ™

```yaml
performance_alerts:
  critical:
    - metric: "grading_response_time"
      threshold: 5.0
      duration: "2m"
      action: "immediate_notification"
    
    - metric: "assist_success_rate"
      threshold: 0.90
      duration: "1m"
      action: "immediate_notification"
  
  warning:
    - metric: "grading_response_time"
      threshold: 3.5
      duration: "5m"
      action: "delayed_notification"
    
    - metric: "system_throughput"
      threshold: 30
      duration: "10m"
      action: "performance_review"

notification_channels:
  - type: "slack"
    webhook: "${SLACK_WEBHOOK_URL}"
    channels: ["#alerts", "#performance"]
  
  - type: "email"
    recipients: ["ops@company.com", "dev@company.com"]
  
  - type: "sms"
    recipients: ["${ONCALL_PHONE}"]
    conditions: ["critical_only"]
```

è¿™ä¸ªæ€§èƒ½æµ‹è¯•é…ç½®ç¡®ä¿äº†ï¼š

1. **å…¨é¢çš„æ€§èƒ½æµ‹è¯•è¦†ç›–**ï¼šå•æ¬¡ã€å¹¶å‘ã€å‹åŠ›ã€æ··åˆè´Ÿè½½æµ‹è¯•
2. **ä¸¥æ ¼çš„æ€§èƒ½åŸºå‡†**ï¼šæ˜ç¡®çš„å“åº”æ—¶é—´å’ŒæˆåŠŸç‡è¦æ±‚
3. **å®æ—¶ç›‘æ§é¢æ¿**ï¼šç³»ç»Ÿå¥åº·çŠ¶æ€å’Œæ€§èƒ½è¶‹åŠ¿ç›‘æ§
4. **è‡ªåŠ¨åŒ–å‘Šè­¦**ï¼šæ€§èƒ½å¼‚å¸¸æ—¶åŠæ—¶é€šçŸ¥
5. **è¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š**ï¼šæ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®

é€šè¿‡è¿™å¥—æµ‹è¯•ä½“ç³»ï¼Œå¯ä»¥ç¡®ä¿å®æ—¶åŠŸèƒ½åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„ç¨³å®šæ€§å’Œç”¨æˆ·ä½“éªŒã€‚